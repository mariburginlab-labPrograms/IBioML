{"config":{"lang":["en","es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IBioML","text":"<p>IBioML es un toolkit de Machine Learning especializado para experimentos de neurodecodificaci\u00f3n desarrollado en el IBioBA (Instituto de Investigaci\u00f3n en Biomedicina de Buenos Aires).</p>"},{"location":"#caracteristicas-principales","title":"\u2728 Caracter\u00edsticas Principales","text":"<ul> <li>\ud83e\udde0 Modelos especializados para datos neuronales (MLP, RNN, LSTM, GRU)</li> <li>\ud83d\udcca Preprocesamiento autom\u00e1tico de archivos <code>.mat</code> de MATLAB</li> <li>\ud83d\udd0d Optimizaci\u00f3n de hiperpar\u00e1metros con Optuna</li> <li>\ud83d\udcc8 Visualizaci\u00f3n avanzada de resultados y m\u00e9tricas</li> <li>\ud83d\udd04 Validaci\u00f3n cruzada anidada para evaluaci\u00f3n robusta</li> <li>\ud83d\udcc1 Gesti\u00f3n inteligente de experimentos y resultados</li> </ul>"},{"location":"#inicio-rapido","title":"\ud83d\ude80 Inicio R\u00e1pido","text":""},{"location":"#instalacion","title":"Instalaci\u00f3n","text":"<pre><code>pip install ibioml\n</code></pre>"},{"location":"#uso-basico","title":"Uso B\u00e1sico","text":"<pre><code>from ibioml.preprocessing import preprocess_data\nfrom ibioml.models import MLPModel\nfrom ibioml.tuner import run_study\n\n# 1. Preprocesar datos\npreprocess_data(\n    file_path='datasets/mi_experimento.mat',\n    file_name_to_save='data/experimento_procesado',\n    bins_before=5,\n    bins_after=5,\n    bins_current=1,\n    threshDPrime=2.5,\n    firingMinimo=1000\n)\n\n# 2. Configurar experimento\nmlp_config = {\n    \"model_class\": MLPModel,\n    \"output_size\": 1,\n    \"device\": \"cuda\",\n    \"num_epochs\": 200,\n    \"batch_size\": 32,\n    # Hiperpar\u00e1metros a optimizar\n    \"hidden_size\": (int, 128, 512, 64),  # (tipo, min, max, step)\n    \"lr\": (float, 1e-5, 1e-2, True),     # (tipo, min, max, log_scale)\n}\n\n# 3. Ejecutar experimento\nrun_study(\n    X, y, T,\n    model_space=mlp_config,\n    num_trials=50,\n    outer_folds=5,\n    save_path=\"results/mi_experimento\"\n)\n</code></pre>"},{"location":"#documentacion","title":"\ud83d\udcd6 Documentaci\u00f3n","text":"<ul> <li>Gu\u00eda de Instalaci\u00f3n - Configuraci\u00f3n del entorno</li> <li>Preprocesamiento - Transformaci\u00f3n de datos</li> <li>Experimentos - Configuraci\u00f3n y ejecuci\u00f3n</li> <li>Visualizaci\u00f3n - An\u00e1lisis de resultados</li> <li>API Reference - Documentaci\u00f3n completa de la API</li> </ul>"},{"location":"#tipos-de-experimentos-soportados","title":"\ud83d\udd2c Tipos de Experimentos Soportados","text":""},{"location":"#modelos-disponibles","title":"Modelos Disponibles","text":"Modelo Descripci\u00f3n Uso Recomendado MLP Perceptr\u00f3n multicapa Datos sin estructura temporal RNN Red neuronal recurrente Secuencias temporales b\u00e1sicas LSTM Long Short-Term Memory Secuencias con dependencias largas GRU Gated Recurrent Unit Alternativa eficiente a LSTM"},{"location":"#configuraciones-de-datos","title":"Configuraciones de Datos","text":"<p>IBioML genera autom\u00e1ticamente m\u00faltiples variantes de tus datos:</p> Con ContextoSin Contexto <ul> <li><code>withCtxt_onlyPosition</code> - Solo informaci\u00f3n de posici\u00f3n</li> <li><code>withCtxt_onlyVelocity</code> - Solo informaci\u00f3n de velocidad  </li> <li><code>withCtxt_bothTargets</code> - Posici\u00f3n y velocidad</li> </ul> <ul> <li><code>onlyPosition</code> - Posici\u00f3n sin contexto</li> <li><code>onlyVelocity</code> - Velocidad sin contexto</li> <li><code>bothTargets</code> - Ambos targets sin contexto</li> </ul> <p>Cada configuraci\u00f3n est\u00e1 disponible en formato <code>flat</code> (para modelos no recurrentes) y formato temporal (para modelos recurrentes).</p>"},{"location":"#arquitectura","title":"\ud83c\udfd7\ufe0f Arquitectura","text":"<pre><code>graph TD\n    A[Datos .mat] --&gt; B[Preprocesamiento]\n    B --&gt; C[Datos Procesados]\n    C --&gt; D[Configuraci\u00f3n del Experimento]\n    D --&gt; E[Optimizaci\u00f3n de Hiperpar\u00e1metros]\n    E --&gt; F[Validaci\u00f3n Cruzada]\n    F --&gt; G[Resultados]\n    G --&gt; H[Visualizaci\u00f3n]</code></pre>"},{"location":"#contribuir","title":"\ud83e\udd1d Contribuir","text":"<p>\u00a1Las contribuciones son bienvenidas! Ve la gu\u00eda de contribuci\u00f3n para m\u00e1s detalles.</p>"},{"location":"#licencia","title":"\ud83d\udcc4 Licencia","text":"<p>Este proyecto est\u00e1 bajo la Licencia MIT. Ve el archivo LICENSE para m\u00e1s detalles.</p>"},{"location":"#contacto","title":"\ud83d\udce7 Contacto","text":"<ul> <li>Email: jiponce@ibioba-mpsp-conicet.gov.ar</li> <li>GitHub: mariburginlab-labPrograms/IBioML</li> </ul>    Desarrollado con \u2764\ufe0f en el IBioBA"},{"location":"DEVELOPMENT/","title":"Comandos \u00fatiles para desarrollar la documentaci\u00f3n","text":""},{"location":"DEVELOPMENT/#instalar-dependencias-de-documentacion","title":"Instalar dependencias de documentaci\u00f3n","text":"<p>pip install -r requirements-docs.txt pip install -e .</p>"},{"location":"DEVELOPMENT/#servir-la-documentacion-localmente-con-auto-reload","title":"Servir la documentaci\u00f3n localmente (con auto-reload)","text":"<p>mkdocs serve</p>"},{"location":"DEVELOPMENT/#construir-la-documentacion-estatica","title":"Construir la documentaci\u00f3n est\u00e1tica","text":"<p>mkdocs build</p>"},{"location":"DEVELOPMENT/#desplegar-la-documentacion-a-github-pages-manual","title":"Desplegar la documentaci\u00f3n a GitHub Pages (manual)","text":"<p>mkdocs gh-deploy</p>"},{"location":"DEVELOPMENT/#verificar-enlaces-rotos","title":"Verificar enlaces rotos","text":"<p>mkdocs build --strict</p>"},{"location":"DEVELOPMENT/#construir-con-modo-verbose-para-debugging","title":"Construir con modo verbose para debugging","text":"<p>mkdocs build --verbose</p>"},{"location":"DEVELOPMENT/#limpiar-archivos-de-construccion","title":"Limpiar archivos de construcci\u00f3n","text":"<p>mkdocs build --clean</p>"},{"location":"DEVELOPMENT/#variables-de-entorno-utiles","title":"Variables de entorno \u00fatiles","text":"<p>export PYTHONPATH=\"${PYTHONPATH}:$(pwd)\"</p>"},{"location":"DEVELOPMENT/#estructura-de-archivos-de-documentacion","title":"Estructura de archivos de documentaci\u00f3n","text":"<p>\"\"\" docs/ \u251c\u2500\u2500 index.md                    # P\u00e1gina principal \u251c\u2500\u2500 installation.md             # Gu\u00eda de instalaci\u00f3n \u251c\u2500\u2500 preprocessing.md             # Documentaci\u00f3n de preprocesamiento \u251c\u2500\u2500 experiments.md               # Configuraci\u00f3n de experimentos \u251c\u2500\u2500 visualization.md             # Gu\u00eda de visualizaci\u00f3n \u251c\u2500\u2500 contributing.md              # Gu\u00eda de contribuci\u00f3n \u251c\u2500\u2500 api/                        # Documentaci\u00f3n de API \u2502   \u251c\u2500\u2500 models.md               # Documentaci\u00f3n de modelos \u2502   \u251c\u2500\u2500 preprocessing.md        # API de preprocesamiento \u2502   \u251c\u2500\u2500 training.md             # API de entrenamiento \u2502   \u2514\u2500\u2500 results.md              # API de resultados \u251c\u2500\u2500 examples/                   # Ejemplos y tutoriales \u2502   \u251c\u2500\u2500 basic_tutorial.md       # Tutorial b\u00e1sico \u2502   \u2514\u2500\u2500 full_experiment.md      # Experimento completo \u251c\u2500\u2500 images/                     # Im\u00e1genes para la documentaci\u00f3n \u251c\u2500\u2500 stylesheets/                # Estilos CSS personalizados \u2502   \u2514\u2500\u2500 extra.css               # Estilos adicionales \u2514\u2500\u2500 ... \"\"\"</p>"},{"location":"DEVELOPMENT/#tips-para-escribir-documentacion","title":"Tips para escribir documentaci\u00f3n","text":"<ol> <li>Usa docstrings estilo Google en tu c\u00f3digo Python</li> <li>Incluye ejemplos en los docstrings</li> <li>Agrega type hints para mejor documentaci\u00f3n autom\u00e1tica</li> <li>Usa admoniciones para destacar informaci\u00f3n importante:</li> <li> <p>Nota</p> </li> <li> <p>Advertencia</p> </li> <li> <p>Consejo</p> </li> <li> <p>Peligro</p> </li> <li> <p>Ejemplos de c\u00f3digo con syntax highlighting:    <pre><code>def mi_funcion(param: str) -&gt; int:\n    \"\"\"Ejemplo de funci\u00f3n documentada.\n\n    Args:\n        param: Descripci\u00f3n del par\u00e1metro\n\n    Returns:\n        Descripci\u00f3n del valor de retorno\n\n    Example:\n        &gt;&gt;&gt; mi_funcion(\"test\")\n        42\n    \"\"\"\n    return 42\n</code></pre></p> </li> </ol>"},{"location":"DEVELOPMENT/#troubleshooting","title":"Troubleshooting","text":""},{"location":"DEVELOPMENT/#error-module-not-found","title":"Error: Module not found","text":"<ul> <li>Aseg\u00farate de que <code>pip install -e .</code> se ejecut\u00f3 correctamente</li> <li>Verifica que el PYTHONPATH incluya el directorio del proyecto</li> </ul>"},{"location":"DEVELOPMENT/#error-mkdocstrings-no-encuentra-los-modulos","title":"Error: mkdocstrings no encuentra los m\u00f3dulos","text":"<ul> <li>Verifica que los paths en mkdocs.yml sean correctos</li> <li>Aseg\u00farate de que los init.py existan en los directorios de m\u00f3dulos</li> </ul>"},{"location":"DEVELOPMENT/#links-rotos","title":"Links rotos","text":"<ul> <li>Usa <code>mkdocs build --strict</code> para encontrar enlaces rotos</li> <li>Verifica las rutas relativas en los archivos markdown</li> </ul>"},{"location":"DEVELOPMENT/#problemas-con-el-tema-material","title":"Problemas con el tema Material","text":"<ul> <li>Verifica la versi\u00f3n de mkdocs-material</li> <li>Revisa la sintaxis de las extensiones en mkdocs.yml</li> </ul>"},{"location":"DEVELOPMENT/#documentation-build-test","title":"Documentation build test","text":""},{"location":"contributing/","title":"Contribuir a IbioML","text":"<p>\u00a1Gracias por tu inter\u00e9s en contribuir a IbioML! Este proyecto se beneficia enormemente de las contribuciones de la comunidad.</p>"},{"location":"contributing/#formas-de-contribuir","title":"\ud83d\ude80 Formas de Contribuir","text":""},{"location":"contributing/#reportar-bugs","title":"\ud83d\udc1b Reportar Bugs","text":"<ul> <li>Usa el sistema de issues de GitHub</li> <li>Incluye informaci\u00f3n detallada sobre el error</li> <li>Proporciona un ejemplo m\u00ednimo reproducible</li> </ul>"},{"location":"contributing/#sugerir-nuevas-caracteristicas","title":"\ud83d\udca1 Sugerir Nuevas Caracter\u00edsticas","text":"<ul> <li>Abre un issue describiendo la caracter\u00edstica</li> <li>Explica el caso de uso y beneficios</li> <li>Discute la implementaci\u00f3n propuesta</li> </ul>"},{"location":"contributing/#mejorar-documentacion","title":"\ud83d\udcdd Mejorar Documentaci\u00f3n","text":"<ul> <li>Corregir errores tipogr\u00e1ficos</li> <li>Agregar ejemplos o aclaraciones</li> <li>Traducir contenido</li> </ul>"},{"location":"contributing/#contribuir-codigo","title":"\ud83d\udd27 Contribuir C\u00f3digo","text":"<ul> <li>Implementar nuevas caracter\u00edsticas</li> <li>Corregir bugs</li> <li>Mejorar rendimiento</li> <li>Agregar tests</li> </ul>"},{"location":"contributing/#configuracion-del-entorno-de-desarrollo","title":"\ud83d\udee0\ufe0f Configuraci\u00f3n del Entorno de Desarrollo","text":""},{"location":"contributing/#1-fork-y-clone","title":"1. Fork y Clone","text":"<pre><code># Fork el repositorio en GitHub, luego:\ngit clone https://github.com/tu-usuario/IbioML.git\ncd IbioML\n</code></pre>"},{"location":"contributing/#2-configurar-entorno-virtual","title":"2. Configurar Entorno Virtual","text":"<pre><code># Crear entorno virtual\npython -m venv venv\nsource venv/bin/activate  # En Windows: venv\\Scripts\\activate\n\n# Instalar en modo desarrollo\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/#3-instalar-dependencias-de-desarrollo","title":"3. Instalar Dependencias de Desarrollo","text":"<pre><code>pip install pytest black flake8 mypy pre-commit mkdocs-material\n</code></pre>"},{"location":"contributing/#4-configurar-pre-commit-hooks","title":"4. Configurar Pre-commit Hooks","text":"<pre><code>pre-commit install\n</code></pre>"},{"location":"contributing/#guias-de-contribucion","title":"\ud83d\udccb Gu\u00edas de Contribuci\u00f3n","text":""},{"location":"contributing/#estilo-de-codigo","title":"Estilo de C\u00f3digo","text":"<p>IbioML sigue las convenciones de Python (PEP 8) con algunas extensiones:</p> <pre><code># Usar type hints\ndef preprocess_data(\n    file_path: str,\n    file_name_to_save: str,\n    bins_before: int = 5,\n    bins_after: int = 5\n) -&gt; None:\n    \"\"\"\n    Preprocesa datos neuronales.\n\n    Args:\n        file_path: Ruta al archivo .mat\n        file_name_to_save: Nombre base para archivos de salida\n        bins_before: Ventana temporal hacia atr\u00e1s\n        bins_after: Ventana temporal hacia adelante\n    \"\"\"\n    pass\n\n# Nombres descriptivos\ndef calculate_r2_score(predictions: np.ndarray, targets: np.ndarray) -&gt; float:\n    \"\"\"Calcula el coeficiente de determinaci\u00f3n R\u00b2.\"\"\"\n    pass\n\n# Documentaci\u00f3n clara en espa\u00f1ol para funciones p\u00fablicas\nclass MLPModel(nn.Module):\n    \"\"\"\n    Perceptr\u00f3n multicapa para neurodecodificaci\u00f3n.\n\n    Esta clase implementa una red neuronal feedforward con m\u00faltiples\n    capas ocultas y dropout para regularizaci\u00f3n.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"contributing/#formateo-automatico","title":"Formateo Autom\u00e1tico","text":"<pre><code># Formatear c\u00f3digo con black\nblack ibioml/\n\n# Verificar estilo con flake8\nflake8 ibioml/\n\n# Verificar tipos con mypy\nmypy ibioml/\n</code></pre>"},{"location":"contributing/#estructura-de-commits","title":"Estructura de Commits","text":"<p>Usa el formato de Conventional Commits:</p> <pre><code>tipo(\u00e1mbito): descripci\u00f3n breve\n\nDescripci\u00f3n m\u00e1s detallada si es necesario.\n\nFixes #123\n</code></pre> <p>Tipos principales: - <code>feat</code>: Nueva caracter\u00edstica - <code>fix</code>: Correcci\u00f3n de bug - <code>docs</code>: Cambios en documentaci\u00f3n - <code>style</code>: Cambios de formato (sin afectar l\u00f3gica) - <code>refactor</code>: Refactorizaci\u00f3n de c\u00f3digo - <code>test</code>: Agregar o modificar tests - <code>chore</code>: Tareas de mantenimiento</p> <p>Ejemplos: <pre><code>feat(models): agregar soporte para modelos transformer\n\ndocs(preprocessing): mejorar ejemplos de uso\n\nfix(tuner): corregir error en validaci\u00f3n cruzada anidada\n</code></pre></p>"},{"location":"contributing/#testing","title":"\ud83e\uddea Testing","text":""},{"location":"contributing/#ejecutar-tests","title":"Ejecutar Tests","text":"<pre><code># Todos los tests\npytest\n\n# Tests espec\u00edficos\npytest tests/test_models.py\n\n# Con cobertura\npytest --cov=ibioml\n</code></pre>"},{"location":"contributing/#escribir-tests","title":"Escribir Tests","text":"<pre><code>import pytest\nimport numpy as np\nfrom ibioml.models import MLPModel\n\nclass TestMLPModel:\n    \"\"\"Tests para el modelo MLP.\"\"\"\n\n    def test_model_creation(self):\n        \"\"\"Test b\u00e1sico de creaci\u00f3n del modelo.\"\"\"\n        model = MLPModel(\n            input_size=100,\n            hidden_size=50,\n            output_size=1,\n            num_layers=2,\n            dropout=0.1\n        )\n        assert model.input_size == 100\n        assert model.output_size == 1\n\n    def test_forward_pass(self):\n        \"\"\"Test del forward pass.\"\"\"\n        model = MLPModel(100, 50, 1, 2, 0.1)\n        x = torch.randn(10, 100)\n        output = model(x)\n        assert output.shape == (10, 1)\n\n    @pytest.mark.parametrize(\"batch_size\", [1, 16, 32])\n    def test_different_batch_sizes(self, batch_size):\n        \"\"\"Test con diferentes tama\u00f1os de lote.\"\"\"\n        model = MLPModel(100, 50, 1, 2, 0.1)\n        x = torch.randn(batch_size, 100)\n        output = model(x)\n        assert output.shape == (batch_size, 1)\n</code></pre>"},{"location":"contributing/#documentacion","title":"\ud83d\udcda Documentaci\u00f3n","text":""},{"location":"contributing/#estructura-de-documentacion","title":"Estructura de Documentaci\u00f3n","text":"<pre><code>docs/\n\u251c\u2500\u2500 index.md              # P\u00e1gina principal\n\u251c\u2500\u2500 installation.md       # Gu\u00eda de instalaci\u00f3n\n\u251c\u2500\u2500 preprocessing.md       # Preprocesamiento\n\u251c\u2500\u2500 experiments.md         # Configuraci\u00f3n de experimentos\n\u251c\u2500\u2500 visualization.md       # Visualizaci\u00f3n\n\u251c\u2500\u2500 api/                   # Documentaci\u00f3n de API\n\u2502   \u251c\u2500\u2500 models.md\n\u2502   \u251c\u2500\u2500 preprocessing.md\n\u2502   \u2514\u2500\u2500 training.md\n\u2514\u2500\u2500 examples/              # Tutoriales y ejemplos\n    \u251c\u2500\u2500 basic_tutorial.md\n    \u2514\u2500\u2500 advanced_usage.md\n</code></pre>"},{"location":"contributing/#escribir-documentacion","title":"Escribir Documentaci\u00f3n","text":"<pre><code># T\u00edtulo de la Secci\u00f3n\n\nDescripci\u00f3n breve y clara de qu\u00e9 hace esta funcionalidad.\n\n## Uso B\u00e1sico\n\n```python\n# Ejemplo de c\u00f3digo simple\nfrom ibioml.models import MLPModel\n\nmodel = MLPModel(input_size=100, hidden_size=50, output_size=1)\n</code></pre>"},{"location":"contributing/#parametros","title":"Par\u00e1metros","text":"Par\u00e1metro Tipo Descripci\u00f3n <code>input_size</code> int N\u00famero de caracter\u00edsticas de entrada <code>hidden_size</code> int Neuronas en capas ocultas <p>Recomendaci\u00f3n</p> <p>Para mejores resultados, usa <code>hidden_size</code> entre 64 y 512.</p> <p>Advertencia</p> <p>Valores muy altos de <code>dropout</code> pueden degradar el rendimiento.</p> <pre><code>### Generar Documentaci\u00f3n Localmente\n\n```bash\n# Instalar dependencias\npip install mkdocs-material mkdocstrings[python]\n\n# Servir documentaci\u00f3n localmente\nmkdocs serve\n\n# Compilar documentaci\u00f3n\nmkdocs build\n</code></pre>"},{"location":"contributing/#proceso-de-pull-request","title":"\ud83d\udd04 Proceso de Pull Request","text":""},{"location":"contributing/#1-preparar-el-pr","title":"1. Preparar el PR","text":"<pre><code># Crear rama para tu caracter\u00edstica\ngit checkout -b feat/nueva-caracteristica\n\n# Hacer cambios y commits\ngit add .\ngit commit -m \"feat(models): agregar modelo transformer\"\n\n# Push a tu fork\ngit push origin feat/nueva-caracteristica\n</code></pre>"},{"location":"contributing/#2-crear-pull-request","title":"2. Crear Pull Request","text":"<ol> <li>Ve a GitHub y crea un PR desde tu rama</li> <li>Usa la plantilla de PR (si existe)</li> <li>Describe claramente los cambios</li> <li>Relaciona con issues relevantes</li> </ol>"},{"location":"contributing/#3-plantilla-de-pr","title":"3. Plantilla de PR","text":"<pre><code>## Descripci\u00f3n\n\nDescripci\u00f3n breve de los cambios realizados.\n\n## Tipo de cambio\n\n- [ ] Bug fix (cambio que corrige un issue)\n- [ ] Nueva caracter\u00edstica (cambio que agrega funcionalidad)\n- [ ] Breaking change (cambio que rompe compatibilidad)\n- [ ] Documentaci\u00f3n\n\n## Testing\n\n- [ ] Tests existentes pasan\n- [ ] Agregu\u00e9 tests para nuevos cambios\n- [ ] Tests cubren casos edge\n\n## Checklist\n\n- [ ] Mi c\u00f3digo sigue el estilo del proyecto\n- [ ] Agregu\u00e9 documentaci\u00f3n para nuevas caracter\u00edsticas\n- [ ] Los tests pasan localmente\n- [ ] Actualic\u00e9 CHANGELOG.md (si aplica)\n\n## Issues relacionados\n\nFixes #123\n</code></pre>"},{"location":"contributing/#4-revision-de-codigo","title":"4. Revisi\u00f3n de C\u00f3digo","text":"<ul> <li>Responde constructivamente a los comentarios</li> <li>Haz los cambios solicitados</li> <li>Mant\u00e9n la discusi\u00f3n profesional y enfocada</li> </ul>"},{"location":"contributing/#arquitectura-del-proyecto","title":"\ud83c\udfd7\ufe0f Arquitectura del Proyecto","text":""},{"location":"contributing/#estructura-de-carpetas","title":"Estructura de Carpetas","text":"<pre><code>ibioml/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 models.py              # Modelos de ML\n\u251c\u2500\u2500 preprocessing.py       # Preprocesamiento de datos\n\u251c\u2500\u2500 trainer.py            # L\u00f3gica de entrenamiento\n\u251c\u2500\u2500 tuner.py              # Optimizaci\u00f3n de hiperpar\u00e1metros\n\u251c\u2500\u2500 plots.py              # Funciones de visualizaci\u00f3n\n\u251c\u2500\u2500 utils/                # Utilidades\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 data_scaler.py\n\u2502   \u251c\u2500\u2500 evaluators.py\n\u2502   \u251c\u2500\u2500 model_factory.py\n\u2502   \u2514\u2500\u2500 preprocessing_funcs.py\n\u2514\u2500\u2500 results/              # Gesti\u00f3n de resultados (nueva)\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 experiment_results.py\n    \u2514\u2500\u2500 visualizer.py\n</code></pre>"},{"location":"contributing/#principios-de-diseno","title":"Principios de Dise\u00f1o","text":"<ol> <li>Modularidad: Cada m\u00f3dulo tiene una responsabilidad clara</li> <li>Extensibilidad: F\u00e1cil agregar nuevos modelos y funcionalidades</li> <li>Usabilidad: API simple e intuitiva</li> <li>Robustez: Manejo de errores y validaci\u00f3n de entrada</li> <li>Rendimiento: Optimizado para datasets grandes</li> </ol>"},{"location":"contributing/#areas-que-necesitan-contribuciones","title":"\ud83c\udfaf \u00c1reas que Necesitan Contribuciones","text":""},{"location":"contributing/#alta-prioridad","title":"Alta Prioridad","text":"<ul> <li> Soporte para modelos Transformer</li> <li> Mejoras en visualizaci\u00f3n interactiva</li> <li> Optimizaci\u00f3n de memoria para datasets grandes</li> <li> Integraci\u00f3n con MLflow/Weights &amp; Biases</li> <li> Tests adicionales (especialmente integration tests)</li> </ul>"},{"location":"contributing/#media-prioridad","title":"Media Prioridad","text":"<ul> <li> Soporte para m\u00e1s formatos de datos (HDF5, Parquet)</li> <li> An\u00e1lisis estad\u00edsticos avanzados</li> <li> Exportaci\u00f3n de modelos a ONNX</li> <li> Paralelizaci\u00f3n de experimentos</li> <li> Documentaci\u00f3n en ingl\u00e9s</li> </ul>"},{"location":"contributing/#baja-prioridad","title":"Baja Prioridad","text":"<ul> <li> Interfaz gr\u00e1fica web</li> <li> Soporte para modelos probabil\u00edsticos</li> <li> Integraci\u00f3n con cloud providers</li> <li> Mobile/edge deployment</li> </ul>"},{"location":"contributing/#comunicacion","title":"\ud83e\udd1d Comunicaci\u00f3n","text":""},{"location":"contributing/#canales-de-comunicacion","title":"Canales de Comunicaci\u00f3n","text":"<ul> <li>Issues de GitHub: Para bugs y feature requests</li> <li>Discussions: Para preguntas generales y discusi\u00f3n</li> <li>Email: jiponce@ibioba-mpsp-conicet.gov.ar</li> </ul>"},{"location":"contributing/#codigo-de-conducta","title":"C\u00f3digo de Conducta","text":"<ul> <li>S\u00e9 respetuoso y constructivo</li> <li>Acepta feedback de manera positiva</li> <li>Ayuda a otros contribuidores</li> <li>Mant\u00e9n discusiones t\u00e9cnicas enfocadas</li> </ul>"},{"location":"contributing/#licencia","title":"\ud83d\udcdc Licencia","text":"<p>Al contribuir a IbioML, aceptas que tus contribuciones sean licenciadas bajo la misma licencia MIT del proyecto.</p> <p>\u00a1Gracias por contribuir a IbioML! \ud83d\ude80</p>"},{"location":"experiments/","title":"Configuraci\u00f3n y Ejecuci\u00f3n de Experimentos","text":"<p>IbioML proporciona un sistema flexible y potente para ejecutar experimentos de neurodecodificaci\u00f3n con optimizaci\u00f3n autom\u00e1tica de hiperpar\u00e1metros y validaci\u00f3n cruzada anidada.</p>"},{"location":"experiments/#vision-general","title":"\ud83c\udfaf Visi\u00f3n General","text":"<p>Un experimento t\u00edpico en IbioML incluye:</p> <ol> <li>Carga de datos preprocesados</li> <li>Configuraci\u00f3n del modelo y hiperpar\u00e1metros</li> <li>Optimizaci\u00f3n con Optuna (Bayesian, Random, Grid Search)</li> <li>Validaci\u00f3n cruzada anidada para evaluaci\u00f3n robusta</li> <li>Guardado autom\u00e1tico de resultados y modelos</li> </ol>"},{"location":"experiments/#experimento-basico","title":"\ud83d\ude80 Experimento B\u00e1sico","text":""},{"location":"experiments/#configuracion-minima","title":"Configuraci\u00f3n M\u00ednima","text":"<pre><code>import pickle\nfrom ibioml.models import MLPModel\nfrom ibioml.tuner import run_study\n\n# 1. Cargar datos preprocesados\nwith open('data/experimento_withCtxt_flat.pickle', 'rb') as f:\n    X, y, trial_markers = pickle.load(f)\n\n# 2. Configuraci\u00f3n b\u00e1sica del modelo\nconfig = {\n    \"model_class\": MLPModel,\n    \"output_size\": y.shape[1],  # 1 para single target, 2 para position+velocity\n    \"device\": \"cuda\",           # \"cuda\" o \"cpu\"\n    \"num_epochs\": 200,\n    \"es_patience\": 10,          # Early stopping patience\n    \"reg_type\": None,           # Regularizaci\u00f3n: None, 'l1', 'l2'\n    \"lambda_reg\": None,\n    \"batch_size\": 32,\n\n    # Hiperpar\u00e1metros fijos\n    \"hidden_size\": 256,\n    \"num_layers\": 2,\n    \"dropout\": 0.2,\n    \"lr\": 1e-3\n}\n\n# 3. Ejecutar experimento\nrun_study(\n    X, y, trial_markers,\n    model_space=config,\n    num_trials=1,              # Solo un trial (sin optimizaci\u00f3n)\n    outer_folds=5,\n    inner_folds=1,\n    save_path=\"results/experimento_basico\"\n)\n</code></pre>"},{"location":"experiments/#optimizacion-de-hiperparametros","title":"\ud83d\udd27 Optimizaci\u00f3n de Hiperpar\u00e1metros","text":""},{"location":"experiments/#configuracion-con-optimizacion","title":"Configuraci\u00f3n con Optimizaci\u00f3n","text":"<p>Para activar la optimizaci\u00f3n, define hiperpar\u00e1metros como tuplas:</p> <pre><code>config_optimized = {\n    # Par\u00e1metros fijos\n    \"model_class\": MLPModel,\n    \"output_size\": 1,\n    \"device\": \"cuda\",\n    \"num_epochs\": 200,\n    \"es_patience\": 10,\n    \"reg_type\": None,\n    \"lambda_reg\": None,\n    \"batch_size\": 32,\n\n    # Hiperpar\u00e1metros a optimizar\n    \"hidden_size\": (int, 128, 512, 64),     # (tipo, min, max, step)\n    \"num_layers\": (int, 1, 4),              # (tipo, min, max)\n    \"dropout\": (float, 0.0, 0.5),           # (tipo, min, max)\n    \"lr\": (float, 1e-5, 1e-2, True),        # (tipo, min, max, log_scale)\n}\n\nrun_study(\n    X, y, trial_markers,\n    model_space=config_optimized,\n    num_trials=50,              # 50 configuraciones diferentes\n    outer_folds=5,\n    inner_folds=3,              # Validaci\u00f3n cruzada interna\n    save_path=\"results/experimento_optimizado\",\n    search_alg=\"bayes\"          # \"bayes\", \"random\", \"grid\"\n)\n</code></pre>"},{"location":"experiments/#formato-de-hiperparametros","title":"Formato de Hiperpar\u00e1metros","text":""},{"location":"experiments/#parametros-enteros","title":"Par\u00e1metros Enteros","text":"<pre><code>\"hidden_size\": (int, min_val, max_val, step)\n\"num_layers\": (int, 1, 5)  # step=1 por defecto\n</code></pre>"},{"location":"experiments/#parametros-de-punto-flotante","title":"Par\u00e1metros de Punto Flotante","text":"<pre><code>\"dropout\": (float, 0.0, 0.8)           # Escala lineal\n\"lr\": (float, 1e-6, 1e-1, True)        # Escala logar\u00edtmica\n</code></pre>"},{"location":"experiments/#modelos-disponibles","title":"\ud83e\udde0 Modelos Disponibles","text":""},{"location":"experiments/#modelos-para-datos-aplanados-_flatpickle","title":"Modelos para Datos Aplanados (<code>*_flat.pickle</code>)","text":"<pre><code>from ibioml.models import MLPModel\n\nmlp_config = {\n    \"model_class\": MLPModel,\n    \"hidden_size\": (int, 64, 1024, 32),\n    \"num_layers\": (int, 1, 5),\n    \"dropout\": (float, 0.0, 0.7),\n    \"lr\": (float, 1e-5, 1e-2, True),\n    # ... otros par\u00e1metros\n}\n</code></pre>"},{"location":"experiments/#modelos-para-datos-temporales-pickle","title":"Modelos para Datos Temporales (<code>.pickle</code>)","text":"<pre><code>from ibioml.models import RNNModel, LSTMModel, GRUModel\n\n# RNN b\u00e1sica\nrnn_config = {\n    \"model_class\": RNNModel,\n    \"hidden_size\": (int, 32, 256, 16),\n    \"num_layers\": (int, 1, 3),\n    \"dropout\": (float, 0.0, 0.5),\n    \"lr\": (float, 1e-5, 1e-2, True),\n    # ... otros par\u00e1metros\n}\n\n# LSTM (recomendado para secuencias largas)\nlstm_config = {\n    \"model_class\": LSTMModel,\n    \"hidden_size\": (int, 64, 512, 32),\n    \"num_layers\": (int, 1, 4),\n    \"dropout\": (float, 0.0, 0.6),\n    \"lr\": (float, 1e-5, 1e-2, True),\n    # ... otros par\u00e1metros\n}\n</code></pre>"},{"location":"experiments/#tipos-de-experimentos","title":"\ud83d\udcca Tipos de Experimentos","text":""},{"location":"experiments/#experimento-single-target","title":"Experimento Single-Target","text":"<pre><code># Solo decodificar posici\u00f3n\nwith open('data/experimento_withCtxt_onlyPosition_flat.pickle', 'rb') as f:\n    X_pos, y_pos, T = pickle.load(f)\n\nposition_config = {\n    \"model_class\": MLPModel,\n    \"output_size\": 1,  # Una sola salida\n    \"hidden_size\": (int, 128, 512, 64),\n    \"lr\": (float, 1e-5, 1e-2, True),\n    # ... resto de configuraci\u00f3n\n}\n\nrun_study(X_pos, y_pos, T, model_space=position_config, \n          save_path=\"results/position_decoding\")\n</code></pre>"},{"location":"experiments/#experimento-multi-target","title":"Experimento Multi-Target","text":"<pre><code># Decodificar posici\u00f3n y velocidad simult\u00e1neamente\nwith open('data/experimento_withCtxt_bothTargets_flat.pickle', 'rb') as f:\n    X_both, y_both, T = pickle.load(f)\n\ndual_config = {\n    \"model_class\": MLPModel,\n    \"output_size\": 2,  # Posici\u00f3n + velocidad\n    \"hidden_size\": (int, 256, 1024, 64),  # Redes m\u00e1s grandes para dual-output\n    \"lr\": (float, 1e-5, 1e-2, True),\n    # ... resto de configuraci\u00f3n\n}\n\nrun_study(X_both, y_both, T, model_space=dual_config,\n          save_path=\"results/dual_target_decoding\")\n</code></pre>"},{"location":"experiments/#comparacion-de-arquitecturas","title":"Comparaci\u00f3n de Arquitecturas","text":"<pre><code># Funci\u00f3n helper para experimentos comparativos\ndef run_architecture_comparison(X, y, T, base_path):\n    architectures = {\n        'mlp': MLPModel,\n        'rnn': RNNModel,\n        'lstm': LSTMModel,\n        'gru': GRUModel\n    }\n\n    base_config = {\n        \"output_size\": y.shape[1],\n        \"device\": \"cuda\",\n        \"num_epochs\": 150,\n        \"batch_size\": 32,\n        \"hidden_size\": (int, 128, 256, 32),\n        \"lr\": (float, 1e-4, 1e-2, True),\n    }\n\n    for arch_name, model_class in architectures.items():\n        config = base_config.copy()\n        config[\"model_class\"] = model_class\n\n        run_study(\n            X, y, T,\n            model_space=config,\n            num_trials=20,\n            outer_folds=5,\n            save_path=f\"{base_path}/{arch_name}\"\n        )\n        print(f\"\u2705 Completado: {arch_name}\")\n\n# Ejecutar comparaci\u00f3n\nrun_architecture_comparison(X, y, T, \"results/architecture_comparison\")\n</code></pre>"},{"location":"experiments/#configuracion-avanzada","title":"\u2699\ufe0f Configuraci\u00f3n Avanzada","text":""},{"location":"experiments/#algoritmos-de-optimizacion","title":"Algoritmos de Optimizaci\u00f3n","text":"<pre><code># Optimizaci\u00f3n Bayesiana (recomendado)\nrun_study(X, y, T, model_space=config, \n          search_alg=\"bayes\", num_trials=50)\n\n# B\u00fasqueda aleatoria (para espacios grandes)\nrun_study(X, y, T, model_space=config,\n          search_alg=\"random\", num_trials=100)\n\n# B\u00fasqueda en grilla (para espacios peque\u00f1os)\nrun_study(X, y, T, model_space=config,\n          search_alg=\"grid\", num_trials=25)\n</code></pre>"},{"location":"experiments/#configuracion-de-validacion-cruzada","title":"Configuraci\u00f3n de Validaci\u00f3n Cruzada","text":"<pre><code># Validaci\u00f3n cruzada est\u00e1ndar\nrun_study(X, y, T, model_space=config,\n          outer_folds=5,    # 5-fold CV externo\n          inner_folds=3)    # 3-fold CV interno\n\n# Para datasets peque\u00f1os\nrun_study(X, y, T, model_space=config,\n          outer_folds=3,\n          inner_folds=1)    # Sin CV interno\n\n# Para evaluaci\u00f3n robusta\nrun_study(X, y, T, model_space=config,\n          outer_folds=10,   # 10-fold CV\n          inner_folds=5)\n</code></pre>"},{"location":"experiments/#regularizacion","title":"Regularizaci\u00f3n","text":"<pre><code># Sin regularizaci\u00f3n\nconfig = {\n    \"reg_type\": None,\n    \"lambda_reg\": None,\n    # ... otros par\u00e1metros\n}\n\n# Con regularizaci\u00f3n L2\nconfig = {\n    \"reg_type\": \"l2\",\n    \"lambda_reg\": (float, 1e-6, 1e-2, True),\n    # ... otros par\u00e1metros\n}\n\n# Con regularizaci\u00f3n L1\nconfig = {\n    \"reg_type\": \"l1\", \n    \"lambda_reg\": (float, 1e-5, 1e-1, True),\n    # ... otros par\u00e1metros\n}\n</code></pre>"},{"location":"experiments/#estructura-de-resultados","title":"\ud83d\udcc1 Estructura de Resultados","text":""},{"location":"experiments/#organizacion-automatica","title":"Organizaci\u00f3n Autom\u00e1tica","text":"<pre><code>results/\n\u251c\u2500\u2500 experimento_basico/\n\u2502   \u2514\u2500\u2500 study_2024-01-15_14-30-25/    # Timestamp autom\u00e1tico\n\u2502       \u251c\u2500\u2500 final_results.json         # Resultados finales\n\u2502       \u2514\u2500\u2500 training_results/          # Resultados por fold\n\u2502           \u251c\u2500\u2500 fold_0/\n\u2502           \u2502   \u251c\u2500\u2500 results.json       # M\u00e9tricas del fold\n\u2502           \u2502   \u2514\u2500\u2500 best_model.pt      # Mejor modelo del fold\n\u2502           \u251c\u2500\u2500 fold_1/\n\u2502           \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"experiments/#contenido-de-resultados","title":"Contenido de Resultados","text":"<pre><code># final_results.json\n{\n    \"best_r2_score_test\": 0.847,\n    \"best_params\": {\n        \"hidden_size\": 256,\n        \"lr\": 0.0031,\n        \"dropout\": 0.23\n    },\n    \"mean_r2_test\": 0.821,\n    \"std_r2_test\": 0.045,\n    \"study_name\": \"study_2024-01-15_14-30-25\",\n    \"total_trials\": 50,\n    \"experiment_duration_minutes\": 23.5\n}\n</code></pre>"},{"location":"experiments/#monitoreo-de-experimentos","title":"\ud83d\udd0d Monitoreo de Experimentos","text":""},{"location":"experiments/#seguimiento-en-tiempo-real","title":"Seguimiento en Tiempo Real","text":"<pre><code>import optuna\n\n# Visualizar progreso (requiere optuna-dashboard)\nstudy = optuna.load_study(\n    study_name=\"mi_experimento\",\n    storage=\"sqlite:///results/optuna_studies.db\"\n)\n\n# Gr\u00e1ficos de optimizaci\u00f3n\nfig = optuna.visualization.plot_optimization_history(study)\nfig.show()\n\nfig = optuna.visualization.plot_param_importances(study)\nfig.show()\n</code></pre>"},{"location":"experiments/#logs-y-debugging","title":"Logs y Debugging","text":"<pre><code>import logging\n\n# Configurar logging detallado\nlogging.basicConfig(level=logging.INFO)\n\n# Ejecutar con logs verbosos\nrun_study(X, y, T, model_space=config,\n          save_path=\"results/experimento_debug\",\n          num_trials=5)  # Pocas iteraciones para debug\n</code></pre>"},{"location":"experiments/#solucion-de-problemas","title":"\ud83d\udea8 Soluci\u00f3n de Problemas","text":""},{"location":"experiments/#errores-comunes","title":"Errores Comunes","text":"<p>CUDA out of memory</p> <pre><code>config = {\n    \"batch_size\": 16,    # Reducir tama\u00f1o de lote\n    \"hidden_size\": (int, 64, 256, 32),  # Redes m\u00e1s peque\u00f1as\n    # ... otros par\u00e1metros\n}\n</code></pre> <p>Experimento muy lento</p> <pre><code>config = {\n    \"num_epochs\": 50,     # Menos \u00e9pocas por trial\n    \"es_patience\": 5,     # Early stopping m\u00e1s agresivo\n    # ... otros par\u00e1metros\n}\n\nrun_study(X, y, T, model_space=config,\n          num_trials=10,   # Menos trials\n          outer_folds=3)   # Menos folds\n</code></pre> <p>Resultados inconsistentes</p> <pre><code># Fijar semillas para reproducibilidad\nimport torch\nimport numpy as np\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nconfig = {\n    \"device\": \"cpu\",  # Para m\u00e1xima reproducibilidad\n    # ... otros par\u00e1metros\n}\n</code></pre>"},{"location":"experiments/#mejores-practicas","title":"\ud83d\udcc8 Mejores Pr\u00e1cticas","text":""},{"location":"experiments/#configuracion-de-produccion","title":"Configuraci\u00f3n de Producci\u00f3n","text":"<pre><code>production_config = {\n    \"model_class\": MLPModel,\n    \"output_size\": 1,\n    \"device\": \"cuda\",\n    \"num_epochs\": 300,\n    \"es_patience\": 15,\n    \"batch_size\": 64,\n\n    # Espacio de b\u00fasqueda bien definido\n    \"hidden_size\": (int, 128, 512, 32),\n    \"num_layers\": (int, 2, 4),\n    \"dropout\": (float, 0.1, 0.5),\n    \"lr\": (float, 1e-5, 1e-2, True),\n}\n\nrun_study(\n    X, y, T,\n    model_space=production_config,\n    num_trials=100,           # B\u00fasqueda exhaustiva\n    outer_folds=10,           # Evaluaci\u00f3n robusta\n    inner_folds=5,\n    save_path=\"results/production_experiment\",\n    search_alg=\"bayes\"\n)\n</code></pre>"},{"location":"experiments/#experimentos-en-lotes","title":"Experimentos en Lotes","text":"<pre><code>def batch_experiments():\n    datasets = [\n        'data/S19_withCtxt_flat.pickle',\n        'data/S20_withCtxt_flat.pickle', \n        'data/S21_withCtxt_flat.pickle'\n    ]\n\n    for dataset_path in datasets:\n        subject_id = dataset_path.split('/')[-1].split('_')[0]\n\n        with open(dataset_path, 'rb') as f:\n            X, y, T = pickle.load(f)\n\n        run_study(\n            X, y, T,\n            model_space=production_config,\n            num_trials=50,\n            outer_folds=5,\n            save_path=f\"results/batch_experiment/{subject_id}\"\n        )\n\n        print(f\"\u2705 Completado: {subject_id}\")\n\nbatch_experiments()\n</code></pre>"},{"location":"experiments/#proximos-pasos","title":"\ud83d\udcca Pr\u00f3ximos Pasos","text":"<p>Despu\u00e9s de ejecutar experimentos:</p> <ol> <li>Visualizar resultados \u2192 An\u00e1lisis y gr\u00e1ficos</li> <li>API Reference \u2192 Documentaci\u00f3n t\u00e9cnica detallada</li> <li>Ejemplos completos \u2192 Casos de uso avanzados</li> </ol>"},{"location":"installation/","title":"Instalaci\u00f3n","text":""},{"location":"installation/#requisitos-del-sistema","title":"Requisitos del Sistema","text":"<ul> <li>Python: 3.8 o superior</li> <li>Sistema Operativo: Windows, macOS, Linux</li> <li>GPU: Opcional (CUDA compatible para aceleraci\u00f3n)</li> </ul>"},{"location":"installation/#metodos-de-instalacion","title":"M\u00e9todos de Instalaci\u00f3n","text":""},{"location":"installation/#instalacion-recomendada-pip","title":"\ud83c\udfaf Instalaci\u00f3n Recomendada (pip)","text":"<pre><code>pip install ibioml\n</code></pre>"},{"location":"installation/#instalacion-desde-codigo-fuente","title":"\ud83d\udd27 Instalaci\u00f3n desde C\u00f3digo Fuente","text":"<p>Para obtener la versi\u00f3n m\u00e1s reciente con las \u00faltimas caracter\u00edsticas:</p> <pre><code># Clonar el repositorio\ngit clone https://github.com/tuusuario/IBioML.git\ncd IBioML\n\n# Crear entorno virtual (recomendado)\npython -m venv venv\nsource venv/bin/activate  # En Windows: venv\\Scripts\\activate\n\n# Instalar en modo desarrollo\npip install -e .\n</code></pre>"},{"location":"installation/#instalacion-con-conda","title":"\ud83d\udc0d Instalaci\u00f3n con Conda","text":"<pre><code># Crear entorno conda\nconda create -n ibioml python=3.9\nconda activate ibioml\n\n# Instalar dependencias principales\nconda install numpy pandas scikit-learn matplotlib seaborn scipy\n\n# Instalar PyTorch (ajustar seg\u00fan tu sistema)\nconda install pytorch torchvision torchaudio -c pytorch\n\n# Instalar IBioML\npip install ibioml\n</code></pre>"},{"location":"installation/#dependencias","title":"Dependencias","text":"<p>IBioML requiere las siguientes librer\u00edas:</p>"},{"location":"installation/#dependencias-principales","title":"Dependencias Principales","text":"Librer\u00eda Versi\u00f3n Prop\u00f3sito <code>numpy</code> &gt;=1.19.0 Operaciones num\u00e9ricas <code>pandas</code> &gt;=1.3.0 Manipulaci\u00f3n de datos <code>scikit-learn</code> &gt;=1.0.0 ML utilities y m\u00e9tricas <code>torch</code> &gt;=1.9.0 Deep learning framework <code>matplotlib</code> &gt;=3.3.0 Visualizaci\u00f3n b\u00e1sica <code>seaborn</code> &gt;=0.11.0 Visualizaci\u00f3n estad\u00edstica <code>scipy</code> &gt;=1.7.0 Operaciones cient\u00edficas"},{"location":"installation/#dependencias-opcionales","title":"Dependencias Opcionales","text":"<p>Para funcionalidades avanzadas:</p> <pre><code># Para optimizaci\u00f3n de hiperpar\u00e1metros\npip install optuna\n\n# Para documentaci\u00f3n interactiva\npip install jupyter ipywidgets\n\n# Para an\u00e1lisis estad\u00edsticos avanzados\npip install statsmodels\n</code></pre>"},{"location":"installation/#configuracion-del-entorno","title":"Configuraci\u00f3n del Entorno","text":""},{"location":"installation/#variables-de-entorno","title":"Variables de Entorno","text":"<p>Para un rendimiento \u00f3ptimo, configura estas variables:</p> <pre><code># Para usar GPU (si est\u00e1 disponible)\nexport CUDA_VISIBLE_DEVICES=0\n\n# Para reproducibilidad\nexport PYTHONHASHSEED=42\n</code></pre>"},{"location":"installation/#configuracion-de-gpu","title":"Configuraci\u00f3n de GPU","text":"<p>Para verificar que PyTorch detecta tu GPU:</p> <pre><code>import torch\n\nprint(f\"CUDA disponible: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"Dispositivos GPU: {torch.cuda.device_count()}\")\n    print(f\"GPU actual: {torch.cuda.get_device_name()}\")\n</code></pre>"},{"location":"installation/#verificacion-de-la-instalacion","title":"Verificaci\u00f3n de la Instalaci\u00f3n","text":"<p>Ejecuta este script para verificar que todo funciona correctamente:</p> <pre><code>import sys\nimport ibioml\n\nprint(\"\u2705 IBioML instalado correctamente!\")\nprint(f\"Versi\u00f3n: {ibioml.__version__}\")\nprint(f\"Python: {sys.version}\")\n\n# Verificar m\u00f3dulos principales\ntry:\n    from ibioml.models import MLPModel\n    from ibioml.preprocessing import preprocess_data\n    from ibioml.tuner import run_study\n    print(\"\u2705 Todos los m\u00f3dulos principales disponibles\")\nexcept ImportError as e:\n    print(f\"\u274c Error importando m\u00f3dulos: {e}\")\n\n# Verificar dependencias\ntry:\n    import torch\n    import numpy as np\n    import pandas as pd\n    import sklearn\n    print(\"\u2705 Todas las dependencias disponibles\")\nexcept ImportError as e:\n    print(f\"\u274c Faltan dependencias: {e}\")\n</code></pre>"},{"location":"installation/#configuracion-avanzada","title":"Configuraci\u00f3n Avanzada","text":""},{"location":"installation/#para-desarrollo","title":"Para Desarrollo","text":"<p>Si planeas contribuir al desarrollo de IBioML:</p> <pre><code># Instalar dependencias de desarrollo\npip install -e \".[dev]\"\n\n# O manualmente:\npip install pytest black flake8 mypy pre-commit\n</code></pre>"},{"location":"installation/#para-servidoreshpc","title":"Para Servidores/HPC","text":"<p>En clusters de computaci\u00f3n o servidores:</p> <pre><code># Instalaci\u00f3n sin dependencias de visualizaci\u00f3n\npip install ibioml --no-deps\npip install numpy pandas scikit-learn torch scipy\n\n# Para ambientes sin internet\npip download ibioml -d ./packages\npip install ./packages/*.whl --no-index --find-links ./packages\n</code></pre>"},{"location":"installation/#problemas-comunes","title":"Problemas Comunes","text":""},{"location":"installation/#error-no-module-named-ibioml","title":"Error: \"No module named 'ibioml'\"","text":"<pre><code># Verificar instalaci\u00f3n\npip list | grep ibioml\n\n# Reinstalar si es necesario\npip uninstall ibioml\npip install ibioml\n</code></pre>"},{"location":"installation/#error-cuda-out-of-memory","title":"Error: \"CUDA out of memory\"","text":"<pre><code># Reducir batch_size en la configuraci\u00f3n\nconfig = {\n    \"batch_size\": 16,  # En lugar de 32 o 64\n    # ... resto de configuraci\u00f3n\n}\n</code></pre>"},{"location":"installation/#error-permission-denied","title":"Error: \"Permission denied\"","text":"<pre><code># Instalar solo para el usuario actual\npip install --user ibioml\n</code></pre>"},{"location":"installation/#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<p>Una vez instalado IBioML exitosamente:</p> <ol> <li>\ud83d\udcda Lee la gu\u00eda de preprocesamiento</li> <li>\ud83d\ude80 Ejecuta tu primer experimento</li> <li>\ud83d\udcca Explora las opciones de visualizaci\u00f3n</li> <li>\ud83d\udcd6 Consulta la API reference para uso avanzado</li> </ol>"},{"location":"preprocessing/","title":"Preprocesamiento de Datos","text":"<p>IbioML incluye un sistema robusto de preprocesamiento espec\u00edficamente dise\u00f1ado para transformar datos neuronales en formato <code>.mat</code> de MATLAB en estructuras optimizadas para machine learning.</p>"},{"location":"preprocessing/#vision-general","title":"\ud83c\udfaf Visi\u00f3n General","text":"<p>El preprocesamiento en IbioML realiza las siguientes operaciones:</p> <ol> <li>Carga de datos desde archivos <code>.mat</code></li> <li>Incorporaci\u00f3n de contexto (informaci\u00f3n de recompensa)</li> <li>Creaci\u00f3n de ventanas temporales con historia de spikes</li> <li>Filtrado de calidad (rendimiento, duraci\u00f3n de trials)</li> <li>Limpieza neuronal (eliminaci\u00f3n de neuronas con baja actividad)</li> <li>Generaci\u00f3n de m\u00faltiples formatos de datos</li> </ol>"},{"location":"preprocessing/#estructura-de-datos-de-entrada","title":"\ud83d\udcca Estructura de Datos de Entrada","text":""},{"location":"preprocessing/#formato-del-archivo-mat","title":"Formato del Archivo .mat","text":"<p>Tu archivo <code>.mat</code> debe contener las siguientes variables:</p> Variable Dimensiones Descripci\u00f3n <code>neuronActivity</code> (time_bins, neurons) Actividad neuronal binaria <code>position</code> (time_bins, 1) Posici\u00f3n del sujeto <code>velocity</code> (time_bins, 1) Velocidad del sujeto <code>rewCtxt</code> (time_bins, 1) Contexto de recompensa (0/1) <code>trialFinalBin</code> (trials, 1) \u00daltimo bin de cada trial <code>dPrime</code> (trials, 1) Medida de rendimiento por trial <code>trialDurationInBins</code> (trials, 1) Duraci\u00f3n de cada trial"},{"location":"preprocessing/#configuracion-de-parametros","title":"\ud83d\udd27 Configuraci\u00f3n de Par\u00e1metros","text":""},{"location":"preprocessing/#parametros-principales","title":"Par\u00e1metros Principales","text":"<pre><code>from ibioml.preprocessing import preprocess_data\n\npreprocess_data(\n    file_path='datasets/mi_experimento.mat',\n    file_name_to_save='data/experimento_procesado',\n    bins_before=5,      # Ventana temporal hacia atr\u00e1s\n    bins_after=5,       # Ventana temporal hacia adelante  \n    bins_current=1,     # Bins del momento actual\n    threshDPrime=2.5,   # Umbral de rendimiento\n    firingMinimo=1000   # Spikes m\u00ednimos por neurona\n)\n</code></pre>"},{"location":"preprocessing/#descripcion-de-parametros","title":"Descripci\u00f3n de Par\u00e1metros","text":""},{"location":"preprocessing/#bins_before-y-bins_after","title":"<code>bins_before</code> y <code>bins_after</code>","text":"<p>Define la ventana temporal de contexto:</p> <pre><code># Ejemplo con bins_before=3, bins_after=2, bins_current=1\n# Para el bin t, se incluyen:\n# [t-3, t-2, t-1, t, t+1, t+2] -&gt; ventana de 6 bins total\n</code></pre> <p>Recomendaciones</p> <ul> <li>bins_before=5, bins_after=5: Para capturar contexto temporal amplio</li> <li>bins_before=3, bins_after=3: Para an\u00e1lisis m\u00e1s r\u00e1pidos</li> <li>bins_before=0, bins_after=0: Solo informaci\u00f3n instant\u00e1nea</li> </ul>"},{"location":"preprocessing/#threshdprime","title":"<code>threshDPrime</code>","text":"<p>Umbral de discriminabilidad para filtrar trials de baja calidad:</p> <ul> <li>2.0: Criterio permisivo (incluye m\u00e1s datos)</li> <li>2.5: Criterio balanceado (recomendado)</li> <li>3.0: Criterio estricto (solo trials de alta calidad)</li> </ul>"},{"location":"preprocessing/#firingminimo","title":"<code>firingMinimo</code>","text":"<p>N\u00famero m\u00ednimo de spikes que debe tener una neurona para ser incluida:</p> <ul> <li>500: Para datasets peque\u00f1os</li> <li>1000: Valor est\u00e1ndar recomendado</li> <li>2000: Para an\u00e1lisis que requieren alta actividad</li> </ul>"},{"location":"preprocessing/#organizacion-de-archivos-de-salida","title":"\ud83d\udcc1 Organizaci\u00f3n de Archivos de Salida","text":""},{"location":"preprocessing/#estructura-recomendada","title":"Estructura Recomendada","text":"<pre><code>data/\n\u251c\u2500\u2500 bins200ms/              # Resoluci\u00f3n temporal\n\u2502   \u251c\u2500\u2500 5_5_1/             # bins_before_after_current\n\u2502   \u2502   \u251c\u2500\u2500 2_5/           # threshold (2.5 -&gt; \"2_5\")\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 experimento_withCtxt_onlyPosition.pickle\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 experimento_withCtxt_onlyPosition_flat.pickle\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 experimento_withCtxt_onlyVelocity.pickle\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 experimento_withCtxt_onlyVelocity_flat.pickle\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 experimento_withCtxt_bothTargets.pickle\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 experimento_withCtxt_bothTargets_flat.pickle\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 experimento_onlyPosition.pickle\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 experimento_onlyPosition_flat.pickle\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 experimento_onlyVelocity.pickle\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 experimento_onlyVelocity_flat.pickle\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 experimento_bothTargets.pickle\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 experimento_bothTargets_flat.pickle\n\u2502   \u2502   \u2514\u2500\u2500 3_0/           # Otra configuraci\u00f3n de threshold\n\u2502   \u2514\u2500\u2500 3_3_1/             # Otra configuraci\u00f3n temporal\n\u2514\u2500\u2500 bins100ms/              # Otra resoluci\u00f3n temporal\n</code></pre>"},{"location":"preprocessing/#ejemplo-de-uso-con-organizacion","title":"Ejemplo de Uso con Organizaci\u00f3n","text":"<pre><code>preprocess_data(\n    file_path='datasets/sujeto_S19.mat',\n    file_name_to_save='data/bins200ms/5_5_1/2_5/S19_preprocessed',\n    bins_before=5,\n    bins_after=5,\n    bins_current=1,\n    threshDPrime=2.5,\n    firingMinimo=1000\n)\n</code></pre>"},{"location":"preprocessing/#archivos-generados","title":"\ud83d\udce6 Archivos Generados","text":""},{"location":"preprocessing/#variantes-de-datos","title":"Variantes de Datos","text":"<p>Cada ejecuci\u00f3n de <code>preprocess_data</code> genera 12 archivos organizados en dos grupos principales:</p> Con Contexto (<code>withCtxt</code>)Sin Contexto <p>Incluye informaci\u00f3n de contexto de recompensa:</p> <ul> <li><code>_withCtxt_onlyPosition.pickle</code> / <code>_withCtxt_onlyPosition_flat.pickle</code></li> <li><code>_withCtxt_onlyVelocity.pickle</code> / <code>_withCtxt_onlyVelocity_flat.pickle</code></li> <li><code>_withCtxt_bothTargets.pickle</code> / <code>_withCtxt_bothTargets_flat.pickle</code></li> </ul> <p>Solo informaci\u00f3n neuronal:</p> <ul> <li><code>_onlyPosition.pickle</code> / <code>_onlyPosition_flat.pickle</code></li> <li><code>_onlyVelocity.pickle</code> / <code>_onlyVelocity_flat.pickle</code></li> <li><code>_bothTargets.pickle</code> / <code>_bothTargets_flat.pickle</code></li> </ul>"},{"location":"preprocessing/#estructura-de-archivos","title":"Estructura de Archivos","text":"<p>Cada archivo <code>.pickle</code> contiene una tupla:</p> <pre><code>(X, y, trial_markers)\n</code></pre> <p>Donde: - X: Datos de entrada (actividad neuronal) - y: Variables objetivo (posici\u00f3n/velocidad) - trial_markers: Identificadores de trial para cada muestra</p>"},{"location":"preprocessing/#formatos-de-datos","title":"\ud83d\udd0d Formatos de Datos","text":""},{"location":"preprocessing/#datos-no-aplanados-para-rnns","title":"Datos No Aplanados (para RNNs)","text":"<pre><code># Estructura: (samples, time_bins, features)\nX.shape = (n_samples, bins_before + bins_current + bins_after, n_neurons)\n</code></pre> <p>Uso: Modelos recurrentes (RNN, LSTM, GRU)</p>"},{"location":"preprocessing/#datos-aplanados-para-mlps","title":"Datos Aplanados (para MLPs)","text":"<pre><code># Estructura: (samples, features_flattened)\nX_flat.shape = (n_samples, (bins_before + bins_current + bins_after) * n_neurons)\n</code></pre> <p>Uso: Modelos no recurrentes (MLP, SVM, etc.)</p>"},{"location":"preprocessing/#funciones-de-utilidad","title":"\ud83d\udd27 Funciones de Utilidad","text":""},{"location":"preprocessing/#visualizacion-de-calidad-de-datos","title":"Visualizaci\u00f3n de Calidad de Datos","text":"<pre><code>from ibioml.preprocessing import plot_trial_duration, plot_low_performance\n\n# Cargar datos para an\u00e1lisis\nmat_contents = io.loadmat('datasets/mi_experimento.mat')\n\n# Visualizar duraci\u00f3n de trials\nplot_trial_duration(mat_contents['trialDurationInBins'])\n\n# Visualizar rendimiento por trial\nplot_low_performance(mat_contents['dPrime'])\n</code></pre>"},{"location":"preprocessing/#verificacion-de-archivos-generados","title":"Verificaci\u00f3n de Archivos Generados","text":"<pre><code>import pickle\n\n# Cargar archivo para verificar\nwith open('data/experimento_withCtxt_flat.pickle', 'rb') as f:\n    X, y, trial_markers = pickle.load(f)\n\nprint(f\"Forma de X: {X.shape}\")\nprint(f\"Forma de y: {y.shape}\")\nprint(f\"N\u00famero de trials: {len(np.unique(trial_markers))}\")\nprint(f\"Samples por trial (promedio): {len(trial_markers) / len(np.unique(trial_markers)):.1f}\")\n</code></pre>"},{"location":"preprocessing/#optimizacion-de-rendimiento","title":"\u26a1 Optimizaci\u00f3n de Rendimiento","text":""},{"location":"preprocessing/#para-datasets-grandes","title":"Para Datasets Grandes","text":"<pre><code># Reducir memoria usando par\u00e1metros m\u00e1s restrictivos\npreprocess_data(\n    file_path='datasets/dataset_grande.mat',\n    file_name_to_save='data/dataset_optimizado',\n    bins_before=3,          # Ventana m\u00e1s peque\u00f1a\n    bins_after=3,\n    bins_current=1,\n    threshDPrime=3.0,       # Criterio m\u00e1s estricto\n    firingMinimo=2000       # Neuronas m\u00e1s activas\n)\n</code></pre>"},{"location":"preprocessing/#procesamiento-en-lotes","title":"Procesamiento en Lotes","text":"<pre><code>import os\n\n# Procesar m\u00faltiples archivos\ndatasets = ['S19.mat', 'S20.mat', 'S21.mat']\nbase_config = {\n    'bins_before': 5,\n    'bins_after': 5,\n    'bins_current': 1,\n    'threshDPrime': 2.5,\n    'firingMinimo': 1000\n}\n\nfor dataset in datasets:\n    subject_id = dataset.replace('.mat', '')\n    preprocess_data(\n        file_path=f'datasets/{dataset}',\n        file_name_to_save=f'data/bins200ms/5_5_1/2_5/{subject_id}_preprocessed',\n        **base_config\n    )\n    print(f\"\u2705 Procesado: {subject_id}\")\n</code></pre>"},{"location":"preprocessing/#solucion-de-problemas","title":"\ud83d\udea8 Soluci\u00f3n de Problemas","text":""},{"location":"preprocessing/#errores-comunes","title":"Errores Comunes","text":"<p>KeyError: 'neuronActivity'</p> <p>Verifica que tu archivo <code>.mat</code> contenga todas las variables requeridas.</p> <p>MemoryError durante el preprocesamiento</p> <ul> <li>Reduce <code>bins_before</code> y <code>bins_after</code></li> <li>Aumenta <code>firingMinimo</code> para filtrar m\u00e1s neuronas</li> <li>Procesa en lotes m\u00e1s peque\u00f1os</li> </ul> <p>Archivos vac\u00edos despu\u00e9s del filtrado</p> <ul> <li>Reduce <code>threshDPrime</code></li> <li>Verifica la calidad de tus datos de entrada</li> <li>Ajusta <code>firingMinimo</code> a un valor menor</li> </ul>"},{"location":"preprocessing/#verificacion-de-calidad","title":"Verificaci\u00f3n de Calidad","text":"<pre><code>def verificar_preprocesamiento(archivo_pickle):\n    with open(archivo_pickle, 'rb') as f:\n        X, y, T = pickle.load(f)\n\n    print(f\"\ud83d\udcca Resumen de {archivo_pickle}:\")\n    print(f\"   Muestras: {X.shape[0]:,}\")\n    print(f\"   Features: {X.shape[1] if len(X.shape)==2 else X.shape[1]*X.shape[2]:,}\")\n    print(f\"   Targets: {y.shape[1] if len(y.shape)&gt;1 else 1}\")\n    print(f\"   Trials \u00fanicos: {len(np.unique(T))}\")\n\n    # Verificar valores faltantes\n    if np.any(np.isnan(X)):\n        print(\"   \u26a0\ufe0f  Advertencia: Valores NaN en X\")\n    if np.any(np.isnan(y)):\n        print(\"   \u26a0\ufe0f  Advertencia: Valores NaN en y\")\n\n    print(\"   \u2705 Archivo v\u00e1lido\")\n\n# Verificar todos los archivos generados\narchivos = [\n    'data/experimento_withCtxt_flat.pickle',\n    'data/experimento_onlyPosition_flat.pickle'\n]\n\nfor archivo in archivos:\n    verificar_preprocesamiento(archivo)\n</code></pre>"},{"location":"preprocessing/#proximos-pasos","title":"\ud83d\udcc8 Pr\u00f3ximos Pasos","text":"<p>Una vez completado el preprocesamiento:</p> <ol> <li>Configurar experimentos \u2192 Aprende a usar los datos procesados</li> <li>API Reference \u2192 Documentaci\u00f3n detallada de funciones</li> <li>Ejemplos \u2192 Tutoriales paso a paso</li> </ol>"},{"location":"visualization/","title":"Visualizaci\u00f3n de Resultados","text":"<p>IBioML incluye un sistema de visualizaci\u00f3n moderno y flexible para analizar los resultados de tus experimentos de neurodecodificaci\u00f3n.</p>"},{"location":"visualization/#vision-general","title":"\ud83c\udfaf Visi\u00f3n General","text":"<p>El sistema de visualizaci\u00f3n de IBioML se basa en dos componentes principales:</p> <ol> <li><code>ExperimentResults</code>: Gesti\u00f3n eficiente de datos de experimentos</li> <li><code>Visualizer</code>: Generaci\u00f3n de gr\u00e1ficos y an\u00e1lisis visual</li> </ol>"},{"location":"visualization/#uso-basico","title":"\ud83d\ude80 Uso B\u00e1sico","text":""},{"location":"visualization/#visualizacion-de-un-solo-experimento","title":"Visualizaci\u00f3n de un Solo Experimento","text":"<pre><code>from ibioml.results import Visualizer\n\n# Crear visualizador para un experimento\nviz = Visualizer(\"results/mi_experimento/study_2024-01-15_14-30-25\")\n\n# Resumen r\u00e1pido del experimento\nviz.summary()\n\n# Boxplot de R\u00b2 scores\nax = viz.plot_r2_scores_boxplot()\nplt.show()\n</code></pre>"},{"location":"visualization/#comparacion-de-multiples-experimentos","title":"Comparaci\u00f3n de M\u00faltiples Experimentos","text":"<pre><code>from ibioml.results import MultiExperimentResults, MultiExperimentVisualizer\n\n# Cargar m\u00faltiples experimentos\nmulti_results = MultiExperimentResults(\"results/architecture_comparison\")\n\n# Crear visualizador comparativo\nmulti_viz = MultiExperimentVisualizer(multi_results)\n\n# Comparar R\u00b2 scores entre experimentos\nax = multi_viz.plot_r2_comparison_boxplot()\nplt.show()\n</code></pre>"},{"location":"visualization/#tipos-de-visualizaciones","title":"\ud83d\udcca Tipos de Visualizaciones","text":""},{"location":"visualization/#1-boxplots-de-rendimiento","title":"1. Boxplots de Rendimiento","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Visualizar distribuci\u00f3n de R\u00b2 scores\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Comparar diferentes configuraciones\nexperiments = {\n    'MLP con contexto': 'results/mlp_with_context',\n    'MLP sin contexto': 'results/mlp_no_context', \n    'LSTM con contexto': 'results/lstm_with_context',\n    'RNN con contexto': 'results/rnn_with_context'\n}\n\nfor i, (name, path) in enumerate(experiments.items()):\n    ax = axes[i//2, i%2]\n    viz = Visualizer(path)\n    viz.plot_r2_scores_boxplot(ax=ax)\n    ax.set_title(name)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"visualization/#2-predicciones-vs-valores-reales","title":"2. Predicciones vs. Valores Reales","text":"<pre><code># Scatter plot de predicciones vs. valores verdaderos\nviz = Visualizer(\"results/mi_experimento/study_2024-01-15_14-30-25\")\n\n# Para el primer fold\nfold_names = viz.results_loader.get_fold_names()\nif fold_names:\n    ax = viz.plot_predictions_vs_true(fold_names[0])\n    if ax:\n        plt.show()\n</code></pre>"},{"location":"visualization/#3-analisis-de-hiperparametros","title":"3. An\u00e1lisis de Hiperpar\u00e1metros","text":"<pre><code>import optuna\nimport matplotlib.pyplot as plt\n\n# Cargar estudio de Optuna (si disponible)\ndef plot_hyperparameter_analysis(study_path):\n    # Cargar estudio desde la base de datos de Optuna\n    study = optuna.load_study(\n        study_name=\"mi_estudio\",\n        storage=f\"sqlite:///{study_path}/optuna_study.db\"\n    )\n\n    # Historia de optimizaci\u00f3n\n    fig1 = optuna.visualization.plot_optimization_history(study)\n    fig1.show()\n\n    # Importancia de par\u00e1metros\n    fig2 = optuna.visualization.plot_param_importances(study)\n    fig2.show()\n\n    # Distribuci\u00f3n de par\u00e1metros\n    fig3 = optuna.visualization.plot_parallel_coordinate(study)\n    fig3.show()\n\n# plot_hyperparameter_analysis(\"results/mi_experimento\")\n</code></pre>"},{"location":"visualization/#4-curvas-de-aprendizaje","title":"4. Curvas de Aprendizaje","text":"<pre><code>def plot_learning_curves(results_path):\n    \"\"\"\n    Graficar curvas de entrenamiento y validaci\u00f3n por fold.\n    \"\"\"\n    viz = Visualizer(results_path)\n    fold_names = viz.results_loader.get_fold_names()\n\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.flatten()\n\n    for i, fold_name in enumerate(fold_names[:6]):  # M\u00e1ximo 6 folds\n        if i &gt;= len(axes):\n            break\n\n        fold_results = viz.results_loader.get_fold_summary(fold_name)\n\n        # Verificar si hay datos de curvas de aprendizaje\n        if 'train_losses' in fold_results and 'val_losses' in fold_results:\n            epochs = range(len(fold_results['train_losses']))\n\n            axes[i].plot(epochs, fold_results['train_losses'], \n                        label='Entrenamiento', color='blue', alpha=0.7)\n            axes[i].plot(epochs, fold_results['val_losses'], \n                        label='Validaci\u00f3n', color='red', alpha=0.7)\n            axes[i].set_title(f'{fold_name}')\n            axes[i].set_xlabel('\u00c9poca')\n            axes[i].set_ylabel('Loss')\n            axes[i].legend()\n            axes[i].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n# plot_learning_curves(\"results/mi_experimento/study_2024-01-15_14-30-25\")\n</code></pre>"},{"location":"visualization/#visualizaciones-avanzadas","title":"\ud83d\udcc8 Visualizaciones Avanzadas","text":""},{"location":"visualization/#1-heatmap-de-rendimiento-por-configuracion","title":"1. Heatmap de Rendimiento por Configuraci\u00f3n","text":"<pre><code>import seaborn as sns\nimport pandas as pd\n\ndef create_performance_heatmap(multi_results_path):\n    \"\"\"\n    Crear heatmap de rendimiento para m\u00faltiples experimentos.\n    \"\"\"\n    multi_results = MultiExperimentResults(multi_results_path)\n\n    # Recopilar datos de todos los experimentos\n    performance_data = []\n    for exp_name, exp_results in multi_results.experiments.items():\n        final_results = exp_results.final_results\n        performance_data.append({\n            'Experimento': exp_name,\n            'R\u00b2 Test': final_results.get('best_r2_score_test', 0),\n            'R\u00b2 Promedio': final_results.get('mean_r2_test', 0),\n            'Desviaci\u00f3n Est\u00e1ndar': final_results.get('std_r2_test', 0)\n        })\n\n    df = pd.DataFrame(performance_data)\n    df_pivot = df.set_index('Experimento')[['R\u00b2 Test', 'R\u00b2 Promedio', 'Desviaci\u00f3n Est\u00e1ndar']]\n\n    plt.figure(figsize=(10, 6))\n    sns.heatmap(df_pivot.T, annot=True, cmap='viridis', fmt='.3f')\n    plt.title('Rendimiento por Experimento')\n    plt.tight_layout()\n    plt.show()\n\n# create_performance_heatmap(\"results/architecture_comparison\")\n</code></pre>"},{"location":"visualization/#2-distribucion-de-errores","title":"2. Distribuci\u00f3n de Errores","text":"<pre><code>def plot_error_distribution(results_path, fold_name):\n    \"\"\"\n    Analizar distribuci\u00f3n de errores de predicci\u00f3n.\n    \"\"\"\n    viz = Visualizer(results_path)\n    data = viz.get_predictions_and_true_values(fold_name)\n\n    if data is None:\n        print(f\"No hay datos disponibles para {fold_name}\")\n        return\n\n    predictions, true_values = data\n    errors = predictions - true_values\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    # Histograma de errores\n    axes[0].hist(errors, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n    axes[0].set_title('Distribuci\u00f3n de Errores')\n    axes[0].set_xlabel('Error (Predicci\u00f3n - Real)')\n    axes[0].set_ylabel('Frecuencia')\n    axes[0].axvline(0, color='red', linestyle='--', alpha=0.7)\n\n    # Q-Q plot para normalidad\n    from scipy import stats\n    stats.probplot(errors, dist=\"norm\", plot=axes[1])\n    axes[1].set_title('Q-Q Plot (Normalidad)')\n\n    # Errores vs. valores predichos\n    axes[2].scatter(predictions, errors, alpha=0.5, s=10)\n    axes[2].set_xlabel('Valores Predichos')\n    axes[2].set_ylabel('Errores')\n    axes[2].set_title('Errores vs. Predicciones')\n    axes[2].axhline(0, color='red', linestyle='--', alpha=0.7)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Estad\u00edsticas del error\n    print(f\"\ud83d\udcca Estad\u00edsticas de Error para {fold_name}:\")\n    print(f\"   Error medio: {np.mean(errors):.4f}\")\n    print(f\"   Error est\u00e1ndar: {np.std(errors):.4f}\")\n    print(f\"   MAE: {np.mean(np.abs(errors)):.4f}\")\n    print(f\"   RMSE: {np.sqrt(np.mean(errors**2)):.4f}\")\n\n# fold_names = Visualizer(\"results/mi_experimento\").results_loader.get_fold_names()\n# plot_error_distribution(\"results/mi_experimento\", fold_names[0])\n</code></pre>"},{"location":"visualization/#3-analisis-temporal","title":"3. An\u00e1lisis Temporal","text":"<pre><code>def plot_temporal_analysis(results_path, fold_name):\n    \"\"\"\n    Analizar rendimiento a trav\u00e9s del tiempo (dentro de trials).\n    \"\"\"\n    viz = Visualizer(results_path)\n\n    # Cargar datos de predicciones y trial markers\n    with open('data/mi_experimento_flat.pickle', 'rb') as f:\n        _, _, trial_markers = pickle.load(f)\n\n    data = viz.get_predictions_and_true_values(fold_name)\n    if data is None:\n        return\n\n    predictions, true_values = data\n\n    # Calcular R\u00b2 por trial\n    unique_trials = np.unique(trial_markers)\n    trial_r2_scores = []\n\n    for trial in unique_trials:\n        trial_mask = trial_markers == trial\n        if np.sum(trial_mask) &gt; 10:  # Suficientes puntos en el trial\n            trial_pred = predictions[trial_mask]\n            trial_true = true_values[trial_mask]\n\n            # Calcular R\u00b2 para este trial\n            ss_res = np.sum((trial_true - trial_pred) ** 2)\n            ss_tot = np.sum((trial_true - np.mean(trial_true)) ** 2)\n            r2 = 1 - (ss_res / ss_tot) if ss_tot &gt; 0 else 0\n            trial_r2_scores.append(r2)\n        else:\n            trial_r2_scores.append(np.nan)\n\n    # Visualizar\n    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n\n    # R\u00b2 por trial\n    valid_scores = [s for s in trial_r2_scores if not np.isnan(s)]\n    axes[0].plot(range(len(trial_r2_scores)), trial_r2_scores, \n                 marker='o', alpha=0.7, markersize=3)\n    axes[0].axhline(np.mean(valid_scores), color='red', linestyle='--', \n                    label=f'Promedio: {np.mean(valid_scores):.3f}')\n    axes[0].set_title('R\u00b2 Score por Trial')\n    axes[0].set_xlabel('N\u00famero de Trial')\n    axes[0].set_ylabel('R\u00b2 Score')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    # Distribuci\u00f3n de R\u00b2 por trial\n    axes[1].hist(valid_scores, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n    axes[1].set_title('Distribuci\u00f3n de R\u00b2 por Trial')\n    axes[1].set_xlabel('R\u00b2 Score')\n    axes[1].set_ylabel('N\u00famero de Trials')\n    axes[1].axvline(np.mean(valid_scores), color='red', linestyle='--')\n\n    plt.tight_layout()\n    plt.show()\n\n# plot_temporal_analysis(\"results/mi_experimento\", fold_names[0])\n</code></pre>"},{"location":"visualization/#personalizacion-avanzada","title":"\ud83d\udee0\ufe0f Personalizaci\u00f3n Avanzada","text":""},{"location":"visualization/#temas-y-estilos","title":"Temas y Estilos","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configurar estilo global\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\n# O usar tema personalizado\ndef setup_publication_style():\n    \"\"\"Configurar estilo para publicaciones.\"\"\"\n    plt.rcParams.update({\n        'font.size': 12,\n        'axes.titlesize': 14,\n        'axes.labelsize': 12,\n        'xtick.labelsize': 10,\n        'ytick.labelsize': 10,\n        'legend.fontsize': 10,\n        'figure.titlesize': 16,\n        'lines.linewidth': 2,\n        'lines.markersize': 6\n    })\n\nsetup_publication_style()\n</code></pre>"},{"location":"visualization/#exportacion-de-graficos","title":"Exportaci\u00f3n de Gr\u00e1ficos","text":"<pre><code>def save_publication_figures(results_path, output_dir=\"figures\"):\n    \"\"\"\n    Generar y guardar figuras listas para publicaci\u00f3n.\n    \"\"\"\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n\n    viz = Visualizer(results_path)\n\n    # Figura 1: Boxplot principal\n    fig, ax = plt.subplots(figsize=(6, 4))\n    viz.plot_r2_scores_boxplot(ax=ax)\n    plt.savefig(f'{output_dir}/r2_boxplot.pdf', dpi=300, bbox_inches='tight')\n    plt.savefig(f'{output_dir}/r2_boxplot.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\n    # Figura 2: Predicciones vs. reales\n    fold_names = viz.results_loader.get_fold_names()\n    if fold_names:\n        fig, ax = plt.subplots(figsize=(6, 6))\n        viz.plot_predictions_vs_true(fold_names[0], ax=ax)\n        plt.savefig(f'{output_dir}/predictions_vs_true.pdf', dpi=300, bbox_inches='tight')\n        plt.savefig(f'{output_dir}/predictions_vs_true.png', dpi=300, bbox_inches='tight')\n        plt.close()\n\n    print(f\"\u2705 Figuras guardadas en '{output_dir}/'\")\n\n# save_publication_figures(\"results/mi_experimento\")\n</code></pre>"},{"location":"visualization/#dashboard-interactivo","title":"\ud83d\udcca Dashboard Interactivo","text":""},{"location":"visualization/#usando-plotly-para-interactividad","title":"Usando Plotly para Interactividad","text":"<pre><code>import plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\ndef create_interactive_dashboard(multi_results_path):\n    \"\"\"\n    Crear dashboard interactivo con Plotly.\n    \"\"\"\n    multi_results = MultiExperimentResults(multi_results_path)\n\n    # Recopilar datos para dashboard\n    all_data = []\n    for exp_name, exp_results in multi_results.experiments.items():\n        fold_names = exp_results.get_fold_names()\n        for fold_name in fold_names:\n            try:\n                fold_summary = exp_results.get_fold_summary(fold_name)\n                all_data.append({\n                    'Experimento': exp_name,\n                    'Fold': fold_name,\n                    'R2_Score': fold_summary.get('r2_score', 0),\n                    'Loss_Val': fold_summary.get('loss_val', 0)\n                })\n            except:\n                continue\n\n    df = pd.DataFrame(all_data)\n\n    # Crear subplots\n    fig = make_subplots(\n        rows=2, cols=2,\n        subplot_titles=('R\u00b2 por Experimento', 'Distribuci\u00f3n de R\u00b2', \n                       'Loss de Validaci\u00f3n', 'Comparaci\u00f3n Detallada'),\n        specs=[[{\"type\": \"box\"}, {\"type\": \"histogram\"}],\n               [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n    )\n\n    # Boxplot de R\u00b2\n    for exp in df['Experimento'].unique():\n        exp_data = df[df['Experimento'] == exp]\n        fig.add_trace(\n            go.Box(y=exp_data['R2_Score'], name=exp, showlegend=False),\n            row=1, col=1\n        )\n\n    # Histograma de R\u00b2\n    fig.add_trace(\n        go.Histogram(x=df['R2_Score'], nbinsx=20, showlegend=False),\n        row=1, col=2\n    )\n\n    # Scatter de R\u00b2 vs Loss\n    for exp in df['Experimento'].unique():\n        exp_data = df[df['Experimento'] == exp]\n        fig.add_trace(\n            go.Scatter(\n                x=exp_data['Loss_Val'], \n                y=exp_data['R2_Score'],\n                mode='markers',\n                name=exp,\n                text=exp_data['Fold'],\n                showlegend=False\n            ),\n            row=2, col=1\n        )\n\n    # Barplot promedio por experimento\n    exp_means = df.groupby('Experimento')['R2_Score'].mean()\n    fig.add_trace(\n        go.Bar(x=exp_means.index, y=exp_means.values, showlegend=False),\n        row=2, col=2\n    )\n\n    # Actualizar layout\n    fig.update_layout(\n        height=800,\n        title_text=\"Dashboard de Resultados de Neurodecodificaci\u00f3n\",\n        showlegend=True\n    )\n\n    # Mostrar dashboard\n    fig.show()\n\n    # Guardar como HTML\n    fig.write_html(\"dashboard_resultados.html\")\n    print(\"\u2705 Dashboard guardado como 'dashboard_resultados.html'\")\n\n# create_interactive_dashboard(\"results/architecture_comparison\")\n</code></pre>"},{"location":"visualization/#solucion-de-problemas","title":"\ud83d\udea8 Soluci\u00f3n de Problemas","text":""},{"location":"visualization/#errores-comunes","title":"Errores Comunes","text":"<p>No se encuentran datos de predicciones</p> <pre><code># Verificar estructura de resultados\nviz = Visualizer(\"results/mi_experimento\")\nfold_summary = viz.results_loader.get_fold_summary(\"fold_0\")\nprint(\"Claves disponibles:\", fold_summary.keys())\n</code></pre> <p>Gr\u00e1ficos vac\u00edos o con errores</p> <pre><code># Verificar datos antes de graficar\nr2_df = viz.get_r2_scores_dataframe()\nif r2_df.empty:\n    print(\"\u274c No hay datos de R\u00b2 disponibles\")\nelse:\n    print(\"\u2705 Datos encontrados:\", len(r2_df), \"filas\")\n</code></pre> <p>Problemas de memoria con datasets grandes</p> <pre><code># Limpiar cach\u00e9 regularmente\nviz.results_loader.clear_cache()\n\n# O usar submuestreo para visualizaci\u00f3n\nsample_size = min(1000, len(predictions))\nsample_idx = np.random.choice(len(predictions), sample_size, replace=False)\npred_sample = predictions[sample_idx]\ntrue_sample = true_values[sample_idx]\n</code></pre>"},{"location":"visualization/#proximos-pasos","title":"\ud83d\udcc8 Pr\u00f3ximos Pasos","text":"<ul> <li>API de Resultados \u2192 Documentaci\u00f3n t\u00e9cnica detallada</li> <li>Ejemplos Avanzados \u2192 Casos de uso complejos</li> <li>Contribuir \u2192 Agregar nuevas visualizaciones</li> </ul>"},{"location":"api/models/","title":"API Reference - Modelos","text":"<p>Esta secci\u00f3n documenta todos los modelos de machine learning disponibles en IBioML.</p>"},{"location":"api/models/#modelos-base","title":"Modelos Base","text":""},{"location":"api/models/#ibioml.models.BaseMLP","title":"ibioml.models.BaseMLP","text":"<pre><code>BaseMLP(input_size, hidden_size, num_layers, dropout)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Clase base MLP con funcionalidad compartida.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>El n\u00famero de caracter\u00edsticas de entrada de forma (batch_size, input_size).</p> required <code>hidden_size</code> <code>int</code> <p>El n\u00famero de neuronas en cada capa oculta.</p> required <code>num_layers</code> <code>int</code> <p>El n\u00famero de capas ocultas.</p> required <code>dropout</code> <code>float</code> <p>La probabilidad de dropout para regularizaci\u00f3n.</p> required Source code in <code>ibioml/models.py</code> <pre><code>def __init__(self, input_size, hidden_size, num_layers, dropout):\n    \"\"\"\n    Args:\n        input_size (int): El n\u00famero de caracter\u00edsticas de entrada de forma (batch_size, input_size).\n        hidden_size (int): El n\u00famero de neuronas en cada capa oculta.\n        num_layers (int): El n\u00famero de capas ocultas.\n        dropout (float): La probabilidad de dropout para regularizaci\u00f3n.\n    \"\"\"\n    super(BaseMLP, self).__init__()\n    # Crear capas compartidas\n    self.shared_layers = nn.Sequential()\n    self.shared_layers.append(nn.Linear(input_size, hidden_size))\n    self.shared_layers.append(nn.BatchNorm1d(hidden_size))\n    self.shared_layers.append(nn.ReLU())\n    self.shared_layers.append(nn.Dropout(dropout))\n\n    for _ in range(num_layers - 1):\n        self.shared_layers.append(nn.Linear(hidden_size, hidden_size))\n        self.shared_layers.append(nn.BatchNorm1d(hidden_size))\n        self.shared_layers.append(nn.ReLU())\n        self.shared_layers.append(nn.Dropout(dropout))\n</code></pre>"},{"location":"api/models/#ibioml.models.BaseRNN","title":"ibioml.models.BaseRNN","text":"<pre><code>BaseRNN(input_size, hidden_size, output_size, num_layers, dropout, rnn_type)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Clase RNN base con funcionalidad compartida.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>N\u00famero de caracter\u00edsticas de entrada.</p> required <code>hidden_size</code> <code>int</code> <p>N\u00famero de caracter\u00edsticas en el estado oculto.</p> required <code>output_size</code> <code>int</code> <p>N\u00famero de caracter\u00edsticas de salida.</p> required <code>num_layers</code> <code>int</code> <p>N\u00famero de capas recurrentes.</p> required <code>dropout</code> <code>float</code> <p>Probabilidad de dropout.</p> required <code>rnn_type</code> <code>str</code> <p>Tipo de RNN ('RNN', 'GRU', 'LSTM').</p> required Source code in <code>ibioml/models.py</code> <pre><code>def __init__(self, input_size, hidden_size, output_size, num_layers, dropout, rnn_type):\n    \"\"\"\n    Args:\n        input_size (int): N\u00famero de caracter\u00edsticas de entrada.\n        hidden_size (int): N\u00famero de caracter\u00edsticas en el estado oculto.\n        output_size (int): N\u00famero de caracter\u00edsticas de salida.\n        num_layers (int): N\u00famero de capas recurrentes.\n        dropout (float): Probabilidad de dropout.\n        rnn_type (str): Tipo de RNN ('RNN', 'GRU', 'LSTM').\n    \"\"\"\n    super(BaseRNN, self).__init__()\n    self.hidden_size = hidden_size\n    self.num_layers = num_layers\n\n    # Seleccionar el tipo de RNN\n    if rnn_type == \"RNN\":\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n    elif rnn_type == \"GRU\":\n        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n    elif rnn_type == \"LSTM\":\n        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n    else:\n        raise ValueError(f\"Tipo de RNN no soportado: {rnn_type}\")\n\n    self.batch_norm = nn.BatchNorm1d(hidden_size)\n    self.fc = nn.Linear(hidden_size, output_size)\n</code></pre>"},{"location":"api/models/#ibioml.models.BaseRNN.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Paso hacia adelante del modelo. Args:     x (torch.Tensor): Tensor de entrada de forma (batch_size, sequence_length, input_size).</p> <p>Returns:</p> Type Description <p>torch.Tensor: Tensor de salida de forma (batch_size, output_size).</p> Source code in <code>ibioml/models.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Paso hacia adelante del modelo.\n    Args:\n        x (torch.Tensor): Tensor de entrada de forma (batch_size, sequence_length, input_size).\n\n    Returns:\n        torch.Tensor: Tensor de salida de forma (batch_size, output_size).\n    \"\"\"\n    x = x.to(device)\n\n    # Establecer estados ocultos y de celda iniciales\n    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n    if isinstance(self.rnn, nn.LSTM):\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n        out, _ = self.rnn(x, (h0, c0))  # LSTM requiere (h0, c0)\n    else:\n        out, _ = self.rnn(x, h0)  # RNN y GRU solo requieren h0\n\n    # Aplicar normalizaci\u00f3n por lotes\n    out = self.batch_norm(out[:, -1, :])  # Usar el \u00faltimo paso de tiempo\n\n    # Decodificar el estado oculto del \u00faltimo paso de tiempo\n    out = self.fc(out)\n    return out\n</code></pre>"},{"location":"api/models/#modelos-de-neurodecodificacion","title":"Modelos de Neurodecodificaci\u00f3n","text":""},{"location":"api/models/#perceptron-multicapa-mlp","title":"Perceptr\u00f3n Multicapa (MLP)","text":""},{"location":"api/models/#ibioml.models.MLPModel","title":"ibioml.models.MLPModel","text":"<pre><code>MLPModel(input_size, hidden_size, output_size, num_layers, dropout)\n</code></pre> <p>               Bases: <code>BaseMLP</code></p> <p>Modelo MLP con salida \u00fanica.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>El n\u00famero de caracter\u00edsticas de entrada de forma (batch_size, input_size).</p> required <code>hidden_size</code> <code>int</code> <p>El n\u00famero de neuronas en cada capa oculta.</p> required <code>output_size</code> <code>int</code> <p>El n\u00famero de caracter\u00edsticas de salida.</p> required <code>num_layers</code> <code>int</code> <p>El n\u00famero de capas ocultas.</p> required <code>dropout</code> <code>float</code> <p>La probabilidad de dropout para regularizaci\u00f3n.</p> required Source code in <code>ibioml/models.py</code> <pre><code>def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n    \"\"\"\n    Args:\n        input_size (int): El n\u00famero de caracter\u00edsticas de entrada de forma (batch_size, input_size).\n        hidden_size (int): El n\u00famero de neuronas en cada capa oculta.\n        output_size (int): El n\u00famero de caracter\u00edsticas de salida.\n        num_layers (int): El n\u00famero de capas ocultas.\n        dropout (float): La probabilidad de dropout para regularizaci\u00f3n.\n    \"\"\"\n    super(MLPModel, self).__init__(input_size, hidden_size, num_layers, dropout)\n    self.output_layer = nn.Linear(hidden_size, output_size)\n</code></pre>"},{"location":"api/models/#ibioml.models.MLPModel.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Paso hacia adelante del modelo. Args:     x (torch.Tensor): El tensor de entrada de forma (batch_size, input_size).</p> <p>Returns:</p> Type Description <p>torch.Tensor: El tensor de salida despu\u00e9s de pasar por la red, de forma (batch_size, output_size).</p> Source code in <code>ibioml/models.py</code> <pre><code>def forward(self, x):\n    \"\"\" Paso hacia adelante del modelo.\n    Args:\n        x (torch.Tensor): El tensor de entrada de forma (batch_size, input_size).\n\n    Returns:\n        torch.Tensor: El tensor de salida despu\u00e9s de pasar por la red, de forma (batch_size, output_size).\n    \"\"\"\n    x = x.to(device)\n    features = self.shared_layers(x)\n    return self.output_layer(features)\n</code></pre>"},{"location":"api/models/#redes-neuronales-recurrentes","title":"Redes Neuronales Recurrentes","text":""},{"location":"api/models/#ibioml.models.RNNModel","title":"ibioml.models.RNNModel","text":"<pre><code>RNNModel(input_size, hidden_size, output_size, num_layers, dropout)\n</code></pre> <p>               Bases: <code>BaseRNN</code></p> <p>Modelo RNN.</p> Source code in <code>ibioml/models.py</code> <pre><code>def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n    super(RNNModel, self).__init__(input_size, hidden_size, output_size, num_layers, dropout, rnn_type=\"RNN\")\n</code></pre>"},{"location":"api/models/#ibioml.models.LSTMModel","title":"ibioml.models.LSTMModel","text":"<pre><code>LSTMModel(input_size, hidden_size, output_size, num_layers, dropout)\n</code></pre> <p>               Bases: <code>BaseRNN</code></p> <p>Modelo LSTM.</p> Source code in <code>ibioml/models.py</code> <pre><code>def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n    super(LSTMModel, self).__init__(input_size, hidden_size, output_size, num_layers, dropout, rnn_type=\"LSTM\")\n</code></pre>"},{"location":"api/models/#ibioml.models.GRUModel","title":"ibioml.models.GRUModel","text":"<pre><code>GRUModel(input_size, hidden_size, output_size, num_layers, dropout)\n</code></pre> <p>               Bases: <code>BaseRNN</code></p> <p>Modelo GRU.</p> Source code in <code>ibioml/models.py</code> <pre><code>def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n    super(GRUModel, self).__init__(input_size, hidden_size, output_size, num_layers, dropout, rnn_type=\"GRU\")\n</code></pre>"},{"location":"api/models/#modelos-dual-output","title":"Modelos Dual-Output","text":""},{"location":"api/models/#mlp-dual-output","title":"MLP Dual-Output","text":""},{"location":"api/models/#ibioml.models.DualOutputMLPModel","title":"ibioml.models.DualOutputMLPModel","text":"<pre><code>DualOutputMLPModel(input_size, hidden_size, output_size, num_layers, dropout)\n</code></pre> <p>               Bases: <code>BaseMLP</code></p> <p>Modelo MLP con dos cabezas de salida separadas.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>El n\u00famero de caracter\u00edsticas de entrada de forma (batch_size, input_size).</p> required <code>hidden_size</code> <code>int</code> <p>El n\u00famero de neuronas en cada capa oculta.</p> required <code>output_size</code> <code>int</code> <p>El n\u00famero de caracter\u00edsticas de salida por cabeza.</p> required <code>num_layers</code> <code>int</code> <p>El n\u00famero de capas ocultas.</p> required <code>dropout</code> <code>float</code> <p>La probabilidad de dropout para regularizaci\u00f3n.</p> required Source code in <code>ibioml/models.py</code> <pre><code>def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n    \"\"\"\n    Args:\n        input_size (int): El n\u00famero de caracter\u00edsticas de entrada de forma (batch_size, input_size).\n        hidden_size (int): El n\u00famero de neuronas en cada capa oculta.\n        output_size (int): El n\u00famero de caracter\u00edsticas de salida por cabeza.\n        num_layers (int): El n\u00famero de capas ocultas.\n        dropout (float): La probabilidad de dropout para regularizaci\u00f3n.\n    \"\"\"\n    super(DualOutputMLPModel, self).__init__(input_size, hidden_size, num_layers, dropout)\n    # Crear dos cabezas de salida separadas\n    self.output_head1 = nn.Linear(hidden_size, output_size)\n    self.output_head2 = nn.Linear(hidden_size, output_size)\n</code></pre>"},{"location":"api/models/#ibioml.models.DualOutputMLPModel.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Paso hacia adelante del modelo. Args:     x (torch.Tensor): El tensor de entrada de forma (batch_size, input_size).</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>(Tensor, Tensor)</code> <p>Dos tensores de salida, cada uno de forma (batch_size, output_size).</p> Source code in <code>ibioml/models.py</code> <pre><code>def forward(self, x):\n    \"\"\" Paso hacia adelante del modelo.\n    Args:\n        x (torch.Tensor): El tensor de entrada de forma (batch_size, input_size).\n\n    Returns:\n        tuple(torch.Tensor, torch.Tensor): Dos tensores de salida, cada uno de forma (batch_size, output_size).\n    \"\"\"\n    x = x.to(device)\n    features = self.shared_layers(x)\n    output1 = self.output_head1(features)\n    output2 = self.output_head2(features)\n    return output1, output2\n</code></pre>"},{"location":"api/models/#rnn-dual-output","title":"RNN Dual-Output","text":""},{"location":"api/models/#ibioml.models.DualOutputRNNModel","title":"ibioml.models.DualOutputRNNModel","text":"<pre><code>DualOutputRNNModel(input_size, hidden_size, output_size, num_layers, dropout)\n</code></pre> <p>               Bases: <code>DualOutputRNN</code></p> <p>Modelo RNN con doble salida.</p> Source code in <code>ibioml/models.py</code> <pre><code>def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n    super(DualOutputRNNModel, self).__init__(input_size, hidden_size, output_size, num_layers, dropout, rnn_type=\"RNN\")\n</code></pre>"},{"location":"api/models/#ibioml.models.DualOutputLSTMModel","title":"ibioml.models.DualOutputLSTMModel","text":"<pre><code>DualOutputLSTMModel(input_size, hidden_size, output_size, num_layers, dropout)\n</code></pre> <p>               Bases: <code>DualOutputRNN</code></p> <p>Modelo LSTM con doble salida.</p> Source code in <code>ibioml/models.py</code> <pre><code>def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n    super(DualOutputLSTMModel, self).__init__(input_size, hidden_size, output_size, num_layers, dropout, rnn_type=\"LSTM\")\n</code></pre>"},{"location":"api/models/#ibioml.models.DualOutputGRUModel","title":"ibioml.models.DualOutputGRUModel","text":"<pre><code>DualOutputGRUModel(input_size, hidden_size, output_size, num_layers, dropout)\n</code></pre> <p>               Bases: <code>DualOutputRNN</code></p> <p>Modelo GRU con doble salida.</p> Source code in <code>ibioml/models.py</code> <pre><code>def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n    super(DualOutputGRUModel, self).__init__(input_size, hidden_size, output_size, num_layers, dropout, rnn_type=\"GRU\")\n</code></pre>"},{"location":"api/models/#utilidades-de-modelos","title":"Utilidades de Modelos","text":""},{"location":"api/models/#ibioml.utils.model_factory.create_model_class","title":"ibioml.utils.model_factory.create_model_class","text":"<pre><code>create_model_class(base_model_class, y_dim)\n</code></pre> <p>Factory function que selecciona autom\u00e1ticamente entre versiones de salida \u00fanica o dual de un modelo basado en la dimensi\u00f3n de y.</p> <p>Parameters:</p> Name Type Description Default <code>base_model_class</code> <p>Clase base del modelo (MLPModel, RNNModel, etc.)</p> required <code>y_dim</code> <p>Dimensi\u00f3n de salida (1 o 2)</p> required <p>Returns:</p> Type Description <p>La clase de modelo apropiada</p> Source code in <code>ibioml/utils/model_factory.py</code> <pre><code>def create_model_class(base_model_class, y_dim):\n    \"\"\"\n    Factory function que selecciona autom\u00e1ticamente entre versiones\n    de salida \u00fanica o dual de un modelo basado en la dimensi\u00f3n de y.\n\n    Args:\n        base_model_class: Clase base del modelo (MLPModel, RNNModel, etc.)\n        y_dim: Dimensi\u00f3n de salida (1 o 2)\n\n    Returns:\n        La clase de modelo apropiada\n    \"\"\"\n    model_mapping = {\n        # Mapeo de modelos base a sus versiones dual output\n        MLPModel: DualOutputMLPModel,\n        RNNModel: DualOutputRNNModel,\n        GRUModel: DualOutputGRUModel,\n        LSTMModel: DualOutputLSTMModel\n    }\n\n    if y_dim == 2:\n        # Si y tiene dos dimensiones (posici\u00f3n y velocidad), usar dual output\n        if base_model_class in model_mapping:\n            return model_mapping[base_model_class]\n        else:\n            raise ValueError(f\"No existe versi\u00f3n DualOutput para {base_model_class.__name__}\")\n    else:\n        # Para una dimensi\u00f3n, usar el modelo base\n        return base_model_class\n</code></pre>"},{"location":"api/models/#ejemplos-de-uso","title":"Ejemplos de Uso","text":""},{"location":"api/models/#crear-un-mlp-basico","title":"Crear un MLP b\u00e1sico","text":"<pre><code>from ibioml.models import MLPModel\nimport torch\n\n# Configuraci\u00f3n del modelo\nmodel = MLPModel(\n    input_size=1000,     # Caracter\u00edsticas de entrada\n    hidden_size=256,     # Neuronas en capas ocultas\n    output_size=1,       # N\u00famero de salidas\n    num_layers=3,        # N\u00famero de capas ocultas\n    dropout=0.2          # Tasa de dropout\n)\n\n# Datos de ejemplo\nx = torch.randn(32, 1000)  # Batch de 32 muestras\noutput = model(x)\nprint(f\"Forma de salida: {output.shape}\")  # [32, 1]\n</code></pre>"},{"location":"api/models/#crear-un-lstm-para-datos-temporales","title":"Crear un LSTM para datos temporales","text":"<pre><code>from ibioml.models import LSTMModel\nimport torch\n\n# Configuraci\u00f3n del modelo\nmodel = LSTMModel(\n    input_size=100,      # Caracter\u00edsticas por timestep\n    hidden_size=128,     # Tama\u00f1o del estado oculto\n    output_size=2,       # Posici\u00f3n + velocidad\n    num_layers=2,        # Capas LSTM\n    dropout=0.3\n)\n\n# Datos de ejemplo (batch, sequence, features)\nx = torch.randn(16, 10, 100)  # 16 muestras, 10 timesteps, 100 features\noutput = model(x)\nprint(f\"Forma de salida: {output.shape}\")  # [16, 2]\n</code></pre>"},{"location":"api/models/#usar-la-factory-para-crear-modelos","title":"Usar la factory para crear modelos","text":"<pre><code>from ibioml.utils.model_factory import create_model_class\nfrom ibioml.models import MLPModel\n\n# Crear clase de modelo con configuraci\u00f3n espec\u00edfica\nModelClass = create_model_class(MLPModel, output_size=1)\n\n# Instanciar modelo\nmodel = ModelClass(\n    input_size=500,\n    hidden_size=256,\n    num_layers=2,\n    dropout=0.1\n)\n</code></pre>"},{"location":"api/preprocessing/","title":"Preprocesamiento de Datos","text":"<p>Esta secci\u00f3n documenta los m\u00f3dulos para el preprocesamiento de datos neuronales.</p>"},{"location":"api/preprocessing/#modulo-principal-de-preprocesamiento","title":"M\u00f3dulo Principal de Preprocesamiento","text":""},{"location":"api/preprocessing/#ibioml.preprocess_data","title":"ibioml.preprocess_data","text":""},{"location":"api/preprocessing/#ibioml.preprocess_data.add_context_to_data","title":"add_context_to_data","text":"<pre><code>add_context_to_data(neural_data, rewCtxt)\n</code></pre> <p>Agregamos el contexto a los datos</p> Source code in <code>ibioml/preprocess_data.py</code> <pre><code>def add_context_to_data(neural_data, rewCtxt):\n    \"\"\"\n    Agregamos el contexto a los datos\n    \"\"\"\n    rewCtxt_neg = np.logical_not(rewCtxt).astype(\"uint8\")\n    neural_data_with_ctxt = np.concatenate((neural_data, rewCtxt[:,np.newaxis], rewCtxt_neg[:,np.newaxis]), axis=1)\n    return neural_data_with_ctxt\n</code></pre>"},{"location":"api/preprocessing/#ibioml.preprocess_data.clean_neurons_by_low_firing_rate","title":"clean_neurons_by_low_firing_rate","text":"<pre><code>clean_neurons_by_low_firing_rate(X, firingMinimo)\n</code></pre> <p>Eliminamos las neuronas con pocos spikes</p> Source code in <code>ibioml/preprocess_data.py</code> <pre><code>def clean_neurons_by_low_firing_rate(X, firingMinimo):\n    \"\"\"\n    Eliminamos las neuronas con pocos spikes\n    \"\"\"\n    nd_sum = np.nansum(X[:,0,:], axis=0)\n    rmv_nrn_clean = np.where(nd_sum &lt; firingMinimo)\n    X = np.delete(X, rmv_nrn_clean, 2)\n    return X\n</code></pre>"},{"location":"api/preprocessing/#ibioml.preprocess_data.get_idx_by_high_trial_duration","title":"get_idx_by_high_trial_duration","text":"<pre><code>get_idx_by_high_trial_duration(trialDurationInBins, trialFinalBin)\n</code></pre> <p>Obtenemos los \u00edndices de los trials con duraci\u00f3n muy larga</p> Source code in <code>ibioml/preprocess_data.py</code> <pre><code>def get_idx_by_high_trial_duration(trialDurationInBins, trialFinalBin):\n    \"\"\"\n    Obtenemos los \u00edndices de los trials con duraci\u00f3n muy larga\n    \"\"\"\n    threshTrialDuration=np.mean(trialDurationInBins)+3*np.std(trialDurationInBins)\n    trialsTooLong= np.ravel(np.where(trialDurationInBins&gt;=threshTrialDuration))\n    print('El trial ', trialsTooLong , 'es muy largo')\n\n    indices_to_remove_trialDuration=[]\n    for trial in trialsTooLong:\n        if trial==0:\n            startInd=0\n        else:\n            startInd=trialFinalBin[trial-1]+1\n        endInd=trialFinalBin[trial]\n        indices_to_remove_trialDuration.extend(range(startInd,endInd))\n\n    return np.array(indices_to_remove_trialDuration)\n</code></pre>"},{"location":"api/preprocessing/#ibioml.preprocess_data.get_idx_by_low_performance","title":"get_idx_by_low_performance","text":"<pre><code>get_idx_by_low_performance(dPrime, trialFinalBin, threshold)\n</code></pre> <p>Obtenemos los \u00edndices de los trials con bajo rendimiento</p> Source code in <code>ibioml/preprocess_data.py</code> <pre><code>def get_idx_by_low_performance(dPrime, trialFinalBin, threshold):\n    \"\"\"\n    Obtenemos los \u00edndices de los trials con bajo rendimiento\n    \"\"\"\n    low_performance_trials_indices = np.where((dPrime &lt;= threshold) | (np.isnan(dPrime)))[0]\n\n    # Mostrar los \u00edndices de los trials\n    print(\"Trials con dPrime menor o igual a 2.8:\", low_performance_trials_indices)\n    print(\"Cantidad de trials:\", len(low_performance_trials_indices))\n\n    # Crear una lista para almacenar los \u00edndices de los time bins a eliminar\n    indices_to_remove_low_performance = []\n\n    for trial in low_performance_trials_indices:\n        if trial==0:\n            startInd=0\n        else:\n            startInd=trialFinalBin[trial-1]+1\n        endInd=trialFinalBin[trial] \n        indices_to_remove_low_performance.extend(range(startInd,endInd+1))  \n\n    return np.array(indices_to_remove_low_performance)\n</code></pre>"},{"location":"api/preprocessing/#ibioml.preprocess_data.load_data","title":"load_data","text":"<pre><code>load_data(file_path)\n</code></pre> <p>Cargamos los datos de un archivo .mat</p> Source code in <code>ibioml/preprocess_data.py</code> <pre><code>def load_data(file_path):\n    \"\"\"\n    Cargamos los datos de un archivo .mat\n    \"\"\"\n    mat_contents = io.loadmat(file_path)\n    neural_data = mat_contents['neuronActivity'].copy()\n    rewCtxt = mat_contents['rewCtxt'].copy()\n    trialFinalBin = np.ravel(mat_contents['trialFinalBin'].copy())\n    dPrime = np.ravel(mat_contents['dPrime'].copy())\n    criterion = np.ravel(mat_contents['criterion'].copy())\n    rewCtxt = rewCtxt.squeeze()\n    print(\"Shape del neural data:\", neural_data.shape)\n    print(\"Shape del neural data en Rewarded Context:\", neural_data[rewCtxt==1,:].shape)\n\n    # variables a decodificar\n    pos_binned = mat_contents['position'].copy()\n    vels_binned = mat_contents['velocity'].copy()\n\n    return mat_contents, neural_data, rewCtxt, trialFinalBin, dPrime, criterion, rewCtxt, pos_binned, vels_binned\n</code></pre>"},{"location":"api/preprocessing/#ibioml.preprocess_data.plot_low_performance","title":"plot_low_performance","text":"<pre><code>plot_low_performance(dPrime)\n</code></pre> <p>Plot the low performance</p> Source code in <code>ibioml/preprocess_data.py</code> <pre><code>def plot_low_performance(dPrime):\n    \"\"\"\n    Plot the low performance\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(dPrime, label='dPrime')\n    plt.axhline(2.5, color='r', linestyle='--', label='Umbral de rendimiento')\n    plt.xlabel('\u00cdndice del trial')\n    plt.ylabel('dPrime')\n    # x grid que marque 5 lugares equidistantes\n    plt.xticks(np.arange(0, len(dPrime), len(dPrime)//5))\n    plt.grid(True, axis='x')\n    plt.legend()\n    plt.show()\n</code></pre>"},{"location":"api/preprocessing/#ibioml.preprocess_data.plot_trial_duration","title":"plot_trial_duration","text":"<pre><code>plot_trial_duration(trialDurationInBins)\n</code></pre> <p>Plot the trial duration</p> Source code in <code>ibioml/preprocess_data.py</code> <pre><code>def plot_trial_duration(trialDurationInBins):\n    \"\"\"\n    Plot the trial duration\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(trialDurationInBins, label='Duraci\u00f3n de los trials')\n    plt.axhline(np.mean(trialDurationInBins) + 3 * np.std(trialDurationInBins), color='r', linestyle='--', label='Umbral de duraci\u00f3n')\n    plt.xlabel('\u00cdndice del trial')\n    plt.ylabel('Duraci\u00f3n del trial (bins)')\n    plt.show()\n</code></pre>"},{"location":"api/preprocessing/#ibioml.preprocess_data.process_history","title":"process_history","text":"<pre><code>process_history(neural_data, bins_before, bins_after, bins_current)\n</code></pre> <p>Procesamos la historia de los spikes</p> Source code in <code>ibioml/preprocess_data.py</code> <pre><code>def process_history(neural_data, bins_before, bins_after, bins_current):\n    \"\"\"\n    Procesamos la historia de los spikes\n    \"\"\"\n    X = get_spikes_with_history(neural_data, bins_before, bins_after, bins_current)\n    print(\"Shape del X:\", X.shape)\n    return X\n</code></pre>"},{"location":"api/preprocessing/#ibioml.preprocess_data.save_data","title":"save_data","text":"<pre><code>save_data(X, y, trial_markers=None, file_path=None)\n</code></pre> <p>Save the data in a pickle file with optional trial markers</p> Source code in <code>ibioml/preprocess_data.py</code> <pre><code>def save_data(X, y, trial_markers=None, file_path=None):\n    \"\"\"\n    Save the data in a pickle file with optional trial markers\n    \"\"\"\n    if trial_markers is not None:\n        with open(file_path, 'wb') as f:\n            pickle.dump((X, y, trial_markers), f)\n        print(\"Datos con marcadores de trial guardados en\", file_path)\n    else:\n        with open(file_path, 'wb') as f:\n            pickle.dump((X, y), f)\n        print(\"Datos guardados correctamente en\", file_path)\n</code></pre>"},{"location":"api/preprocessing/#funciones-de-preprocesamiento","title":"Funciones de Preprocesamiento","text":""},{"location":"api/preprocessing/#ibioml.utils.preprocessing_funcs","title":"ibioml.utils.preprocessing_funcs","text":""},{"location":"api/preprocessing/#ibioml.utils.preprocessing_funcs.bin_output","title":"bin_output","text":"<pre><code>bin_output(outputs, output_times, dt, wdw_start, wdw_end, downsample_factor=1)\n</code></pre> <p>Function that puts outputs into bins</p>"},{"location":"api/preprocessing/#ibioml.utils.preprocessing_funcs.bin_output--parameters","title":"Parameters","text":"<p>outputs: matrix of size \"number of times the output was recorded\" x \"number of features in the output\"     each entry in the matrix is the value of the output feature output_times: a vector of size \"number of times the output was recorded\"     each entry has the time the output was recorded dt: number (any format)     size of time bins wdw_start: number (any format)     the start time for binning the outputs wdw_end: number (any format)     the end time for binning the outputs downsample_factor: integer, optional, default=1     how much to downsample the outputs prior to binning     larger values will increase speed, but decrease precision</p>"},{"location":"api/preprocessing/#ibioml.utils.preprocessing_funcs.bin_output--returns","title":"Returns","text":"<p>outputs_binned: matrix of size \"number of time bins\" x \"number of features in the output\"     the average value of each output feature in every time bin</p> Source code in <code>ibioml/utils/preprocessing_funcs.py</code> <pre><code>def bin_output(outputs,output_times,dt,wdw_start,wdw_end,downsample_factor=1):\n    \"\"\"\n    Function that puts outputs into bins\n\n    Parameters\n    ----------\n    outputs: matrix of size \"number of times the output was recorded\" x \"number of features in the output\"\n        each entry in the matrix is the value of the output feature\n    output_times: a vector of size \"number of times the output was recorded\"\n        each entry has the time the output was recorded\n    dt: number (any format)\n        size of time bins\n    wdw_start: number (any format)\n        the start time for binning the outputs\n    wdw_end: number (any format)\n        the end time for binning the outputs\n    downsample_factor: integer, optional, default=1\n        how much to downsample the outputs prior to binning\n        larger values will increase speed, but decrease precision\n\n    Returns\n    -------\n    outputs_binned: matrix of size \"number of time bins\" x \"number of features in the output\"\n        the average value of each output feature in every time bin\n    \"\"\"\n\n    ###Downsample output###\n    #We just take 1 out of every \"downsample_factor\" values#\n    if downsample_factor!=1: #Don't downsample if downsample_factor=1\n        downsample_idxs=np.arange(0,output_times.shape[0],downsample_factor) #Get the idxs of values we are going to include after downsampling\n        outputs=outputs[downsample_idxs,:] #Get the downsampled outputs\n        output_times=output_times[downsample_idxs] #Get the downsampled output times\n\n    ###Put outputs into bins###\n    edges=np.arange(wdw_start,wdw_end,dt) #Get edges of time bins\n    num_bins=edges.shape[0]-1 #Number of bins\n    output_dim=outputs.shape[1] #Number of output features\n    outputs_binned=np.empty([num_bins,output_dim]) #Initialize matrix of binned outputs\n    #Loop through bins, and get the mean outputs in those bins\n    for i in range(num_bins): #Loop through bins\n        idxs=np.where((np.squeeze(output_times)&gt;=edges[i]) &amp; (np.squeeze(output_times)&lt;edges[i+1]))[0] #Indices to consider the output signal (when it's in the correct time range)\n        for j in range(output_dim): #Loop through output features\n            outputs_binned[i,j]=np.mean(outputs[idxs,j])\n\n    return outputs_binned\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.preprocessing_funcs.bin_spikes","title":"bin_spikes","text":"<pre><code>bin_spikes(spike_times, dt, wdw_start, wdw_end)\n</code></pre> <p>Function that puts spikes into bins</p>"},{"location":"api/preprocessing/#ibioml.utils.preprocessing_funcs.bin_spikes--parameters","title":"Parameters","text":"<p>spike_times: an array of arrays     an array of neurons. within each neuron's array is an array containing all the spike times of that neuron dt: number (any format)     size of time bins wdw_start: number (any format)     the start time for putting spikes in bins wdw_end: number (any format)     the end time for putting spikes in bins</p>"},{"location":"api/preprocessing/#ibioml.utils.preprocessing_funcs.bin_spikes--returns","title":"Returns","text":"<p>neural_data: a matrix of size \"number of time bins\" x \"number of neurons\"     the number of spikes in each time bin for each neuron</p> Source code in <code>ibioml/utils/preprocessing_funcs.py</code> <pre><code>def bin_spikes(spike_times,dt,wdw_start,wdw_end):\n    \"\"\"\n    Function that puts spikes into bins\n\n    Parameters\n    ----------\n    spike_times: an array of arrays\n        an array of neurons. within each neuron's array is an array containing all the spike times of that neuron\n    dt: number (any format)\n        size of time bins\n    wdw_start: number (any format)\n        the start time for putting spikes in bins\n    wdw_end: number (any format)\n        the end time for putting spikes in bins\n\n    Returns\n    -------\n    neural_data: a matrix of size \"number of time bins\" x \"number of neurons\"\n        the number of spikes in each time bin for each neuron\n    \"\"\"\n    edges=np.arange(wdw_start,wdw_end,dt) #Get edges of time bins\n    num_bins=edges.shape[0]-1 #Number of bins\n    num_neurons=spike_times.shape[0] #Number of neurons\n    neural_data=np.empty([num_bins,num_neurons]) #Initialize array for binned neural data\n    #Count number of spikes in each bin for each neuron, and put in array\n    for i in range(num_neurons):\n        neural_data[:,i]=np.histogram(spike_times[i],edges)[0]\n    return neural_data\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.preprocessing_funcs.create_trial_markers","title":"create_trial_markers","text":"<pre><code>create_trial_markers(trialFinalBin, total_length)\n</code></pre> <p>Crea un vector que indica a qu\u00e9 trial pertenece cada time bin</p> <p>Parameters:</p> Name Type Description Default <code>trialFinalBin</code> <p>\u00cdndices de los bins finales de cada trial</p> required <code>total_length</code> <p>Longitud total del conjunto de datos</p> required <p>Returns:</p> Type Description <p>Array donde cada elemento indica el n\u00famero de trial al que pertenece</p> Source code in <code>ibioml/utils/preprocessing_funcs.py</code> <pre><code>def create_trial_markers(trialFinalBin, total_length):\n    \"\"\"\n    Crea un vector que indica a qu\u00e9 trial pertenece cada time bin\n\n    Args:\n        trialFinalBin: \u00cdndices de los bins finales de cada trial\n        total_length: Longitud total del conjunto de datos\n\n    Returns:\n        Array donde cada elemento indica el n\u00famero de trial al que pertenece\n    \"\"\"\n    trial_markers = np.zeros(total_length, dtype=int)\n\n    for trial_idx in range(len(trialFinalBin)):\n        if trial_idx == 0:\n            start_idx = 0\n        else:\n            start_idx = trialFinalBin[trial_idx-1] + 1\n        end_idx = trialFinalBin[trial_idx]\n        trial_markers[start_idx:end_idx+1] = trial_idx\n\n    return trial_markers\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.preprocessing_funcs.get_spikes_with_history","title":"get_spikes_with_history","text":"<pre><code>get_spikes_with_history(neural_data, bins_before, bins_after, bins_current=1)\n</code></pre> <p>Function that creates the covariate matrix of neural activity</p>"},{"location":"api/preprocessing/#ibioml.utils.preprocessing_funcs.get_spikes_with_history--parameters","title":"Parameters","text":"<p>neural_data: a matrix of size \"number of time bins\" x \"number of neurons\"     the number of spikes in each time bin for each neuron bins_before: integer     How many bins of neural data prior to the output are used for decoding bins_after: integer     How many bins of neural data after the output are used for decoding bins_current: 0 or 1, optional, default=1     Whether to use the concurrent time bin of neural data for decoding</p>"},{"location":"api/preprocessing/#ibioml.utils.preprocessing_funcs.get_spikes_with_history--returns","title":"Returns","text":"<p>X: a matrix of size \"number of total time bins\" x \"number of surrounding time bins used for prediction\" x \"number of neurons\"     For every time bin, there are the firing rates of all neurons from the specified number of time bins before (and after)</p> Source code in <code>ibioml/utils/preprocessing_funcs.py</code> <pre><code>def get_spikes_with_history(neural_data,bins_before,bins_after,bins_current=1):\n    \"\"\"\n    Function that creates the covariate matrix of neural activity\n\n    Parameters\n    ----------\n    neural_data: a matrix of size \"number of time bins\" x \"number of neurons\"\n        the number of spikes in each time bin for each neuron\n    bins_before: integer\n        How many bins of neural data prior to the output are used for decoding\n    bins_after: integer\n        How many bins of neural data after the output are used for decoding\n    bins_current: 0 or 1, optional, default=1\n        Whether to use the concurrent time bin of neural data for decoding\n\n    Returns\n    -------\n    X: a matrix of size \"number of total time bins\" x \"number of surrounding time bins used for prediction\" x \"number of neurons\"\n        For every time bin, there are the firing rates of all neurons from the specified number of time bins before (and after)\n    \"\"\"\n\n    num_examples=neural_data.shape[0] #Number of total time bins we have neural data for\n    num_neurons=neural_data.shape[1] #Number of neurons\n    surrounding_bins=bins_before+bins_after+bins_current #Number of surrounding time bins used for prediction\n    X=np.empty([num_examples,surrounding_bins,num_neurons]) #Initialize covariate matrix with NaNs\n    #X[:] = np.NaN\n    X[:] = np.nan\n    #Loop through each time bin, and collect the spikes occurring in surrounding time bins\n    #Note that the first \"bins_before\" and last \"bins_after\" rows of X will remain filled with NaNs, since they don't get filled in below.\n    #This is because, for example, we cannot collect 10 time bins of spikes before time bin 8\n    start_idx=0\n    for i in range(num_examples-bins_before-bins_after): #The first bins_before and last bins_after bins don't get filled in\n        end_idx=start_idx+surrounding_bins; #The bins of neural data we will be including are between start_idx and end_idx (which will have length \"surrounding_bins\")\n        X[i+bins_before,:,:]=neural_data[start_idx:end_idx,:] #Put neural data from surrounding bins in X, starting at row \"bins_before\"\n        start_idx=start_idx+1;\n    return X\n</code></pre>"},{"location":"api/preprocessing/#escaladores-de-datos","title":"Escaladores de Datos","text":""},{"location":"api/preprocessing/#ibioml.utils.data_scaler","title":"ibioml.utils.data_scaler","text":""},{"location":"api/preprocessing/#ibioml.utils.data_scaler.DataScaler","title":"DataScaler","text":"<pre><code>DataScaler(feature_scaler, target_scaler)\n</code></pre> <p>Escalador completo que combina escaladores de caracter\u00edsticas y objetivos.</p> <p>Esta clase utiliza el patr\u00f3n de composici\u00f3n para delegar el escalado a las clases especializadas seg\u00fan las caracter\u00edsticas de los datos.</p> <p>Parameters:</p> Name Type Description Default <code>feature_scaler</code> <p>Escalador para caracter\u00edsticas (X)</p> required <code>target_scaler</code> <p>Escalador para objetivos (y)</p> required Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def __init__(self, feature_scaler, target_scaler):\n    \"\"\"\n    Args:\n        feature_scaler: Escalador para caracter\u00edsticas (X)\n        target_scaler: Escalador para objetivos (y)\n    \"\"\"\n    self.feature_scaler = feature_scaler\n    self.target_scaler = target_scaler\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.DataScaler.is_dual","title":"is_dual  <code>property</code>","text":"<pre><code>is_dual\n</code></pre> <p>Indica si el escalador de objetivos es dual.</p>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.DataScaler.inverse_transform","title":"inverse_transform","text":"<pre><code>inverse_transform(y_scaled)\n</code></pre> <p>Invierte la transformaci\u00f3n para recuperar valores originales.</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def inverse_transform(self, y_scaled):\n    \"\"\"Invierte la transformaci\u00f3n para recuperar valores originales.\"\"\"\n    return self.target_scaler.inverse_transform(y_scaled)\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.DataScaler.inverse_transform_pos","title":"inverse_transform_pos","text":"<pre><code>inverse_transform_pos(pos_scaled)\n</code></pre> <p>Invierte la transformaci\u00f3n para posici\u00f3n (s\u00f3lo si es DualTargetScaler).</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def inverse_transform_pos(self, pos_scaled):\n    \"\"\"Invierte la transformaci\u00f3n para posici\u00f3n (s\u00f3lo si es DualTargetScaler).\"\"\"\n    if hasattr(self.target_scaler, 'inverse_transform_pos'):\n        return self.target_scaler.inverse_transform_pos(pos_scaled)\n    raise AttributeError(\"El escalador no soporta inverse_transform_pos\")\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.DataScaler.inverse_transform_vel","title":"inverse_transform_vel","text":"<pre><code>inverse_transform_vel(vel_scaled)\n</code></pre> <p>Invierte la transformaci\u00f3n para velocidad (s\u00f3lo si es DualTargetScaler).</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def inverse_transform_vel(self, vel_scaled):\n    \"\"\"Invierte la transformaci\u00f3n para velocidad (s\u00f3lo si es DualTargetScaler).\"\"\"\n    if hasattr(self.target_scaler, 'inverse_transform_vel'):\n        return self.target_scaler.inverse_transform_vel(vel_scaled)\n    raise AttributeError(\"El escalador no soporta inverse_transform_vel\")\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.DataScaler.standardize","title":"standardize","text":"<pre><code>standardize(X_train, y_train, X_test, y_test)\n</code></pre> <p>Estandariza caracter\u00edsticas y objetivos de entrenamiento y prueba.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def standardize(self, X_train, y_train, X_test, y_test):\n    \"\"\"\n    Estandariza caracter\u00edsticas y objetivos de entrenamiento y prueba.\n\n    Returns:\n        tuple: (X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n    \"\"\"\n    # Escalar caracter\u00edsticas\n    X_train_scaled = self.feature_scaler.fit_transform(X_train)\n    X_test_scaled = self.feature_scaler.transform(X_test)\n\n    # Escalar objetivos\n    y_train_scaled = self.target_scaler.fit_transform(y_train)\n    y_test_scaled = self.target_scaler.transform(y_test)\n\n    return X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.DualTargetScaler","title":"DualTargetScaler","text":"<pre><code>DualTargetScaler()\n</code></pre> <p>               Bases: <code>TargetScaler</code></p> <p>Escalador para dos objetivos (posici\u00f3n y velocidad).</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def __init__(self):\n    # Crear escaladores independientes para posici\u00f3n y velocidad\n    self.pos_scaler = StandardScaler()\n    self.vel_scaler = StandardScaler()\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.DualTargetScaler.fit_transform","title":"fit_transform","text":"<pre><code>fit_transform(y_train)\n</code></pre> <p>Ajusta ambos escaladores y transforma los objetivos duales.</p> <p>Parameters:</p> Name Type Description Default <code>y_train</code> <p>Array de forma (n_samples, 2) con posici\u00f3n en [:, 0] y velocidad en [:, 1]</p> required <p>Returns:</p> Type Description <p>Array escalado de la misma forma</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def fit_transform(self, y_train):\n    \"\"\"\n    Ajusta ambos escaladores y transforma los objetivos duales.\n\n    Args:\n        y_train: Array de forma (n_samples, 2) con posici\u00f3n en [:, 0] y velocidad en [:, 1]\n\n    Returns:\n        Array escalado de la misma forma\n    \"\"\"\n    # Extraer y escalar posici\u00f3n (primera columna)\n    pos = y_train[:, 0].reshape(-1, 1)\n    pos_scaled = self.pos_scaler.fit_transform(pos)\n\n    # Extraer y escalar velocidad (segunda columna)\n    vel = y_train[:, 1].reshape(-1, 1)\n    vel_scaled = self.vel_scaler.fit_transform(vel)\n\n    # Combinar resultados\n    return np.hstack([pos_scaled, vel_scaled])\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.DualTargetScaler.inverse_transform","title":"inverse_transform","text":"<pre><code>inverse_transform(y_scaled)\n</code></pre> <p>Invierte la transformaci\u00f3n para ambos objetivos.</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def inverse_transform(self, y_scaled):\n    \"\"\"Invierte la transformaci\u00f3n para ambos objetivos.\"\"\"\n    # Si es una lista de dos arrays separados\n    if isinstance(y_scaled, list) and len(y_scaled) == 2:\n        pos_scaled, vel_scaled = y_scaled\n\n        if pos_scaled.ndim == 1:\n            pos_scaled = pos_scaled.reshape(-1, 1)\n        if vel_scaled.ndim == 1:\n            vel_scaled = vel_scaled.reshape(-1, 1)\n\n        pos = self.pos_scaler.inverse_transform(pos_scaled)\n        vel = self.vel_scaler.inverse_transform(vel_scaled)\n\n        return np.hstack([pos, vel])\n\n    # Si es un \u00fanico array con ambas columnas\n    pos_scaled = y_scaled[:, 0].reshape(-1, 1)\n    vel_scaled = y_scaled[:, 1].reshape(-1, 1)\n\n    pos = self.pos_scaler.inverse_transform(pos_scaled)\n    vel = self.vel_scaler.inverse_transform(vel_scaled)\n\n    return np.hstack([pos, vel])\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.DualTargetScaler.inverse_transform_pos","title":"inverse_transform_pos","text":"<pre><code>inverse_transform_pos(pos_scaled)\n</code></pre> <p>Invierte la transformaci\u00f3n s\u00f3lo para posici\u00f3n.</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def inverse_transform_pos(self, pos_scaled):\n    \"\"\"Invierte la transformaci\u00f3n s\u00f3lo para posici\u00f3n.\"\"\"\n    if pos_scaled.ndim == 1:\n        pos_scaled = pos_scaled.reshape(-1, 1)\n    return self.pos_scaler.inverse_transform(pos_scaled)\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.DualTargetScaler.inverse_transform_vel","title":"inverse_transform_vel","text":"<pre><code>inverse_transform_vel(vel_scaled)\n</code></pre> <p>Invierte la transformaci\u00f3n s\u00f3lo para velocidad.</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def inverse_transform_vel(self, vel_scaled):\n    \"\"\"Invierte la transformaci\u00f3n s\u00f3lo para velocidad.\"\"\"\n    if vel_scaled.ndim == 1:\n        vel_scaled = vel_scaled.reshape(-1, 1)\n    return self.vel_scaler.inverse_transform(vel_scaled)\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.DualTargetScaler.transform","title":"transform","text":"<pre><code>transform(y)\n</code></pre> <p>Transforma los datos de posici\u00f3n y velocidad.</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def transform(self, y):\n    \"\"\"Transforma los datos de posici\u00f3n y velocidad.\"\"\"\n    # Extraer y escalar posici\u00f3n y velocidad\n    pos = y[:, 0].reshape(-1, 1)\n    vel = y[:, 1].reshape(-1, 1)\n\n    pos_scaled = self.pos_scaler.transform(pos)\n    vel_scaled = self.vel_scaler.transform(vel)\n\n    return np.hstack([pos_scaled, vel_scaled])\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.FeatureScaler","title":"FeatureScaler","text":"<pre><code>FeatureScaler()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Clase base abstracta para escaladores de caracter\u00edsticas.</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def __init__(self):\n    self.x_scaler = StandardScaler()\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.FeatureScaler.fit_transform","title":"fit_transform  <code>abstractmethod</code>","text":"<pre><code>fit_transform(X_train)\n</code></pre> <p>Ajusta el escalador y transforma los datos de entrenamiento.</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>@abstractmethod\ndef fit_transform(self, X_train):\n    \"\"\"Ajusta el escalador y transforma los datos de entrenamiento.\"\"\"\n    pass\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.FeatureScaler.transform","title":"transform  <code>abstractmethod</code>","text":"<pre><code>transform(X_unscaled)\n</code></pre> <p>Transforma los datos no escalados.</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>@abstractmethod\ndef transform(self, X_unscaled):\n    \"\"\"Transforma los datos no escalados.\"\"\"\n    pass\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.SingleTargetScaler","title":"SingleTargetScaler","text":"<pre><code>SingleTargetScaler()\n</code></pre> <p>               Bases: <code>TargetScaler</code></p> <p>Escalador para un \u00fanico objetivo.</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def __init__(self):\n    self.scaler = StandardScaler()\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.TargetScaler","title":"TargetScaler","text":"<p>               Bases: <code>ABC</code></p> <p>Clase base abstracta para escaladores de objetivos.</p>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.TargetScaler.fit_transform","title":"fit_transform  <code>abstractmethod</code>","text":"<pre><code>fit_transform(y_train)\n</code></pre> <p>Ajusta el escalador y transforma los objetivos de entrenamiento.</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>@abstractmethod\ndef fit_transform(self, y_train):\n    \"\"\"Ajusta el escalador y transforma los objetivos de entrenamiento.\"\"\"\n    pass\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.TargetScaler.inverse_transform","title":"inverse_transform  <code>abstractmethod</code>","text":"<pre><code>inverse_transform(y_scaled)\n</code></pre> <p>Invierte la transformaci\u00f3n para recuperar los valores originales.</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>@abstractmethod\ndef inverse_transform(self, y_scaled):\n    \"\"\"Invierte la transformaci\u00f3n para recuperar los valores originales.\"\"\"\n    pass\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.TargetScaler.transform","title":"transform  <code>abstractmethod</code>","text":"<pre><code>transform(y)\n</code></pre> <p>Transforma los objetivos usando el escalador ajustado.</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>@abstractmethod\ndef transform(self, y):\n    \"\"\"Transforma los objetivos usando el escalador ajustado.\"\"\"\n    pass\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.ThreeDFeatureScaler","title":"ThreeDFeatureScaler","text":"<pre><code>ThreeDFeatureScaler()\n</code></pre> <p>               Bases: <code>FeatureScaler</code></p> <p>Escalador para caracter\u00edsticas 3D (secuencias).</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def __init__(self):\n    self.x_scaler = StandardScaler()\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.TwoDFeatureScaler","title":"TwoDFeatureScaler","text":"<pre><code>TwoDFeatureScaler()\n</code></pre> <p>               Bases: <code>FeatureScaler</code></p> <p>Escalador para caracter\u00edsticas 2D.</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def __init__(self):\n    self.x_scaler = StandardScaler()\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.create_scaler","title":"create_scaler","text":"<pre><code>create_scaler(X, y)\n</code></pre> <p>Crea un escalador adecuado basado en las caracter\u00edsticas de los datos.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Datos de caracter\u00edsticas</p> required <code>y</code> <p>Datos de objetivos</p> required <p>Returns:</p> Name Type Description <code>DataScaler</code> <p>Escalador configurado para los datos</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def create_scaler(X, y):\n    \"\"\"\n    Crea un escalador adecuado basado en las caracter\u00edsticas de los datos.\n\n    Args:\n        X: Datos de caracter\u00edsticas\n        y: Datos de objetivos\n\n    Returns:\n        DataScaler: Escalador configurado para los datos\n    \"\"\"\n    # Determinar el tipo de escalador de caracter\u00edsticas\n    if X.ndim == 3:\n        feature_scaler = ThreeDFeatureScaler()\n    else:\n        feature_scaler = TwoDFeatureScaler()\n\n    # Determinar el tipo de escalador de objetivos\n    if y.ndim &gt; 1 and y.shape[1] == 2:\n        target_scaler = DualTargetScaler()\n    else:\n        target_scaler = SingleTargetScaler()\n\n    # Crear y devolver el escalador completo\n    return DataScaler(feature_scaler, target_scaler)\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.data_scaler.scale_data","title":"scale_data","text":"<pre><code>scale_data(X_train, y_train, X_test, y_test, return_scaler=False)\n</code></pre> <p>Escala los datos de forma autom\u00e1tica seg\u00fan sus caracter\u00edsticas.</p> <p>Parameters:</p> Name Type Description Default <code>X_train,</code> <code>(y_train, X_test, y_test)</code> <p>Datos a escalar</p> required <code>return_scaler</code> <p>Si True, devuelve tambi\u00e9n el escalador</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Datos escalados y opcionalmente el escalador</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def scale_data(X_train, y_train, X_test, y_test, return_scaler=False):\n    \"\"\"\n    Escala los datos de forma autom\u00e1tica seg\u00fan sus caracter\u00edsticas.\n\n    Args:\n        X_train, y_train, X_test, y_test: Datos a escalar\n        return_scaler: Si True, devuelve tambi\u00e9n el escalador\n\n    Returns:\n        tuple: Datos escalados y opcionalmente el escalador\n    \"\"\"\n    # Crear el escalador adecuado\n    scaler = create_scaler(X_train, y_train)\n\n    # Escalar los datos\n    X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled = scaler.standardize(\n        X_train, y_train, X_test, y_test\n    )\n\n    if return_scaler:\n        return X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, scaler\n\n    return X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled\n</code></pre>"},{"location":"api/preprocessing/#separadores-de-datos","title":"Separadores de Datos","text":""},{"location":"api/preprocessing/#ibioml.utils.splitters","title":"ibioml.utils.splitters","text":""},{"location":"api/preprocessing/#ibioml.utils.splitters.TrialKFold","title":"TrialKFold","text":"<pre><code>TrialKFold(n_splits=5, shuffle=False, random_state=None)\n</code></pre> <p>Implementaci\u00f3n de K-Fold cross-validation que respeta los l\u00edmites de trials.</p> <p>Esta clase asegura que todos los time bins pertenecientes a un mismo trial permanezcan en el mismo conjunto (entrenamiento o prueba) durante el proceso de validaci\u00f3n cruzada.</p>"},{"location":"api/preprocessing/#ibioml.utils.splitters.TrialKFold--parametros","title":"Par\u00e1metros:","text":"<p>n_splits : int     N\u00famero de folds. Debe ser al menos 2. shuffle : bool, default=False     Si es True, los trials son mezclados antes de dividirlos. random_state : int, RandomState, default=None     Controla la aleatoriedad de la mezcla.</p> Source code in <code>ibioml/utils/splitters.py</code> <pre><code>def __init__(self, n_splits=5, shuffle=False, random_state=None):\n    self.n_splits = n_splits\n    self.shuffle = shuffle\n    self.random_state = random_state\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.splitters.TrialKFold.split","title":"split","text":"<pre><code>split(trial_markers)\n</code></pre> <p>Genera \u00edndices para dividir los datos en conjuntos de entrenamiento y prueba.</p>"},{"location":"api/preprocessing/#ibioml.utils.splitters.TrialKFold.split--parametros","title":"Par\u00e1metros:","text":"<p>X : array-like     Datos de entrenamiento. y : array-like, opcional     Etiquetas. trial_markers : array-like, obligatorio     Array que indica a qu\u00e9 trial pertenece cada muestra.</p>"},{"location":"api/preprocessing/#ibioml.utils.splitters.TrialKFold.split--retorna","title":"Retorna:","text":"<p>train_indices : array     \u00cdndices de las muestras en el conjunto de entrenamiento. test_indices : array     \u00cdndices de las muestras en el conjunto de prueba.</p> Source code in <code>ibioml/utils/splitters.py</code> <pre><code>def split(self, trial_markers):\n    \"\"\"\n    Genera \u00edndices para dividir los datos en conjuntos de entrenamiento y prueba.\n\n    Par\u00e1metros:\n    -----------\n    X : array-like\n        Datos de entrenamiento.\n    y : array-like, opcional\n        Etiquetas.\n    trial_markers : array-like, obligatorio\n        Array que indica a qu\u00e9 trial pertenece cada muestra.\n\n    Retorna:\n    --------\n    train_indices : array\n        \u00cdndices de las muestras en el conjunto de entrenamiento.\n    test_indices : array\n        \u00cdndices de las muestras en el conjunto de prueba.\n    \"\"\"\n    # Obtenemos los IDs de trials \u00fanicos\n    unique_trials = np.unique(trial_markers)\n    n_trials = len(unique_trials)\n\n    if n_trials &lt; self.n_splits:\n        raise ValueError(\n            f\"El n\u00famero de trials ({n_trials}) debe ser al menos \"\n            f\"igual al n\u00famero de folds ({self.n_splits})\"\n        )\n\n    # Mezclamos los trials si shuffle=True\n    if self.shuffle:\n        random_state = check_random_state(self.random_state)\n        shuffled_trials = random_state.permutation(unique_trials)\n    else:\n        shuffled_trials = unique_trials\n\n    # Dividimos los trials en n_splits grupos aproximadamente iguales\n    trial_folds = np.array_split(shuffled_trials, self.n_splits)\n\n    # Generamos los \u00edndices para cada fold\n    for i in range(self.n_splits):\n        # Los trials para el conjunto de prueba son los del fold actual\n        test_trials = trial_folds[i]\n        # Creamos una m\u00e1scara para los \u00edndices de prueba\n        test_mask = np.isin(trial_markers, test_trials)\n        test_indices = np.where(test_mask)[0]\n\n        # Los trials para entrenamiento son todos los dem\u00e1s\n        train_mask = ~test_mask\n        train_indices = np.where(train_mask)[0]\n\n        yield train_indices, test_indices\n</code></pre>"},{"location":"api/preprocessing/#ibioml.utils.splitters.trial_train_test_split","title":"trial_train_test_split","text":"<pre><code>trial_train_test_split(X, y, trial_markers, train_size=0.8, random_state=None, shuffle=True, return_mask=False)\n</code></pre> <p>Dividir los datos en conjuntos de entrenamiento y prueba manteniendo los trials intactos.</p>"},{"location":"api/preprocessing/#ibioml.utils.splitters.trial_train_test_split--parametros","title":"Par\u00e1metros:","text":"<p>X : array-like     Datos de entrenamiento. y : array-like     Etiquetas. trial_markers : array-like     Array que indica a qu\u00e9 trial pertenece cada muestra. train_size : float, default=0.8     Proporci\u00f3n de datos para el conjunto de entrenamiento. random_state : int, RandomState, default=None     Controla la aleatoriedad de la mezcla. shuffle : bool, default=True     Si es True, los trials son mezclados antes de dividirlos. return_mask : bool, default=False     Si es True, tambi\u00e9n retorna las m\u00e1scaras de \u00edndices de entrenamiento y prueba.</p>"},{"location":"api/preprocessing/#ibioml.utils.splitters.trial_train_test_split--retorna","title":"Retorna:","text":"<p>X_train : array-like     Datos de entrenamiento. X_test : array-like     Datos de prueba. y_train : array-like     Etiquetas de entrenamiento.  y_test : array-like     Etiquetas de prueba. train_mask : array-like, opcional     M\u00e1scara de \u00edndices de entrenamiento. Solo se retorna si return_mask=True. test_mask : array-like, opcional     M\u00e1scara de \u00edndices de prueba. Solo se retorna si return_mask=True.</p> Source code in <code>ibioml/utils/splitters.py</code> <pre><code>def trial_train_test_split(X, y, trial_markers, train_size=0.8, random_state=None, shuffle=True, return_mask=False):\n    \"\"\"\n    Dividir los datos en conjuntos de entrenamiento y prueba manteniendo los trials intactos.\n\n    Par\u00e1metros:\n    -----------\n    X : array-like\n        Datos de entrenamiento.\n    y : array-like\n        Etiquetas.\n    trial_markers : array-like\n        Array que indica a qu\u00e9 trial pertenece cada muestra.\n    train_size : float, default=0.8\n        Proporci\u00f3n de datos para el conjunto de entrenamiento.\n    random_state : int, RandomState, default=None\n        Controla la aleatoriedad de la mezcla.\n    shuffle : bool, default=True\n        Si es True, los trials son mezclados antes de dividirlos.\n    return_mask : bool, default=False\n        Si es True, tambi\u00e9n retorna las m\u00e1scaras de \u00edndices de entrenamiento y prueba.\n\n    Retorna:\n    --------\n    X_train : array-like\n        Datos de entrenamiento.\n    X_test : array-like\n        Datos de prueba.\n    y_train : array-like\n        Etiquetas de entrenamiento. \n    y_test : array-like\n        Etiquetas de prueba.\n    train_mask : array-like, opcional\n        M\u00e1scara de \u00edndices de entrenamiento. Solo se retorna si return_mask=True.\n    test_mask : array-like, opcional\n        M\u00e1scara de \u00edndices de prueba. Solo se retorna si return_mask=True.\n    \"\"\"\n    # Verificar que train_size est\u00e9 entre 0 y 1\n    if not 0 &lt; train_size &lt; 1:\n        raise ValueError(\"train_size debe estar entre 0 y 1\")\n\n    # Obtenemos los IDs de trials \u00fanicos\n    unique_trials = np.unique(trial_markers)\n    n_trials = len(unique_trials)\n\n    # Mezclamos los trials si shuffle=True\n    if shuffle:\n        random_state = check_random_state(random_state)\n        shuffled_trials = random_state.permutation(unique_trials)\n    else:\n        shuffled_trials = unique_trials \n\n    # Calcular n\u00famero de trials para entrenamiento\n    n_train_trials = int(np.ceil(n_trials * train_size))\n\n    # Dividir trials en train y test\n    train_trials = shuffled_trials[:n_train_trials]\n    test_trials = shuffled_trials[n_train_trials:]\n\n    # Crear m\u00e1scaras para los \u00edndices\n    train_mask = np.isin(trial_markers, train_trials)\n    test_mask = np.isin(trial_markers, test_trials)\n\n    # Dividir X e y usando las m\u00e1scaras\n    X_train = X[train_mask]\n    X_test = X[test_mask]\n    y_train = y[train_mask]\n    y_test = y[test_mask]\n\n    if return_mask:\n        return X_train, X_test, y_train, y_test, train_mask, test_mask\n    else:\n        return X_train, X_test, y_train, y_test\n</code></pre>"},{"location":"api/results/","title":"Gesti\u00f3n de Resultados","text":"<p>Esta secci\u00f3n documenta los m\u00f3dulos para el manejo y an\u00e1lisis de resultados de experimentos.</p>"},{"location":"api/results/#modulo-de-visualizacion","title":"M\u00f3dulo de Visualizaci\u00f3n","text":""},{"location":"api/results/#ibioml.plots","title":"ibioml.plots","text":""},{"location":"api/results/#utilidades-de-evaluacion","title":"Utilidades de Evaluaci\u00f3n","text":""},{"location":"api/results/#ibioml.utils.evaluators","title":"ibioml.utils.evaluators","text":""},{"location":"api/results/#ibioml.utils.evaluators.DualOutputEvaluator","title":"DualOutputEvaluator","text":"<pre><code>DualOutputEvaluator(device)\n</code></pre> <p>               Bases: <code>ModelEvaluator</code></p> <p>Evaluador para modelos de salida dual.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def __init__(self, device):\n    self.device = device\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.DualOutputEvaluator.calculate_r2","title":"calculate_r2","text":"<pre><code>calculate_r2(y_true_scaled, predictions)\n</code></pre> <p>Calcula el score R2 para salidas duales.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def calculate_r2(self, y_true_scaled, predictions):\n    \"\"\"Calcula el score R2 para salidas duales.\"\"\"\n    pos_pred, vel_pred = predictions\n    r2_pos = r2_score(y_true_scaled[:, 0].reshape(-1, 1), pos_pred)\n    r2_vel = r2_score(y_true_scaled[:, 1].reshape(-1, 1), vel_pred)\n    # Devolver R2 promedio y componentes\n    return (r2_pos + r2_vel) / 2, r2_pos, r2_vel\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.DualOutputEvaluator.initialize_results","title":"initialize_results","text":"<pre><code>initialize_results()\n</code></pre> <p>Inicializa el diccionario de resultados con la nueva estructura.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def initialize_results(self):\n    \"\"\"Inicializa el diccionario de resultados con la nueva estructura.\"\"\"\n    results = {\n        \"test_r2_scores\": [],\n        \"test_r2_scores_pos\": [],\n        \"test_r2_scores_vel\": [],\n        \"best_config_per_fold\": [],\n        \"predictions_per_fold\": [],\n        \"true_values_per_fold\": []\n    }\n    return results\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.DualOutputEvaluator.predict","title":"predict","text":"<pre><code>predict(model, X_tensor)\n</code></pre> <p>Realiza predicciones con el modelo.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def predict(self, model, X_tensor):\n    \"\"\"Realiza predicciones con el modelo.\"\"\"\n    pos_pred, vel_pred = model(X_tensor)\n    pos_pred = pos_pred.cpu().numpy()\n    vel_pred = vel_pred.cpu().numpy()\n    return (pos_pred, vel_pred)\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.DualOutputEvaluator.prepare_evaluation_data","title":"prepare_evaluation_data","text":"<pre><code>prepare_evaluation_data(scaler, y_true)\n</code></pre> <p>Prepara los datos para evaluaci\u00f3n.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def prepare_evaluation_data(self, scaler, y_true):\n    \"\"\"Prepara los datos para evaluaci\u00f3n.\"\"\"\n    return scaler.target_scaler.transform(y_true)\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.DualOutputEvaluator.prepare_results","title":"prepare_results","text":"<pre><code>prepare_results(predictions_rescaled, y_true)\n</code></pre> <p>Prepara los resultados para guardar.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def prepare_results(self, predictions_rescaled, y_true):\n    \"\"\"Prepara los resultados para guardar.\"\"\"\n    pos_pred_rescaled, vel_pred_rescaled = predictions_rescaled\n    return [pos_pred_rescaled, vel_pred_rescaled], [y_true[:, 0].tolist(), y_true[:, 1].tolist()]\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.DualOutputEvaluator.print_summary","title":"print_summary","text":"<pre><code>print_summary(results)\n</code></pre> <p>Imprime un resumen de los resultados.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def print_summary(self, results):\n    \"\"\"Imprime un resumen de los resultados.\"\"\"\n    print(f\"Mean R\u00b2 Score: {np.mean(results['test_r2_scores']):.4f} \u00b1 {np.std(results['test_r2_scores']):.4f}\")\n    print(f\"Mean Position R\u00b2 Score: {np.mean(results['test_r2_scores_pos']):.4f} \u00b1 {np.std(results['test_r2_scores_pos']):.4f}\")\n    print(f\"Mean Velocity R\u00b2 Score: {np.mean(results['test_r2_scores_vel']):.4f} \u00b1 {np.std(results['test_r2_scores_vel']):.4f}\")\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.DualOutputEvaluator.rescale_predictions","title":"rescale_predictions","text":"<pre><code>rescale_predictions(scaler, predictions)\n</code></pre> <p>Re-escala las predicciones a valores originales.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def rescale_predictions(self, scaler, predictions):\n    \"\"\"Re-escala las predicciones a valores originales.\"\"\"\n    pos_pred, vel_pred = predictions\n    pos_pred_rescaled = scaler.inverse_transform_pos(pos_pred).ravel().tolist()\n    vel_pred_rescaled = scaler.inverse_transform_vel(vel_pred).ravel().tolist()\n    return (pos_pred_rescaled, vel_pred_rescaled)\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.DualOutputEvaluator.update_results","title":"update_results","text":"<pre><code>update_results(results, fold_results)\n</code></pre> <p>Actualiza el diccionario de resultados con los resultados del fold actual.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def update_results(self, results, fold_results):\n    \"\"\"Actualiza el diccionario de resultados con los resultados del fold actual.\"\"\"\n    fold_r2, r2_pos, r2_vel = fold_results[\"r2_scores\"]\n    results[\"test_r2_scores\"].append(fold_r2)\n    results[\"test_r2_scores_pos\"].append(r2_pos)\n    results[\"test_r2_scores_vel\"].append(r2_vel)\n\n    # Reorganizar las predicciones en formato diccionario \n    pos_pred, vel_pred = fold_results[\"predictions\"] \n    predictions_dict = {\n        \"position\": pos_pred,\n        \"velocity\": vel_pred\n    }\n    results[\"predictions_per_fold\"].append(predictions_dict)\n\n    # Reorganizar los valores verdaderos en formato diccionario\n    pos_true, vel_true = fold_results[\"true_values\"]\n    true_values_dict = {\n        \"position\": pos_true,\n        \"velocity\": vel_true\n    }\n    results[\"true_values_per_fold\"].append(true_values_dict)\n\n    return results\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.ModelEvaluator","title":"ModelEvaluator","text":"<pre><code>ModelEvaluator(device)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Clase abstracta base para evaluadores de modelos.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def __init__(self, device):\n    self.device = device\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.ModelEvaluator.calculate_r2","title":"calculate_r2  <code>abstractmethod</code>","text":"<pre><code>calculate_r2(y_true_scaled, predictions)\n</code></pre> <p>Calcula el score R2.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>@abstractmethod\ndef calculate_r2(self, y_true_scaled, predictions):\n    \"\"\"Calcula el score R2.\"\"\"\n    pass\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.ModelEvaluator.initialize_results","title":"initialize_results  <code>abstractmethod</code>","text":"<pre><code>initialize_results()\n</code></pre> <p>Inicializa el diccionario de resultados.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>@abstractmethod\ndef initialize_results(self):\n    \"\"\"Inicializa el diccionario de resultados.\"\"\"\n    pass\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.ModelEvaluator.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(model, X_tensor)\n</code></pre> <p>Realiza predicciones con el modelo.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>@abstractmethod\ndef predict(self, model, X_tensor):\n    \"\"\"Realiza predicciones con el modelo.\"\"\"\n    pass\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.ModelEvaluator.prepare_evaluation_data","title":"prepare_evaluation_data  <code>abstractmethod</code>","text":"<pre><code>prepare_evaluation_data(scaler, y_true)\n</code></pre> <p>Prepara los datos para evaluaci\u00f3n.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>@abstractmethod\ndef prepare_evaluation_data(self, scaler, y_true):\n    \"\"\"Prepara los datos para evaluaci\u00f3n.\"\"\"\n    pass\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.ModelEvaluator.prepare_results","title":"prepare_results  <code>abstractmethod</code>","text":"<pre><code>prepare_results(predictions_rescaled, y_true)\n</code></pre> <p>Prepara los resultados para guardar.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>@abstractmethod\ndef prepare_results(self, predictions_rescaled, y_true):\n    \"\"\"Prepara los resultados para guardar.\"\"\"\n    pass\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.ModelEvaluator.print_summary","title":"print_summary  <code>abstractmethod</code>","text":"<pre><code>print_summary(results)\n</code></pre> <p>Imprime un resumen de los resultados.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>@abstractmethod\ndef print_summary(self, results):\n    \"\"\"Imprime un resumen de los resultados.\"\"\"\n    pass\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.ModelEvaluator.rescale_predictions","title":"rescale_predictions  <code>abstractmethod</code>","text":"<pre><code>rescale_predictions(scaler, predictions)\n</code></pre> <p>Re-escala las predicciones a valores originales.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>@abstractmethod\ndef rescale_predictions(self, scaler, predictions):\n    \"\"\"Re-escala las predicciones a valores originales.\"\"\"\n    pass\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.ModelEvaluator.update_results","title":"update_results  <code>abstractmethod</code>","text":"<pre><code>update_results(results, fold_results)\n</code></pre> <p>Actualiza el diccionario de resultados con los resultados del fold actual.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>@abstractmethod\ndef update_results(self, results, fold_results):\n    \"\"\"Actualiza el diccionario de resultados con los resultados del fold actual.\"\"\"\n    pass\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.SingleOutputEvaluator","title":"SingleOutputEvaluator","text":"<pre><code>SingleOutputEvaluator(device)\n</code></pre> <p>               Bases: <code>ModelEvaluator</code></p> <p>Evaluador para modelos de salida \u00fanica.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def __init__(self, device):\n    self.device = device\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.SingleOutputEvaluator.calculate_r2","title":"calculate_r2","text":"<pre><code>calculate_r2(y_true_scaled, predictions)\n</code></pre> <p>Calcula el score R2.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def calculate_r2(self, y_true_scaled, predictions):\n    \"\"\"Calcula el score R2.\"\"\"\n    return r2_score(y_true_scaled, predictions), None, None\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.SingleOutputEvaluator.initialize_results","title":"initialize_results","text":"<pre><code>initialize_results()\n</code></pre> <p>Inicializa el diccionario de resultados.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def initialize_results(self):\n    \"\"\"Inicializa el diccionario de resultados.\"\"\"\n    return {\n        \"test_r2_scores\": [],\n        \"best_config_per_fold\": [],\n        \"predictions_per_fold\": [],\n        \"true_values_per_fold\": []\n    }\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.SingleOutputEvaluator.predict","title":"predict","text":"<pre><code>predict(model, X_tensor)\n</code></pre> <p>Realiza predicciones con el modelo.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def predict(self, model, X_tensor):\n    \"\"\"Realiza predicciones con el modelo.\"\"\"\n    if hasattr(model.__class__, \"__name__\") and model.__class__.__name__ == \"MLPModel\":\n        y_pred = model(X_tensor).cpu().numpy()\n    else:\n        y_pred = model(X_tensor).squeeze().cpu().numpy()\n    return y_pred\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.SingleOutputEvaluator.prepare_evaluation_data","title":"prepare_evaluation_data","text":"<pre><code>prepare_evaluation_data(scaler, y_true)\n</code></pre> <p>Prepara los datos para evaluaci\u00f3n.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def prepare_evaluation_data(self, scaler, y_true):\n    \"\"\"Prepara los datos para evaluaci\u00f3n.\"\"\"\n    return scaler.target_scaler.transform(y_true.reshape(-1, 1))\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.SingleOutputEvaluator.prepare_results","title":"prepare_results","text":"<pre><code>prepare_results(predictions_rescaled, y_true)\n</code></pre> <p>Prepara los resultados para guardar.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def prepare_results(self, predictions_rescaled, y_true):\n    \"\"\"Prepara los resultados para guardar.\"\"\"\n    return predictions_rescaled, y_true.ravel().tolist()\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.SingleOutputEvaluator.print_summary","title":"print_summary","text":"<pre><code>print_summary(results)\n</code></pre> <p>Imprime un resumen de los resultados.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def print_summary(self, results):\n    \"\"\"Imprime un resumen de los resultados.\"\"\"\n    print(f\"Mean R\u00b2 Score: {np.mean(results['test_r2_scores']):.4f} \u00b1 {np.std(results['test_r2_scores']):.4f}\")\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.SingleOutputEvaluator.rescale_predictions","title":"rescale_predictions","text":"<pre><code>rescale_predictions(scaler, predictions)\n</code></pre> <p>Re-escala las predicciones a valores originales.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def rescale_predictions(self, scaler, predictions):\n    \"\"\"Re-escala las predicciones a valores originales.\"\"\"\n    if predictions.ndim == 1:\n        predictions = predictions.reshape(-1, 1)\n    return scaler.target_scaler.inverse_transform(predictions).ravel().tolist()\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.SingleOutputEvaluator.update_results","title":"update_results","text":"<pre><code>update_results(results, fold_results)\n</code></pre> <p>Actualiza el diccionario de resultados con los resultados del fold actual.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def update_results(self, results, fold_results):\n    \"\"\"Actualiza el diccionario de resultados con los resultados del fold actual.\"\"\"\n    fold_r2, _, _ = fold_results[\"r2_scores\"]\n    results[\"test_r2_scores\"].append(fold_r2)\n    results[\"predictions_per_fold\"].append(fold_results[\"predictions\"])\n    results[\"true_values_per_fold\"].append(fold_results[\"true_values\"])\n    return results\n</code></pre>"},{"location":"api/results/#ibioml.utils.evaluators.create_evaluator","title":"create_evaluator","text":"<pre><code>create_evaluator(target_dim, device)\n</code></pre> <p>Crea el evaluador apropiado seg\u00fan el tipo de modelo.</p> Source code in <code>ibioml/utils/evaluators.py</code> <pre><code>def create_evaluator(target_dim, device):\n    \"\"\"Crea el evaluador apropiado seg\u00fan el tipo de modelo.\"\"\"\n    if target_dim == 1:\n        return SingleOutputEvaluator(device)\n    elif target_dim == 2:\n        return DualOutputEvaluator(device)\n    else:\n        raise ValueError(\"target_dim debe ser 1 o 2.\")\n</code></pre>"},{"location":"api/results/#utilidades-de-pipeline","title":"Utilidades de Pipeline","text":""},{"location":"api/results/#ibioml.utils.pipeline_utils","title":"ibioml.utils.pipeline_utils","text":""},{"location":"api/results/#ibioml.utils.pipeline_utils.find_stationary_window","title":"find_stationary_window","text":"<pre><code>find_stationary_window(y, threshold)\n</code></pre> <p>Finds the stationary window in a position signal.</p> <p>Parameters: y (array): Position array to evaluate (should be 1D or flattened). threshold (float): Threshold to define how small the change in y must be to be considered stationary.</p> <p>Returns: tuple: Start and end indices of the longest stationary window, or (None, None) if no window is detected.</p> Source code in <code>ibioml/utils/pipeline_utils.py</code> <pre><code>def find_stationary_window(y, threshold):\n    \"\"\"\n    Finds the stationary window in a position signal.\n\n    Parameters:\n    y (array): Position array to evaluate (should be 1D or flattened).\n    threshold (float): Threshold to define how small the change in y must be to be considered stationary.\n\n    Returns:\n    tuple: Start and end indices of the longest stationary window, or (None, None) if no window is detected.\n    \"\"\"\n    # Flatten y to ensure it's a 1D array\n    y = y.flatten()\n\n    # Calculate the derivative of the position signal\n    y_diff = np.diff(y)\n\n    # Identify indices where the derivative is less than the threshold\n    stationary_indices = np.where(np.abs(y_diff) &lt; threshold)[0]\n\n    # Group consecutive indices to find continuous stationary windows\n    groups = [list(group) for key, group in groupby(stationary_indices, key=lambda i, c=iter(range(len(stationary_indices))): next(c) - i)]\n\n    # Find the longest stationary window\n    longest_window = max(groups, key=len) if groups else []\n\n    if longest_window:\n        start_index = longest_window[0]\n        end_index = longest_window[-1]\n        return start_index, end_index\n    else:\n        return None, None\n</code></pre>"},{"location":"api/results/#ibioml.utils.pipeline_utils.map_to_discrete_hidden_size","title":"map_to_discrete_hidden_size","text":"<pre><code>map_to_discrete_hidden_size(value)\n</code></pre> <p>Maps a continuous value between 64 and 512 to the nearest value in [64, 128, 256, 512]</p> Source code in <code>ibioml/utils/pipeline_utils.py</code> <pre><code>def map_to_discrete_hidden_size(value):\n    \"\"\"\n    Maps a continuous value between 64 and 512 to the nearest value in [64, 128, 256, 512]\n    \"\"\"\n    discrete_values = [64, 128, 256, 512]\n    return min(discrete_values, key=lambda x: abs(x - value))\n</code></pre>"},{"location":"api/results/#funciones-de-plotting","title":"Funciones de Plotting","text":""},{"location":"api/results/#ibioml.utils.plot_functions","title":"ibioml.utils.plot_functions","text":""},{"location":"api/results/#ibioml.utils.plot_functions.custom_strategy_boxplot","title":"custom_strategy_boxplot","text":"<pre><code>custom_strategy_boxplot(combined_df, save_path=None)\n</code></pre> <p>Crea un boxplot personalizado para comparar estrategias de CV con estilo minimalista.</p> <p>Parameters:</p> Name Type Description Default <code>combined_df</code> <p>DataFrame con datos en formato largo</p> required <code>save_path</code> <p>Ruta para guardar la figura</p> <code>None</code> Source code in <code>ibioml/utils/plot_functions.py</code> <pre><code>def custom_strategy_boxplot(combined_df, save_path=None):\n    \"\"\"\n    Crea un boxplot personalizado para comparar estrategias de CV con estilo minimalista.\n\n    Args:\n        combined_df: DataFrame con datos en formato largo\n        save_path: Ruta para guardar la figura\n    \"\"\"\n    # Definir la paleta de colores\n    palette = sns.color_palette(\"Set2\")\n    sns.set_palette(palette)\n\n    # Crear el gr\u00e1fico\n    plt.figure(figsize=(12, 7), dpi=200)\n\n    # Crear el boxplot para cada modelo usando la misma paleta\n    sns.boxplot(x='Model', y='R2_Score', data=combined_df, \n                fill=False, showfliers=False, hue='CV_Strategy')\n\n    # Agregar el stripplot para mostrar todos los puntos individuales\n    sns.stripplot(x='Model', y='R2_Score', data=combined_df, \n                  jitter=True, s=10, marker=\"X\", alpha=.3, dodge=True, \n                  hue='CV_Strategy', legend=False)\n\n    # Personalizar el gr\u00e1fico\n    plt.xlabel('Modelo', fontsize=14)\n    plt.ylabel('R\u00b2 Score', fontsize=14)\n    plt.ylim([0, 1])\n    plt.grid(axis='y')\n\n    # Remover las leyendas duplicadas\n    handles, labels = plt.gca().get_legend_handles_labels()\n    unique_labels = dict(zip(labels[:3], handles[:3]))\n\n    ax = plt.gca()\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n\n    # Calcular las medias para anotaci\u00f3n opcional\n    mean_scores = combined_df.groupby(['CV_Strategy', 'Model'])['R2_Score'].mean().reset_index()\n\n    # A\u00f1adir l\u00edneas de referencia en valores clave\n    plt.axhline(y=0.5, color='#999999', linestyle='--', alpha=0.5)\n    plt.axhline(y=0.7, color='#999999', linestyle='--', alpha=0.5)\n    plt.axhline(y=0.9, color='#999999', linestyle='--', alpha=0.5)\n\n    # Leyenda personalizada\n    plt.legend(title='Estrategia de CV', loc='upper right', frameon=True, \n               fancybox=True, shadow=True)\n\n    # Guardar la figura si se proporciona una ruta\n    if save_path:\n        plt.tight_layout()\n        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n\n    plt.show()\n</code></pre>"},{"location":"api/results/#ibioml.utils.plot_functions.enhanced_boxplot_test_r2_scores","title":"enhanced_boxplot_test_r2_scores","text":"<pre><code>enhanced_boxplot_test_r2_scores(r2_df, save_path=None, title='Comparaci\u00f3n de R2 Scores entre Modelos')\n</code></pre> <p>Crea un boxplot mejorado para comparar R2 scores entre modelos.</p> <p>Parameters:</p> Name Type Description Default <code>r2_df</code> <p>DataFrame con datos de R2 scores en formato largo</p> required <code>save_path</code> <p>Ruta para guardar la figura</p> <code>None</code> <code>title</code> <p>T\u00edtulo del gr\u00e1fico</p> <code>'Comparaci\u00f3n de R2 Scores entre Modelos'</code> Source code in <code>ibioml/utils/plot_functions.py</code> <pre><code>def enhanced_boxplot_test_r2_scores(r2_df, save_path=None, title=\"Comparaci\u00f3n de R2 Scores entre Modelos\"):\n    \"\"\"\n    Crea un boxplot mejorado para comparar R2 scores entre modelos.\n\n    Args:\n        r2_df: DataFrame con datos de R2 scores en formato largo\n        save_path: Ruta para guardar la figura\n        title: T\u00edtulo del gr\u00e1fico\n    \"\"\"\n    set_plotting_style()\n    model_colors = get_model_colors()\n\n    # Calcular estad\u00edsticas para anotaciones\n    medians = r2_df.groupby(['Modelo'])['R2 Score'].median()\n    means = r2_df.groupby(['Modelo'])['R2 Score'].mean()\n\n    # Crear la figura con un tama\u00f1o adecuado\n    plt.figure(figsize=(12, 8), dpi=200)\n\n    # Crear el boxplot principal\n    ax = sns.boxplot(x='Modelo', y='R2 Score', data=r2_df, \n                      palette=model_colors, width=0.5,\n                      boxprops=dict(alpha=0.7),\n                      medianprops=dict(color='#333333', linewidth=2.5))\n\n    # Agregar puntos individuales con jitter\n    sns.stripplot(x='Modelo', y='R2 Score', data=r2_df, \n                  palette=model_colors, alpha=0.6, size=8,\n                  jitter=0.2, marker='o', linewidth=1, \n                  edgecolor='w')\n\n    # Mejoras visuales\n    ax.set_xlabel('Modelo', fontsize=16, fontweight='bold', labelpad=15)\n    ax.set_ylabel('R\u00b2 Score', fontsize=16, fontweight='bold', labelpad=15)\n    ax.set_title(title, fontsize=18, fontweight='bold', pad=20)\n    ax.grid(axis='y', linestyle='--', alpha=0.7)\n    ax.set_ylim([0, 1])\n\n    # Ajustar el fondo\n    ax.set_facecolor('#f8f9fa')\n    plt.gcf().set_facecolor('#ffffff')\n\n    # Agregar l\u00edneas horizontales en valores clave\n    plt.axhline(y=0.5, color='#999999', linestyle='--', alpha=0.5)\n    plt.axhline(y=0.7, color='#999999', linestyle='--', alpha=0.5)\n    plt.axhline(y=0.9, color='#999999', linestyle='--', alpha=0.5)\n\n    # A\u00f1adir anotaciones para los valores de mediana y media\n    for i, model in enumerate(r2_df['Modelo'].unique()):\n        # Anotaci\u00f3n de la mediana\n        plt.annotate(f'Mediana: {medians[model]:.3f}',\n                    xy=(i, medians[model] + 0.03), \n                    xytext=(i, medians[model] + 0.07),\n                    ha='center', va='bottom',\n                    fontsize=12, fontweight='bold',\n                    color=model_colors[model],\n                    arrowprops=dict(arrowstyle='-&gt;', color=model_colors[model]))\n\n        # Anotaci\u00f3n de la media con un marcador diferente\n        plt.scatter(i, means[model], marker='*', s=200, \n                   color=model_colors[model], edgecolor='white', zorder=3)\n        plt.annotate(f'Media: {means[model]:.3f}', \n                    xy=(i, means[model] - 0.03),\n                    xytext=(i, means[model] - 0.07),\n                    ha='center', va='top',\n                    fontsize=12, fontweight='bold',\n                    color=model_colors[model],\n                    arrowprops=dict(arrowstyle='-&gt;', color=model_colors[model]))\n\n    # Guardar la figura si se proporciona una ruta\n    if save_path:\n        plt.tight_layout()\n        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n\n    plt.show()\n</code></pre>"},{"location":"api/results/#ibioml.utils.plot_functions.enhanced_comparison_boxplot","title":"enhanced_comparison_boxplot","text":"<pre><code>enhanced_comparison_boxplot(combined_df, save_path=None)\n</code></pre> <p>Crea un boxplot mejorado para comparar diferentes estrategias de CV.</p> <p>Parameters:</p> Name Type Description Default <code>combined_df</code> <p>DataFrame con datos en formato largo</p> required <code>save_path</code> <p>Ruta para guardar la figura</p> <code>None</code> Source code in <code>ibioml/utils/plot_functions.py</code> <pre><code>def enhanced_comparison_boxplot(combined_df, save_path=None):\n    \"\"\"\n    Crea un boxplot mejorado para comparar diferentes estrategias de CV.\n\n    Args:\n        combined_df: DataFrame con datos en formato largo\n        save_path: Ruta para guardar la figura\n    \"\"\"\n    set_plotting_style()\n    model_colors = get_model_colors()\n    strategy_colors = get_strategy_colors()\n\n    # Calcular medias para anotaciones\n    mean_scores = combined_df.groupby(['CV_Strategy', 'Model'])['R2_Score'].mean().reset_index()\n\n    plt.figure(figsize=(16, 10), dpi=200)\n\n    # Crear el boxplot\n    ax = sns.boxplot(x='Model', y='R2_Score', hue='CV_Strategy', data=combined_df,\n                     palette=strategy_colors, width=0.7,\n                     boxprops=dict(alpha=0.7),\n                     medianprops=dict(color='#333333', linewidth=2))\n\n    # A\u00f1adir puntos individuales\n    swarm = sns.swarmplot(x='Model', y='R2_Score', hue='CV_Strategy', data=combined_df,\n                         palette=strategy_colors, alpha=0.7, size=5,\n                         dodge=True, edgecolor='none')\n\n    # Quitar la leyenda duplicada\n    handles, labels = ax.get_legend_handles_labels()\n    plt.legend(handles[:len(strategy_colors)], labels[:len(strategy_colors)],\n               title='Estrategia de CV', bbox_to_anchor=(1.05, 1), \n               loc='upper left', borderaxespad=0,\n               frameon=True, fancybox=True, shadow=True)\n\n    # Mejoras visuales\n    plt.title('Comparaci\u00f3n de Estrategias de Cross-Validation', \n              fontsize=20, fontweight='bold', pad=20)\n    plt.xlabel('Modelo', fontsize=16, fontweight='bold', labelpad=15)\n    plt.ylabel('R\u00b2 Score', fontsize=16, fontweight='bold', labelpad=15)\n    plt.ylim([0, 1])\n    plt.grid(axis='y', linestyle='--', alpha=0.5)\n\n    # Fondo y bordes\n    ax.set_facecolor('#f8f9fa')\n    plt.gcf().set_facecolor('#ffffff')\n\n    # Agregar l\u00edneas horizontales en valores clave\n    plt.axhline(y=0.5, color='#999999', linestyle='--', alpha=0.5)\n    plt.axhline(y=0.7, color='#999999', linestyle='--', alpha=0.5)\n    plt.axhline(y=0.9, color='#999999', linestyle='--', alpha=0.5)\n\n    # Agregar anotaciones para los valores medios\n    dodge_positions = {0: -0.3, 1: 0, 2: 0.3}\n    strategies = combined_df['CV_Strategy'].unique()\n\n    for model_idx, model in enumerate(combined_df['Model'].unique()):\n        for strat_idx, strategy in enumerate(strategies):\n            mean_value = mean_scores[(mean_scores['Model'] == model) &amp; \n                                    (mean_scores['CV_Strategy'] == strategy)]['R2_Score'].values[0]\n\n            plt.annotate(f'{mean_value:.3f}',\n                        xy=(model_idx + dodge_positions[strat_idx], mean_value),\n                        xytext=(model_idx + dodge_positions[strat_idx], mean_value + 0.05),\n                        ha='center', va='bottom',\n                        fontsize=9, fontweight='bold',\n                        color=strategy_colors[strategy],\n                        arrowprops=dict(arrowstyle='-', color=strategy_colors[strategy], alpha=0.5))\n\n    # Guardar la figura si se proporciona una ruta\n    if save_path:\n        plt.tight_layout()\n        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n\n    plt.show()\n</code></pre>"},{"location":"api/results/#ibioml.utils.plot_functions.enhanced_heatmap","title":"enhanced_heatmap","text":"<pre><code>enhanced_heatmap(pivot_data, save_path=None, title='Comparaci\u00f3n de Estrategias de CV')\n</code></pre> <p>Crea un mapa de calor mejorado para comparar estrategias.</p> <p>Parameters:</p> Name Type Description Default <code>pivot_data</code> <p>DataFrame con datos en formato pivote</p> required <code>save_path</code> <p>Ruta para guardar la figura</p> <code>None</code> <code>title</code> <p>T\u00edtulo del gr\u00e1fico</p> <code>'Comparaci\u00f3n de Estrategias de CV'</code> Source code in <code>ibioml/utils/plot_functions.py</code> <pre><code>def enhanced_heatmap(pivot_data, save_path=None, title=\"Comparaci\u00f3n de Estrategias de CV\"):\n    \"\"\"\n    Crea un mapa de calor mejorado para comparar estrategias.\n\n    Args:\n        pivot_data: DataFrame con datos en formato pivote\n        save_path: Ruta para guardar la figura\n        title: T\u00edtulo del gr\u00e1fico\n    \"\"\"\n    set_plotting_style()\n\n    # Crear un colormap personalizado\n    cmap = sns.color_palette(\"YlGnBu\", as_cmap=True)\n\n    plt.figure(figsize=(12, 8), dpi=200)\n\n    # Crear el heatmap\n    ax = sns.heatmap(pivot_data, annot=True, cmap=cmap, \n                    fmt=\".3f\", vmin=0, vmax=1, \n                    linewidths=0.5, linecolor='#ffffff',\n                    cbar_kws={'label': 'R\u00b2 Score Medio', \n                             'shrink': 0.8})\n\n    # Mejoras visuales\n    ax.set_title(title, fontsize=18, fontweight='bold', pad=20)\n\n    # Cambiar el estilo de las etiquetas\n    for text in ax.texts:\n        value = float(text.get_text())\n        if value &gt;= 0.8:\n            text.set_weight('bold')\n        if value &gt;= 0.9:\n            text.set_size(14)\n\n        # Cambiar color del texto seg\u00fan el valor para mejorar legibilidad\n        if value &gt; 0.7:\n            text.set_color('white')\n        else:\n            text.set_color('black')\n\n    # Agregar bordes al gr\u00e1fico\n    for _, spine in ax.spines.items():\n        spine.set_visible(True)\n        spine.set_color('#333333')\n        spine.set_linewidth(2)\n\n    # Cambiar la orientaci\u00f3n de las etiquetas del eje y\n    plt.yticks(rotation=0)\n\n    # Mejorar la barra de colores\n    cbar = ax.collections[0].colorbar\n    cbar.ax.tick_params(labelsize=12)\n    cbar.set_label('R\u00b2 Score Medio', fontsize=14, fontweight='bold', labelpad=15)\n\n    # Guardar la figura si se proporciona una ruta\n    if save_path:\n        plt.tight_layout()\n        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n\n    plt.show()\n</code></pre>"},{"location":"api/results/#ibioml.utils.plot_functions.enhanced_plot_predictions","title":"enhanced_plot_predictions","text":"<pre><code>enhanced_plot_predictions(predictions, true_values, test_r2_scores, fold_to_plot=0, models=None, save_path=None, limit=1000)\n</code></pre> <p>Crea un gr\u00e1fico mejorado para mostrar predicciones vs valores reales.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <p>Diccionario de predicciones por modelo y fold</p> required <code>true_values</code> <p>Diccionario de valores reales por modelo y fold</p> required <code>test_r2_scores</code> <p>Diccionario de R2 scores por modelo</p> required <code>fold_to_plot</code> <p>\u00cdndice del fold a visualizar</p> <code>0</code> <code>models</code> <p>Lista de modelos a incluir</p> <code>None</code> <code>save_path</code> <p>Ruta para guardar la figura</p> <code>None</code> <code>limit</code> <p>N\u00famero de puntos a mostrar</p> <code>1000</code> Source code in <code>ibioml/utils/plot_functions.py</code> <pre><code>def enhanced_plot_predictions(predictions, true_values, test_r2_scores, fold_to_plot=0, \n                             models=None, save_path=None, limit=1000):\n    \"\"\"\n    Crea un gr\u00e1fico mejorado para mostrar predicciones vs valores reales.\n\n    Args:\n        predictions: Diccionario de predicciones por modelo y fold\n        true_values: Diccionario de valores reales por modelo y fold\n        test_r2_scores: Diccionario de R2 scores por modelo\n        fold_to_plot: \u00cdndice del fold a visualizar\n        models: Lista de modelos a incluir\n        save_path: Ruta para guardar la figura\n        limit: N\u00famero de puntos a mostrar\n    \"\"\"\n    set_plotting_style()\n    model_colors = get_model_colors()\n\n    if models is None:\n        models = predictions.keys()\n\n    # Calcular el n\u00famero de filas y columnas para los subplots\n    n_models = len(models)\n    n_cols = min(2, n_models)\n    n_rows = (n_models + n_cols - 1) // n_cols\n\n    # Crear la figura con subplots\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows), dpi=200, sharex=True)\n    if n_rows * n_cols == 1:\n        axes = np.array([axes])\n    axes = axes.flatten()\n\n    # Configurar el fondo para toda la figura\n    fig.patch.set_facecolor('#ffffff')\n\n    for i, (ax, model) in enumerate(zip(axes, models)):\n        # Obtener datos para el modelo\n        y_test = true_values[model][fold_to_plot][:limit]\n        y_pred = predictions[model][fold_to_plot][:limit]\n        r2 = test_r2_scores[model][fold_to_plot]\n\n        # Configurar el fondo del subplot\n        ax.set_facecolor('#f8f9fa')\n\n        # Graficar valores reales\n        real_line = ax.plot(y_test, label='Valores Reales', \n                           color='#333333', linewidth=2, alpha=0.8,\n                           path_effects=[pe.Stroke(linewidth=3, foreground='white'), pe.Normal()])\n\n        # Graficar predicciones\n        model_color = model_colors.get(model.upper(), '#1f77b4')\n        pred_line = ax.plot(y_pred, label='Predicciones', \n                           color=model_color, linewidth=2.5, alpha=0.9,\n                           path_effects=[pe.Stroke(linewidth=3.5, foreground='white', alpha=0.5), pe.Normal()])\n\n        # Mejoras visuales\n        ax.set_title(f'{model.upper()} (R\u00b2 = {r2:.3f})', \n                    fontsize=16, fontweight='bold', pad=10)\n        ax.set_xlabel('Tiempo (bins)', fontsize=12, labelpad=10)\n        ax.set_ylabel('Posici\u00f3n (cm)', fontsize=12, labelpad=10)\n        ax.grid(alpha=0.3, linestyle='--')\n\n        # \u00c1rea sombreada para mostrar diferencias\n        ax.fill_between(range(len(y_test)), y_test, y_pred, \n                       color=model_color, alpha=0.15)\n\n        # Destacar \u00e1reas con buenas y malas predicciones\n        errors = np.abs(y_test - y_pred)\n        error_threshold = np.percentile(errors, 90)  # Top 10% errores\n\n        # Marcar \u00e1reas con errores grandes\n        highlight_indices = np.where(errors &gt; error_threshold)[0]\n        if len(highlight_indices) &gt; 0:\n            clusters = []\n            current_cluster = [highlight_indices[0]]\n\n            for j in range(1, len(highlight_indices)):\n                if highlight_indices[j] - highlight_indices[j-1] &lt;= 5:  # Puntos cercanos\n                    current_cluster.append(highlight_indices[j])\n                else:\n                    if len(current_cluster) &gt;= 3:  # Cluster significativo\n                        clusters.append(current_cluster)\n                    current_cluster = [highlight_indices[j]]\n\n            if len(current_cluster) &gt;= 3:\n                clusters.append(current_cluster)\n\n            # Sombrear \u00e1reas con errores grandes\n            for cluster in clusters[:3]:  # Limitar a 3 clusters para no sobrecargar\n                start, end = max(0, min(cluster)-2), min(len(y_test)-1, max(cluster)+2)\n                ax.axvspan(start, end, color='#ff9999', alpha=0.3, label='_nolegend_')\n\n        # Leyenda\n        ax.legend(loc='upper right', frameon=True, fancybox=True, framealpha=0.9, \n                 shadow=True, fontsize=10)\n\n    # Ajustar el espacio entre subplots\n    plt.tight_layout(pad=3.0)\n\n    # T\u00edtulo general\n    fig.suptitle('Predicciones de Posici\u00f3n vs. Valores Reales', \n                fontsize=20, fontweight='bold', y=1.02)\n\n    # Guardar la figura si se proporciona una ruta\n    if save_path:\n        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n\n    plt.show()\n</code></pre>"},{"location":"api/results/#estilos-de-graficos","title":"Estilos de Gr\u00e1ficos","text":""},{"location":"api/results/#ibioml.utils.plot_styles","title":"ibioml.utils.plot_styles","text":""},{"location":"api/results/#ibioml.utils.plot_styles.get_model_colors","title":"get_model_colors","text":"<pre><code>get_model_colors(model_count=4)\n</code></pre> <p>Genera un diccionario de colores para modelos</p> Source code in <code>ibioml/utils/plot_styles.py</code> <pre><code>def get_model_colors(model_count=4):\n    \"\"\"Genera un diccionario de colores para modelos\"\"\"\n    # Crea una paleta personalizada vibrante pero profesional\n    colors = sns.color_palette(\"tab10\", model_count)\n    models = ['MLP', 'RNN', 'GRU', 'LSTM'][:model_count]\n    return dict(zip(models, colors))\n</code></pre>"},{"location":"api/results/#ibioml.utils.plot_styles.get_strategy_colors","title":"get_strategy_colors","text":"<pre><code>get_strategy_colors()\n</code></pre> <p>Genera un diccionario de colores para estrategias de CV</p> Source code in <code>ibioml/utils/plot_styles.py</code> <pre><code>def get_strategy_colors():\n    \"\"\"Genera un diccionario de colores para estrategias de CV\"\"\"\n    strategies = ['No Shuffle', 'Time Bin Shuffle', 'Trial Shuffle']\n    colors = sns.color_palette(\"mako\", len(strategies))\n    return dict(zip(strategies, colors))\n</code></pre>"},{"location":"api/results/#ibioml.utils.plot_styles.set_plotting_style","title":"set_plotting_style","text":"<pre><code>set_plotting_style()\n</code></pre> <p>Configura un estilo visual mejorado para todas las visualizaciones</p> Source code in <code>ibioml/utils/plot_styles.py</code> <pre><code>def set_plotting_style():\n    \"\"\"Configura un estilo visual mejorado para todas las visualizaciones\"\"\"\n    # Configuraci\u00f3n de estilo base\n    plt.style.use('seaborn-v0_8-whitegrid')\n\n    # Paletas personalizadas\n    palette = sns.color_palette(\"tab10\", 4)\n    sns.set_palette(palette)\n\n    # Tama\u00f1os y estilos de fuente\n    plt.rcParams['font.family'] = 'sans-serif'\n    plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n    plt.rcParams['font.size'] = 12\n    plt.rcParams['axes.labelsize'] = 14\n    plt.rcParams['axes.titlesize'] = 16\n    plt.rcParams['xtick.labelsize'] = 12\n    plt.rcParams['ytick.labelsize'] = 12\n    plt.rcParams['legend.fontsize'] = 12\n    plt.rcParams['figure.titlesize'] = 18\n\n    # Estilo de las l\u00edneas y gr\u00e1ficos\n    plt.rcParams['axes.grid'] = False\n    plt.rcParams['grid.alpha'] = 0.3\n    plt.rcParams['axes.linewidth'] = 1.5\n    plt.rcParams['axes.edgecolor'] = '#333333'\n    plt.rcParams['lines.linewidth'] = 2.5\n    plt.rcParams['lines.markersize'] = 10\n\n    # Estilo de los ejes\n    plt.rcParams['axes.spines.top'] = False\n    plt.rcParams['axes.spines.right'] = False\n</code></pre>"},{"location":"api/training/","title":"Entrenamiento y Optimizaci\u00f3n","text":"<p>Esta secci\u00f3n documenta los m\u00f3dulos relacionados con el entrenamiento de modelos y la optimizaci\u00f3n de hiperpar\u00e1metros.</p>"},{"location":"api/training/#modulo-trainer","title":"M\u00f3dulo Trainer","text":""},{"location":"api/training/#ibioml.trainer","title":"ibioml.trainer","text":""},{"location":"api/training/#ibioml.trainer.BaseTrainer","title":"BaseTrainer","text":"<pre><code>BaseTrainer(config)\n</code></pre> <p>Clase Entrenador base con funcionalidad compartida.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def __init__(self, config):\n    self.config = config\n    if \"train_loader\" in config and \"val_loader\" in config:\n        self.initialize_model(config)\n        self.train_loader = config[\"train_loader\"]\n        self.X_val, self.y_val = config[\"val_loader\"].dataset.tensors[0].to(self.config[\"device\"]), config[\"val_loader\"].dataset.tensors[1].to(self.config[\"device\"])\n    self.losses = []\n    self.metrics = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"r2_score\": [],\n        \"epoch\": [],\n        \"weight_mean\": [],\n        \"weight_std\": [],\n        \"grad_mean\": [],\n        \"grad_std\": [],\n        \"epoch_time_ms\": [],\n        \"split\": []\n    }\n    self.results = {}\n    self.results_dir = config[\"results_dir\"]\n    self.model_name = config[\"model_class\"].__name__\n    self.run_id = f\"{self.model_name}_{int(time.time())}\"\n    self.config[\"run_id\"] = self.run_id\n    self.config[\"model_name\"] = self.model_name\n</code></pre>"},{"location":"api/training/#ibioml.trainer.BaseTrainer.apply_regularization","title":"apply_regularization","text":"<pre><code>apply_regularization(loss)\n</code></pre> <p>Aplicar regularizaci\u00f3n a la p\u00e9rdida.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def apply_regularization(self, loss):\n    \"\"\"Aplicar regularizaci\u00f3n a la p\u00e9rdida.\"\"\"\n    if self.config[\"reg_type\"] is None:\n        return loss\n\n    for name, param in self.model.named_parameters():\n        if 'weight' in name:\n            # Aplicar regularizaci\u00f3n L1\n            if self.config[\"reg_type\"] == 'L1':\n                l1_norm = param.abs().sum()\n                loss += self.config[\"lambda_reg\"] * l1_norm\n\n            # Aplicar regularizaci\u00f3n L2\n            elif self.config[\"reg_type\"] == 'L2':\n                l2_norm = param.pow(2).sum()\n                loss += self.config[\"lambda_reg\"] * l2_norm\n\n    return loss\n</code></pre>"},{"location":"api/training/#ibioml.trainer.BaseTrainer.calculate_loss","title":"calculate_loss","text":"<pre><code>calculate_loss(outputs, targets)\n</code></pre> <p>Calcular p\u00e9rdida seg\u00fan el tipo de modelo.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def calculate_loss(self, outputs, targets):\n    \"\"\"Calcular p\u00e9rdida seg\u00fan el tipo de modelo.\"\"\"\n    raise NotImplementedError(\"Las subclases deben implementar calculate_loss\")\n</code></pre>"},{"location":"api/training/#ibioml.trainer.BaseTrainer.calculate_statistics","title":"calculate_statistics","text":"<pre><code>calculate_statistics()\n</code></pre> <p>Calculate statistics for weights and gradients of the model.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def calculate_statistics(self):\n    \"\"\"Calculate statistics for weights and gradients of the model.\"\"\"\n    weights = []\n    gradients = []\n\n    for param in self.model.parameters():\n        if param.requires_grad:\n            weights.append(param.data.cpu().numpy())\n            # Verificar si el gradiente existe\n            if param.grad is not None:\n                gradients.append(param.grad.cpu().numpy())\n\n    weights = np.concatenate([w.flatten() for w in weights])\n\n    # Verificar si hay gradientes disponibles\n    if gradients:\n        gradients = np.concatenate([g.flatten() for g in gradients])\n        grad_mean = np.mean(gradients)\n        grad_std = np.std(gradients)\n    else:\n        # Si no hay gradientes, usar valores predeterminados\n        grad_mean = 0.0\n        grad_std = 0.0\n\n    weight_mean = np.mean(weights)\n    weight_std = np.std(weights)\n\n    return weight_mean, weight_std, grad_mean, grad_std\n</code></pre>"},{"location":"api/training/#ibioml.trainer.BaseTrainer.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model()\n</code></pre> <p>Evaluar el modelo.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def evaluate_model(self):\n    \"\"\"Evaluar el modelo.\"\"\"\n    raise NotImplementedError(\"Las subclases deben implementar evaluate_model\")\n</code></pre>"},{"location":"api/training/#ibioml.trainer.BaseTrainer.forward_pass","title":"forward_pass","text":"<pre><code>forward_pass(data)\n</code></pre> <p>Realizar paso hacia adelante seg\u00fan el tipo de modelo.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def forward_pass(self, data):\n    \"\"\"Realizar paso hacia adelante seg\u00fan el tipo de modelo.\"\"\"\n    raise NotImplementedError(\"Las subclases deben implementar forward_pass\")\n</code></pre>"},{"location":"api/training/#ibioml.trainer.BaseTrainer.initialize_model","title":"initialize_model","text":"<pre><code>initialize_model(config)\n</code></pre> <p>Inicializar el modelo, el criterio y el optimizador.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def initialize_model(self, config):\n    \"\"\"Inicializar el modelo, el criterio y el optimizador.\"\"\"\n    self.model = config[\"model_class\"](input_size=config[\"input_size\"], hidden_size=config[\"hidden_size\"], output_size=config[\"output_size\"], num_layers=config[\"num_layers\"], dropout=config[\"dropout\"]).to(config[\"device\"])\n    initialize_weights(self.model) \n    self.criterion = nn.MSELoss()\n    self.optimizer = optim.Adam(self.model.parameters(), lr=config[\"lr\"])\n</code></pre>"},{"location":"api/training/#ibioml.trainer.BaseTrainer.process_target","title":"process_target","text":"<pre><code>process_target(target)\n</code></pre> <p>Procesar datos objetivo seg\u00fan el tipo de modelo.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def process_target(self, target):\n    \"\"\"Procesar datos objetivo seg\u00fan el tipo de modelo.\"\"\"\n    raise NotImplementedError(\"Las subclases deben implementar process_target\")\n</code></pre>"},{"location":"api/training/#ibioml.trainer.BaseTrainer.report_metrics","title":"report_metrics","text":"<pre><code>report_metrics(average_train_loss, val_loss, r2_score, epoch, average_train_loss_sin_norm=None, y_pred=None, epoch_time_ms=None)\n</code></pre> <p>Reportar m\u00e9tricas de entrenamiento.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def report_metrics(self, average_train_loss, val_loss, r2_score, epoch, average_train_loss_sin_norm=None, y_pred=None, epoch_time_ms=None):\n    \"\"\"Reportar m\u00e9tricas de entrenamiento.\"\"\"\n    weight_mean, weight_std, grad_mean, grad_std = self.calculate_statistics()\n\n    # Guardar m\u00e9tricas en el diccionario\n    self.metrics[\"train_loss\"].append(average_train_loss)\n    self.metrics[\"val_loss\"].append(val_loss)\n    self.metrics[\"r2_score\"].append(r2_score)\n    self.metrics[\"epoch\"].append(epoch)\n    self.metrics[\"weight_mean\"].append(weight_mean)\n    self.metrics[\"weight_std\"].append(weight_std)\n    self.metrics[\"grad_mean\"].append(grad_mean)\n    self.metrics[\"grad_std\"].append(grad_std)\n    self.metrics[\"epoch_time_ms\"].append(epoch_time_ms)\n\n    if \"split_idx\" in self.config:\n        self.metrics[\"split\"].append(self.config[\"split_idx\"] + 1)\n    else:\n        self.metrics[\"split\"].append(0)\n\n    # Guardar m\u00e9tricas adicionales si est\u00e1n disponibles\n    if average_train_loss_sin_norm is not None:\n        if \"train_loss_sin_norm\" not in self.metrics:\n            self.metrics[\"train_loss_sin_norm\"] = []\n        self.metrics[\"train_loss_sin_norm\"].append(average_train_loss_sin_norm)\n\n    # Imprimir el progreso\n    if \"split_idx\" in self.config:               \n        print(f'Epoch [{epoch}/{self.config[\"num_epochs\"]}], Split: {self.config[\"split_idx\"] + 1}, Train Loss: {average_train_loss:.4f}, Val Loss: {val_loss:.4f}, R2 Score: {r2_score:.4f}')\n    else:\n        print(f'Epoch [{epoch}/{self.config[\"num_epochs\"]}], Train Loss: {average_train_loss:.4f}, Val Loss: {val_loss:.4f}, R2 Score: {r2_score:.4f}')\n    print(f'Weight Mean: {weight_mean:.4f}, Weight Std: {weight_std:.4f}, Grad Mean: {grad_mean:.4f}, Grad Std: {grad_std:.4f}')\n    if epoch_time_ms is not None:\n        print(f'Epoch Time: {epoch_time_ms:.2f} ms')\n    if average_train_loss_sin_norm is not None:\n        print(f'Train Loss sin norm: {average_train_loss_sin_norm:.4f}')\n\n    # Guardar resultados finales cuando termine el entrenamiento\n    if self.done:\n        best_val_loss, best_r2_score, best_y_pred = self.evaluate_model()\n        self.results[\"r2_score\"] = best_r2_score\n        self.results[\"model\"] = self.model\n        self.results[\"config\"] = {k: v for k, v in self.config.items() if not callable(v) and k not in ['dataset', 'device', 'train_loader', 'val_loader', 'y_scaler']}\n        self._save_final_results(best_y_pred)\n</code></pre>"},{"location":"api/training/#ibioml.trainer.BaseTrainer.train","title":"train","text":"<pre><code>train()\n</code></pre> <p>Entrenar el modelo.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def train(self):\n    \"\"\"Entrenar el modelo.\"\"\"\n    # La implementaci\u00f3n sigue siendo la misma que en el Entrenador original\n    epoch = 0\n    self.done = False\n    if self.config[\"es_patience\"] is not None:\n        es = EarlyStopping(patience=self.config[\"es_patience\"])\n\n    while not self.done and epoch &lt; self.config[\"num_epochs\"]:\n        epoch_start_time = time.time()\n\n        epoch += 1\n        average_train_loss, average_train_loss_sin_norm = self.train_one_epoch()\n        self.losses.append(average_train_loss)\n\n        val_loss, r2_score, y_pred = self.evaluate_model()\n\n        epoch_time_ms = (time.time() - epoch_start_time) * 1000\n\n        if self.config[\"es_patience\"] is not None:\n            self.done = es(self.model, val_loss) \n\n        if epoch == self.config[\"num_epochs\"]:\n            self.done = True\n\n        self.report_metrics(average_train_loss, val_loss, r2_score, epoch, average_train_loss_sin_norm, y_pred, epoch_time_ms)\n</code></pre>"},{"location":"api/training/#ibioml.trainer.BaseTrainer.train_one_epoch","title":"train_one_epoch","text":"<pre><code>train_one_epoch()\n</code></pre> <p>M\u00e9todo plantilla para entrenar una \u00e9poca.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def train_one_epoch(self):\n    \"\"\"M\u00e9todo plantilla para entrenar una \u00e9poca.\"\"\"\n    self.model.train()\n    epoch_loss = 0\n    epoch_loss_sin_norm = 0\n\n    for batch_idx, (data, target) in enumerate(self.train_loader):\n        data = data.to(self.config[\"device\"])\n\n        # Procesar objetivos de manera diferente seg\u00fan el tipo de modelo\n        processed_target = self.process_target(target)\n\n        self.optimizer.zero_grad()\n\n        # Paso hacia adelante (manejado por subclases)\n        outputs = self.forward_pass(data.float())\n\n        # Calcular p\u00e9rdida (manejado por subclases)\n        loss = self.calculate_loss(outputs, processed_target)\n\n        if self.config[\"reg_type\"] is not None:\n            epoch_loss_sin_norm += loss.item()\n            loss = self.apply_regularization(loss)\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n        self.optimizer.step()\n        epoch_loss += loss.item()\n\n    average_train_loss = epoch_loss / len(self.train_loader)\n    if self.config[\"reg_type\"] is not None:\n        average_train_loss_sin_norm = epoch_loss_sin_norm / len(self.train_loader)\n        return average_train_loss, average_train_loss_sin_norm\n    else:\n        return average_train_loss, None\n</code></pre>"},{"location":"api/training/#ibioml.trainer.BaseTrainer.train_with_cv","title":"train_with_cv","text":"<pre><code>train_with_cv()\n</code></pre> <p>Entrenar con validaci\u00f3n cruzada.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def train_with_cv(self):\n    \"\"\"Entrenar con validaci\u00f3n cruzada.\"\"\"\n    # La implementaci\u00f3n sigue siendo la misma que en el Entrenador original\n    X_tensor, y_tensor = self.config[\"dataset\"].tensors\n    X = X_tensor.numpy()\n    y = y_tensor.numpy()\n    kf = KFold(n_splits=self.config[\"n_splits\"])\n\n    for split_idx, (train_idx, val_idx) in enumerate(kf.split(X)):\n        print(f\"\\nSplit {split_idx + 1}/{self.config['n_splits']}\")\n\n        X_train = X[train_idx]\n        y_train = y[train_idx]\n        X_val = X[val_idx]\n        y_val = y[val_idx]\n\n        print(f\"Train size: {X_train.shape}, Val size: {X_val.shape}\")\n\n        X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, scaler = scale_data(X_train, y_train, X_val, y_val, return_scaler=True)\n\n        self.train_loader, self.val_loader = create_dataloaders(X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, self.config[\"batch_size\"])\n\n        self.X_val = X_val_scaled\n        self.y_val = y_val_scaled\n\n        self.config = configure_train_config(self.config, X_train, self.train_loader, self.val_loader, split_idx, scaler)\n\n        self.initialize_model(self.config)\n        self.train()\n</code></pre>"},{"location":"api/training/#ibioml.trainer.DualOutputTrainer","title":"DualOutputTrainer","text":"<pre><code>DualOutputTrainer(config)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> Source code in <code>ibioml/trainer.py</code> <pre><code>def __init__(self, config):\n    super(DualOutputTrainer, self).__init__(config)\n    # A\u00f1adir campos para m\u00e9tricas separadas de posici\u00f3n y velocidad\n    self.metrics.update({\n        \"train_loss_pos\": [],\n        \"train_loss_vel\": [],\n        \"val_loss_pos\": [],\n        \"val_loss_vel\": [],\n        \"r2_score_pos\": [],\n        \"r2_score_vel\": []\n    })\n</code></pre>"},{"location":"api/training/#ibioml.trainer.DualOutputTrainer.calculate_loss","title":"calculate_loss","text":"<pre><code>calculate_loss(outputs, targets)\n</code></pre> <p>Calcular p\u00e9rdida combinada para modelos de doble salida.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def calculate_loss(self, outputs, targets):\n    \"\"\"Calcular p\u00e9rdida combinada para modelos de doble salida.\"\"\"\n    output1, output2 = outputs\n    target1, target2 = targets\n    loss1 = self.criterion(output1, target1.float())  # Loss posici\u00f3n\n    loss2 = self.criterion(output2, target2.float())  # Loss velocidad\n\n    # Guardar los valores individuales para uso posterior\n    self.current_pos_loss = loss1.item()\n    self.current_vel_loss = loss2.item()\n\n    return loss1 + loss2  # Devolver p\u00e9rdida combinada\n</code></pre>"},{"location":"api/training/#ibioml.trainer.DualOutputTrainer.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model()\n</code></pre> <p>Evaluar modelos de doble salida con m\u00e9tricas separadas.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def evaluate_model(self):\n    \"\"\"Evaluar modelos de doble salida con m\u00e9tricas separadas.\"\"\"\n    self.model.eval()\n    with torch.no_grad():\n        output1, output2 = self.model(self.X_val)\n\n        # Dividir objetivos de validaci\u00f3n\n        if isinstance(self.y_val, tuple):\n            y_val1, y_val2 = self.y_val\n        else:\n            # Verificar si tenemos exactamente 2 columnas (pos, vel)\n            if self.y_val.shape[1] == 2:\n                y_val1 = self.y_val[:, 0:1]\n                y_val2 = self.y_val[:, 1:2]\n            else:\n                target_size = self.y_val.shape[1] // 2\n                y_val1 = self.y_val[:, :target_size]\n                y_val2 = self.y_val[:, target_size:]\n\n        # Calcular p\u00e9rdida para cada salida\n        loss1 = self.criterion(output1, y_val1)  # Loss posici\u00f3n\n        loss2 = self.criterion(output2, y_val2)  # Loss velocidad\n\n        # P\u00e9rdida combinada\n        val_loss = (loss1 + loss2).item()\n\n        # Guardar loss individuales en m\u00e9tricas\n        val_loss_pos = loss1.item()\n        val_loss_vel = loss2.item()\n        self.metrics[\"val_loss_pos\"].append(val_loss_pos)\n        self.metrics[\"val_loss_vel\"].append(val_loss_vel)\n\n        # Calcular puntuaci\u00f3n R2 para ambas salidas\n        r2_pos = 1 - loss1.item() / torch.var(y_val1).item()  # R2 para posici\u00f3n\n        r2_vel = 1 - loss2.item() / torch.var(y_val2).item()  # R2 para velocidad\n\n        # Guardar R2 individuales en m\u00e9tricas\n        self.metrics[\"r2_score_pos\"].append(r2_pos)\n        self.metrics[\"r2_score_vel\"].append(r2_vel)\n\n        # Puntuaci\u00f3n R2 promedio\n        r2_score = (r2_pos + r2_vel) / 2\n\n        # Combinar predicciones para el reporte\n        y_pred1 = output1.cpu().numpy()\n        y_pred2 = output2.cpu().numpy()\n        y_pred = [y_pred1, y_pred2]\n\n    return val_loss, r2_score, y_pred\n</code></pre>"},{"location":"api/training/#ibioml.trainer.DualOutputTrainer.forward_pass","title":"forward_pass","text":"<pre><code>forward_pass(data)\n</code></pre> <p>Paso hacia adelante para modelos de doble salida.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def forward_pass(self, data):\n    \"\"\"Paso hacia adelante para modelos de doble salida.\"\"\"\n    return self.model(data)\n</code></pre>"},{"location":"api/training/#ibioml.trainer.DualOutputTrainer.process_target","title":"process_target","text":"<pre><code>process_target(target)\n</code></pre> <p>Procesar objetivo para modelos de doble salida, dividiendo si es necesario.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def process_target(self, target):\n    \"\"\"Procesar objetivo para modelos de doble salida, dividiendo si es necesario.\"\"\"\n    if isinstance(target, tuple):\n        # Caso donde target ya viene como tupla\n        return (target[0].to(self.config[\"device\"]), target[1].to(self.config[\"device\"]))\n    else:\n        # Caso especial: matriz con exactamente dos columnas (posici\u00f3n y velocidad)\n        if target.shape[1] == 2:  # Detectar cuando tenemos [posici\u00f3n, velocidad]\n            pos = target[:, 0:1].to(self.config[\"device\"])  # Primera columna (posici\u00f3n)\n            vel = target[:, 1:2].to(self.config[\"device\"])  # Segunda columna (velocidad)\n            return (pos, vel)\n        else:\n            # Caso original (para compatibilidad con implementaciones anteriores)\n            target_size = target.shape[1] // 2\n            target1 = target[:, :target_size].to(self.config[\"device\"])\n            target2 = target[:, target_size:].to(self.config[\"device\"])\n            return (target1, target2)\n</code></pre>"},{"location":"api/training/#ibioml.trainer.DualOutputTrainer.report_metrics","title":"report_metrics","text":"<pre><code>report_metrics(average_train_loss, val_loss, r2_score, epoch, average_train_loss_sin_norm=None, y_pred=None, epoch_time_ms=None)\n</code></pre> <p>Reportar m\u00e9tricas de entrenamiento incluyendo las separadas para posici\u00f3n y velocidad.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def report_metrics(self, average_train_loss, val_loss, r2_score, epoch, average_train_loss_sin_norm=None, y_pred=None, epoch_time_ms=None):\n    \"\"\"Reportar m\u00e9tricas de entrenamiento incluyendo las separadas para posici\u00f3n y velocidad.\"\"\"\n    # Primero llamar al m\u00e9todo base para reportar m\u00e9tricas est\u00e1ndar\n    super().report_metrics(average_train_loss, val_loss, r2_score, epoch, average_train_loss_sin_norm, y_pred, epoch_time_ms)\n\n    # Reportar m\u00e9tricas adicionales espec\u00edficas de posici\u00f3n y velocidad\n    pos_train_loss = self.metrics[\"train_loss_pos\"][-1]\n    vel_train_loss = self.metrics[\"train_loss_vel\"][-1]\n    pos_val_loss = self.metrics[\"val_loss_pos\"][-1]\n    vel_val_loss = self.metrics[\"val_loss_vel\"][-1]\n    pos_r2 = self.metrics[\"r2_score_pos\"][-1]\n    vel_r2 = self.metrics[\"r2_score_vel\"][-1]\n\n    print(f'Posici\u00f3n - Train Loss: {pos_train_loss:.4f}, Val Loss: {pos_val_loss:.4f}, R2: {pos_r2:.4f}')\n    print(f'Velocidad - Train Loss: {vel_train_loss:.4f}, Val Loss: {vel_val_loss:.4f}, R2: {vel_r2:.4f}')\n\n    # Si es el \u00faltimo epoch, guardar R2 individuales en los resultados finales\n    if self.done:\n        self.results[\"r2_score_pos\"] = pos_r2\n        self.results[\"r2_score_vel\"] = vel_r2\n</code></pre>"},{"location":"api/training/#ibioml.trainer.DualOutputTrainer.train_one_epoch","title":"train_one_epoch","text":"<pre><code>train_one_epoch()\n</code></pre> <p>Entrenar una \u00e9poca con seguimiento de m\u00e9tricas separadas.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def train_one_epoch(self):\n    \"\"\"Entrenar una \u00e9poca con seguimiento de m\u00e9tricas separadas.\"\"\"\n    self.model.train()\n    epoch_loss = 0\n    epoch_loss_sin_norm = 0\n    epoch_pos_loss = 0  # Acumulador para loss de posici\u00f3n\n    epoch_vel_loss = 0  # Acumulador para loss de velocidad\n\n    for batch_idx, (data, target) in enumerate(self.train_loader):\n        data = data.to(self.config[\"device\"])\n\n        processed_target = self.process_target(target)\n\n        self.optimizer.zero_grad()\n\n        outputs = self.forward_pass(data.float())\n\n        loss = self.calculate_loss(outputs, processed_target)\n\n        # Acumular las p\u00e9rdidas individuales\n        epoch_pos_loss += self.current_pos_loss\n        epoch_vel_loss += self.current_vel_loss\n\n        if self.config[\"reg_type\"] is not None:\n            epoch_loss_sin_norm += loss.item()\n            loss = self.apply_regularization(loss)\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n        self.optimizer.step()\n        epoch_loss += loss.item()\n\n    # Calcular promedios\n    average_train_loss = epoch_loss / len(self.train_loader)\n    average_pos_loss = epoch_pos_loss / len(self.train_loader)\n    average_vel_loss = epoch_vel_loss / len(self.train_loader)\n\n    # Guardar en m\u00e9tricas\n    self.metrics[\"train_loss_pos\"].append(average_pos_loss)\n    self.metrics[\"train_loss_vel\"].append(average_vel_loss)\n\n    if self.config[\"reg_type\"] is not None:\n        average_train_loss_sin_norm = epoch_loss_sin_norm / len(self.train_loader)\n        return average_train_loss, average_train_loss_sin_norm\n    else:\n        return average_train_loss, None\n</code></pre>"},{"location":"api/training/#ibioml.trainer.EarlyStopping","title":"EarlyStopping","text":"<pre><code>EarlyStopping(patience=5, min_delta=0, restore_best_weights=True)\n</code></pre> <p>Early stopping to stop the training when the loss does not improve after</p> <p>Parameters:</p> Name Type Description Default <code>patience</code> <code>int</code> <p>How many epochs to wait before stopping.</p> <code>5</code> <code>min_delta</code> <code>float</code> <p>Minimum change in the monitored quantity to qualify as an improvement.</p> <code>0</code> <code>restore_best_weights</code> <code>bool</code> <p>Whether to restore the best model weights when stopping.</p> <code>True</code> Source code in <code>ibioml/utils/trainer_funcs.py</code> <pre><code>def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n    self.patience = patience\n    self.min_delta = min_delta\n    self.restore_best_weights = restore_best_weights\n    self.best_model = None\n    self.best_loss = None\n    self.counter = 0\n    self.status = \"\"\n</code></pre>"},{"location":"api/training/#ibioml.trainer.SingleOutputTrainer","title":"SingleOutputTrainer","text":"<pre><code>SingleOutputTrainer(config)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> Source code in <code>ibioml/trainer.py</code> <pre><code>def __init__(self, config):\n    self.config = config\n    if \"train_loader\" in config and \"val_loader\" in config:\n        self.initialize_model(config)\n        self.train_loader = config[\"train_loader\"]\n        self.X_val, self.y_val = config[\"val_loader\"].dataset.tensors[0].to(self.config[\"device\"]), config[\"val_loader\"].dataset.tensors[1].to(self.config[\"device\"])\n    self.losses = []\n    self.metrics = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"r2_score\": [],\n        \"epoch\": [],\n        \"weight_mean\": [],\n        \"weight_std\": [],\n        \"grad_mean\": [],\n        \"grad_std\": [],\n        \"epoch_time_ms\": [],\n        \"split\": []\n    }\n    self.results = {}\n    self.results_dir = config[\"results_dir\"]\n    self.model_name = config[\"model_class\"].__name__\n    self.run_id = f\"{self.model_name}_{int(time.time())}\"\n    self.config[\"run_id\"] = self.run_id\n    self.config[\"model_name\"] = self.model_name\n</code></pre>"},{"location":"api/training/#ibioml.trainer.SingleOutputTrainer.calculate_loss","title":"calculate_loss","text":"<pre><code>calculate_loss(output, target)\n</code></pre> <p>Calcular p\u00e9rdida para modelos de salida \u00fanica.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def calculate_loss(self, output, target):\n    \"\"\"Calcular p\u00e9rdida para modelos de salida \u00fanica.\"\"\"\n    return self.criterion(output, target.float())\n</code></pre>"},{"location":"api/training/#ibioml.trainer.SingleOutputTrainer.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model()\n</code></pre> <p>Evaluar modelos de salida \u00fanica.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def evaluate_model(self):\n    \"\"\"Evaluar modelos de salida \u00fanica.\"\"\"\n    self.model.eval()\n    with torch.no_grad():\n        y_pred = self.model(self.X_val).cpu().numpy()\n        y_pred_tensor = torch.tensor(y_pred, dtype=torch.float32).to(self.config[\"device\"])\n        y_val_tensor = self.y_val.to(self.config[\"device\"])\n        val_loss = self.criterion(y_pred_tensor, y_val_tensor)\n        r2_score = 1 - val_loss.item() / torch.var(y_val_tensor).item()\n\n    return val_loss.item(), r2_score, y_pred\n</code></pre>"},{"location":"api/training/#ibioml.trainer.SingleOutputTrainer.forward_pass","title":"forward_pass","text":"<pre><code>forward_pass(data)\n</code></pre> <p>Paso hacia adelante para modelos de salida \u00fanica.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def forward_pass(self, data):\n    \"\"\"Paso hacia adelante para modelos de salida \u00fanica.\"\"\"\n    return self.model(data)\n</code></pre>"},{"location":"api/training/#ibioml.trainer.SingleOutputTrainer.process_target","title":"process_target","text":"<pre><code>process_target(target)\n</code></pre> <p>Procesar objetivo para modelos de salida \u00fanica.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def process_target(self, target):\n    \"\"\"Procesar objetivo para modelos de salida \u00fanica.\"\"\"\n    return target.to(self.config[\"device\"])\n</code></pre>"},{"location":"api/training/#ibioml.trainer.TrainerFactory","title":"TrainerFactory","text":""},{"location":"api/training/#ibioml.trainer.TrainerFactory.create_trainer","title":"create_trainer  <code>staticmethod</code>","text":"<pre><code>create_trainer(config)\n</code></pre> <p>Crear un entrenador basado en la clase de modelo en la configuraci\u00f3n.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>@staticmethod\ndef create_trainer(config):\n    \"\"\"Crear un entrenador basado en la clase de modelo en la configuraci\u00f3n.\"\"\"\n    model_class = config[\"model_class\"]\n\n    if model_class in [DualOutputMLPModel, DualOutputRNNModel, DualOutputGRUModel, DualOutputLSTMModel]:\n        return DualOutputTrainer(config)\n    else:\n        return SingleOutputTrainer(config)\n</code></pre>"},{"location":"api/training/#ibioml.trainer.configure_train_config","title":"configure_train_config","text":"<pre><code>configure_train_config(config, X_train, train_loader, val_loader, split_idx, scaler)\n</code></pre> <p>Configure the training parameters for the model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>The configuration dictionary.</p> required <code>X_train</code> <code>ndarray</code> <p>The training features.</p> required <code>train_loader</code> <code>DataLoader</code> <p>The training DataLoader.</p> required <code>val_loader</code> <code>DataLoader</code> <p>The validation DataLoader.</p> required <code>split_idx</code> <code>ndarray</code> <p>The indices of the training and validation split.</p> required <code>scaler</code> <code>StandardScaler</code> <p>The StandardScaler object.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The updated configuration dictionary.</p> Source code in <code>ibioml/utils/trainer_funcs.py</code> <pre><code>def configure_train_config(config, X_train, train_loader, val_loader, split_idx, scaler):\n    \"\"\"\n    Configure the training parameters for the model.\n\n    Args:\n        config (dict): The configuration dictionary.\n        X_train (np.ndarray): The training features.\n        train_loader (DataLoader): The training DataLoader.\n        val_loader (DataLoader): The validation DataLoader.\n        split_idx (np.ndarray): The indices of the training and validation split.\n        scaler (StandardScaler): The StandardScaler object.\n\n    Returns:\n        dict: The updated configuration dictionary.\n    \"\"\"\n    train_config = config.copy()\n    train_config[\"input_size\"] = X_train.shape[X_train.ndim-1]\n    train_config[\"train_loader\"] = train_loader\n    train_config[\"test_loader\"] = val_loader\n    train_config[\"split_idx\"] = split_idx\n    train_config[\"y_scaler\"] = scaler.y_scaler\n\n    return train_config\n</code></pre>"},{"location":"api/training/#ibioml.trainer.create_dataloaders","title":"create_dataloaders","text":"<pre><code>create_dataloaders(X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, batch_size)\n</code></pre> <p>Create the PyTorch DataLoader objects for training and validation. Works with both single-output and dual-output targets.</p> <p>Parameters:</p> Name Type Description Default <code>X_train_scaled</code> <p>The scaled training features.</p> required <code>y_train_scaled</code> <p>The scaled training target(s). Can be (n_samples, 1) or (n_samples, 2).</p> required <code>X_val_scaled</code> <p>The scaled validation features.</p> required <code>y_val_scaled</code> <p>The scaled validation target(s).</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>The training and validation DataLoader objects.</p> Source code in <code>ibioml/utils/trainer_funcs.py</code> <pre><code>def create_dataloaders(X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, batch_size):\n    \"\"\"\n    Create the PyTorch DataLoader objects for training and validation.\n    Works with both single-output and dual-output targets.\n\n    Args:\n        X_train_scaled: The scaled training features.\n        y_train_scaled: The scaled training target(s). Can be (n_samples, 1) or (n_samples, 2).\n        X_val_scaled: The scaled validation features.\n        y_val_scaled: The scaled validation target(s).\n        batch_size (int): The batch size to use.\n\n    Returns:\n        tuple: The training and validation DataLoader objects.\n    \"\"\"\n    # Asegurar que los datos son tensores PyTorch\n    if not isinstance(X_train_scaled, torch.Tensor):\n        X_train_scaled = torch.tensor(X_train_scaled, dtype=torch.float32)\n    else:\n        X_train_scaled = X_train_scaled.float()\n\n    if not isinstance(y_train_scaled, torch.Tensor):\n        y_train_scaled = torch.tensor(y_train_scaled, dtype=torch.float32)\n    else:\n        y_train_scaled = y_train_scaled.float()\n\n    if not isinstance(X_val_scaled, torch.Tensor):\n        X_val_scaled = torch.tensor(X_val_scaled, dtype=torch.float32)\n    else:\n        X_val_scaled = X_val_scaled.float()\n\n    if not isinstance(y_val_scaled, torch.Tensor):\n        y_val_scaled = torch.tensor(y_val_scaled, dtype=torch.float32)\n    else:\n        y_val_scaled = y_val_scaled.float()\n\n    # Crear los DataLoaders (ya no usamos clone().detach())\n    train_loader = torch.utils.data.DataLoader(\n        torch.utils.data.TensorDataset(X_train_scaled, y_train_scaled),\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True  # Asegurarse de que el \u00faltimo batch no se queda sin datos\n    )\n\n    val_loader = torch.utils.data.DataLoader(\n        torch.utils.data.TensorDataset(X_val_scaled, y_val_scaled),\n        batch_size=batch_size,\n        shuffle=False\n    )\n\n    return train_loader, val_loader\n</code></pre>"},{"location":"api/training/#ibioml.trainer.train_model","title":"train_model","text":"<pre><code>train_model(config)\n</code></pre> <p>Entrenar un modelo usando el entrenador apropiado.</p> Source code in <code>ibioml/trainer.py</code> <pre><code>def train_model(config):\n    \"\"\"Entrenar un modelo usando el entrenador apropiado.\"\"\"\n    trainer = TrainerFactory.create_trainer(config)\n\n    if \"n_splits\" in config:\n        print(\"n_splits:\", config[\"n_splits\"])\n        trainer.train_with_cv()\n        return trainer.results\n    else:   \n        trainer.train()\n        return trainer.results\n</code></pre>"},{"location":"api/training/#modulo-tuner","title":"M\u00f3dulo Tuner","text":""},{"location":"api/training/#ibioml.tuner","title":"ibioml.tuner","text":""},{"location":"api/training/#ibioml.tuner.Tuner","title":"Tuner","text":"<pre><code>Tuner(model_space=None, evaluator: ModelEvaluator = None)\n</code></pre> <p>Afinador de hiperpar\u00e1metros con validaci\u00f3n cruzada anidada.</p> <p>Esta clase maneja la optimizaci\u00f3n de hiperpar\u00e1metros usando Optuna y CV anidada. Soporta modelos de salida \u00fanica y dual.</p> <p>Inicializa el afinador con una configuraci\u00f3n de modelo opcional y un evaluador.</p> Source code in <code>ibioml/tuner.py</code> <pre><code>def __init__(self, model_space=None, evaluator: ModelEvaluator = None):\n    \"\"\"Inicializa el afinador con una configuraci\u00f3n de modelo opcional y un evaluador.\"\"\"\n    self.model_space = model_space\n    self.evaluator = evaluator\n    self.save_path = \"results\"\n    self.study_name = None\n    self.num_trials = 5\n    self.outer_folds = 5\n    self.inner_folds = 5\n    self.search_alg = \"bayes\"\n    self.search_space = None\n</code></pre>"},{"location":"api/training/#ibioml.tuner.Tuner.run","title":"run","text":"<pre><code>run(X, y, T)\n</code></pre> <p>Ejecuta la optimizaci\u00f3n de hiperpar\u00e1metros con CV anidada.</p> Source code in <code>ibioml/tuner.py</code> <pre><code>def run(self, X, y, T):\n    \"\"\"Ejecuta la optimizaci\u00f3n de hiperpar\u00e1metros con CV anidada.\"\"\"\n    # Setup directories\n    experiment_dir, training_results_dir = self._setup_directories()\n\n    # Initialize results storage\n    results = self._initialize_results()\n\n    # Run outer CV loop\n    self._run_outer_cv_loop(X, y, T, results, training_results_dir)\n\n    # Save final results\n    self._save_final_results(results, experiment_dir)\n\n    # Print summary\n    self._print_cv_summary(results)\n\n    return results\n</code></pre>"},{"location":"api/training/#ibioml.tuner.Tuner.set_evaluator","title":"set_evaluator","text":"<pre><code>set_evaluator(evaluator: ModelEvaluator)\n</code></pre> <p>Establece la instancia del evaluador.</p> Source code in <code>ibioml/tuner.py</code> <pre><code>def set_evaluator(self, evaluator: ModelEvaluator):\n    \"\"\"Establece la instancia del evaluador.\"\"\"\n    self.evaluator = evaluator\n    return self\n</code></pre>"},{"location":"api/training/#ibioml.tuner.Tuner.set_model_space","title":"set_model_space","text":"<pre><code>set_model_space(model_space)\n</code></pre> <p>Establece la configuraci\u00f3n del espacio de modelos.</p> Source code in <code>ibioml/tuner.py</code> <pre><code>def set_model_space(self, model_space):\n    \"\"\"Establece la configuraci\u00f3n del espacio de modelos.\"\"\"\n    self.model_space = model_space\n    return self\n</code></pre>"},{"location":"api/training/#ibioml.tuner.Tuner.set_study_params","title":"set_study_params","text":"<pre><code>set_study_params(save_path='results', study_name=None, num_trials=5, outer_folds=5, inner_folds=5, search_alg='bayes', search_space=None)\n</code></pre> <p>Establece todos los par\u00e1metros del estudio a la vez.</p> Source code in <code>ibioml/tuner.py</code> <pre><code>def set_study_params(self, save_path=\"results\", study_name=None, num_trials=5, \n                    outer_folds=5, inner_folds=5, search_alg=\"bayes\", \n                    search_space=None):\n    \"\"\"Establece todos los par\u00e1metros del estudio a la vez.\"\"\"\n    self.save_path = save_path\n    self.study_name = study_name\n    self.num_trials = num_trials\n    self.outer_folds = outer_folds\n    self.inner_folds = inner_folds\n    self.search_alg = search_alg\n    self.search_space = search_space\n    return self\n</code></pre>"},{"location":"api/training/#ibioml.tuner.create_dataloaders","title":"create_dataloaders","text":"<pre><code>create_dataloaders(X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, batch_size)\n</code></pre> <p>Create the PyTorch DataLoader objects for training and validation. Works with both single-output and dual-output targets.</p> <p>Parameters:</p> Name Type Description Default <code>X_train_scaled</code> <p>The scaled training features.</p> required <code>y_train_scaled</code> <p>The scaled training target(s). Can be (n_samples, 1) or (n_samples, 2).</p> required <code>X_val_scaled</code> <p>The scaled validation features.</p> required <code>y_val_scaled</code> <p>The scaled validation target(s).</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>The training and validation DataLoader objects.</p> Source code in <code>ibioml/utils/trainer_funcs.py</code> <pre><code>def create_dataloaders(X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, batch_size):\n    \"\"\"\n    Create the PyTorch DataLoader objects for training and validation.\n    Works with both single-output and dual-output targets.\n\n    Args:\n        X_train_scaled: The scaled training features.\n        y_train_scaled: The scaled training target(s). Can be (n_samples, 1) or (n_samples, 2).\n        X_val_scaled: The scaled validation features.\n        y_val_scaled: The scaled validation target(s).\n        batch_size (int): The batch size to use.\n\n    Returns:\n        tuple: The training and validation DataLoader objects.\n    \"\"\"\n    # Asegurar que los datos son tensores PyTorch\n    if not isinstance(X_train_scaled, torch.Tensor):\n        X_train_scaled = torch.tensor(X_train_scaled, dtype=torch.float32)\n    else:\n        X_train_scaled = X_train_scaled.float()\n\n    if not isinstance(y_train_scaled, torch.Tensor):\n        y_train_scaled = torch.tensor(y_train_scaled, dtype=torch.float32)\n    else:\n        y_train_scaled = y_train_scaled.float()\n\n    if not isinstance(X_val_scaled, torch.Tensor):\n        X_val_scaled = torch.tensor(X_val_scaled, dtype=torch.float32)\n    else:\n        X_val_scaled = X_val_scaled.float()\n\n    if not isinstance(y_val_scaled, torch.Tensor):\n        y_val_scaled = torch.tensor(y_val_scaled, dtype=torch.float32)\n    else:\n        y_val_scaled = y_val_scaled.float()\n\n    # Crear los DataLoaders (ya no usamos clone().detach())\n    train_loader = torch.utils.data.DataLoader(\n        torch.utils.data.TensorDataset(X_train_scaled, y_train_scaled),\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True  # Asegurarse de que el \u00faltimo batch no se queda sin datos\n    )\n\n    val_loader = torch.utils.data.DataLoader(\n        torch.utils.data.TensorDataset(X_val_scaled, y_val_scaled),\n        batch_size=batch_size,\n        shuffle=False\n    )\n\n    return train_loader, val_loader\n</code></pre>"},{"location":"api/training/#ibioml.tuner.initialize_config","title":"initialize_config","text":"<pre><code>initialize_config(config, X_train, y_train, X_val, y_val, get_scaler=False)\n</code></pre> <p>Inicializa la configuraci\u00f3n para el entrenamiento. Maneja autom\u00e1ticamente modelos de salida \u00fanica y dual.</p> Source code in <code>ibioml/utils/tuner_funcs.py</code> <pre><code>def initialize_config(config, X_train, y_train, X_val, y_val, get_scaler=False):\n    \"\"\"\n    Inicializa la configuraci\u00f3n para el entrenamiento.\n    Maneja autom\u00e1ticamente modelos de salida \u00fanica y dual.\n    \"\"\"\n    # Detectar autom\u00e1ticamente la dimensi\u00f3n de salida\n    if y_train.ndim &gt; 1 and y_train.shape[1] == 2:\n        y_dim = 2\n    else:\n        y_dim = 1\n\n    # Escalar datos\n    X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, scaler = scale_data(\n        X_train, y_train, X_val, y_val, return_scaler=True\n    )\n\n    # Crear DataLoaders\n    train_loader, val_loader = create_dataloaders(\n        X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, config[\"batch_size\"]\n    )\n\n    # Configurar modelo y otros par\u00e1metros\n    train_config = copy.deepcopy(config)\n    train_config[\"input_size\"] = X_train.shape[X_train.ndim-1]\n    train_config[\"train_loader\"] = train_loader\n    train_config[\"val_loader\"] = val_loader\n    train_config[\"scaler\"] = scaler  # Store scaler in config\n\n    # Configuraci\u00f3n espec\u00edfica seg\u00fan la dimensi\u00f3n de salida\n    if y_dim == 2:\n        train_config[\"output_size\"] = 1  # Cada cabeza predice un valor\n    else:\n        train_config[\"y_scaler\"] = scaler.target_scaler\n\n    if get_scaler:\n        return train_config, scaler\n    else:\n        return train_config\n</code></pre>"},{"location":"api/training/#ibioml.tuner.make_serializable","title":"make_serializable","text":"<pre><code>make_serializable(obj)\n</code></pre> <p>Convierte un objeto complejo a una forma serializable en JSON.</p> Source code in <code>ibioml/utils/tuner_funcs.py</code> <pre><code>def make_serializable(obj):\n    \"\"\"Convierte un objeto complejo a una forma serializable en JSON.\"\"\"\n    if isinstance(obj, dict):\n        return {k: make_serializable(v) for k, v in obj.items() \n                if not callable(v) and k not in ['device', 'model_class', 'train_loader', 'val_loader', 'y_scaler', 'scaler']}\n    elif isinstance(obj, list):\n        return [make_serializable(item) for item in obj]\n    elif not isinstance(obj, (str, int, float, bool, type(None))):\n        return str(obj)\n    else:\n        return obj\n</code></pre>"},{"location":"api/training/#ibioml.tuner.run_study","title":"run_study","text":"<pre><code>run_study(X, y, T, model_space, num_trials=5, outer_folds=5, inner_folds=5, save_path='results', search_alg='bayes', search_space=None, study_name=None)\n</code></pre> <p>Ejecuta una validaci\u00f3n cruzada anidada para la optimizaci\u00f3n de hiperpar\u00e1metros utilizando Optuna.</p> <p>Esta funci\u00f3n realiza una b\u00fasqueda de hiperpar\u00e1metros mediante Optuna, empleando validaci\u00f3n cruzada anidada. Permite la evaluaci\u00f3n de modelos con salida simple o m\u00faltiple, y almacena los resultados y configuraciones \u00f3ptimas.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Matriz de caracter\u00edsticas de entrada.</p> required <code>y</code> <code>ndarray</code> <p>Matriz o vector de etiquetas objetivo.</p> required <code>T</code> <code>ndarray</code> <p>Vector de marcadores de ensayo para la divisi\u00f3n de folds.</p> required <code>model_space</code> <code>dict</code> <p>Diccionario con la configuraci\u00f3n base del modelo y par\u00e1metros fijos.</p> required <code>num_trials</code> <code>int</code> <p>N\u00famero de pruebas de Optuna por fold interno. Por defecto 5.</p> <code>5</code> <code>outer_folds</code> <code>int</code> <p>N\u00famero de folds para la validaci\u00f3n cruzada externa. Por defecto 5.</p> <code>5</code> <code>inner_folds</code> <code>int</code> <p>N\u00famero de folds para la validaci\u00f3n cruzada interna. Por defecto 5.</p> <code>5</code> <code>save_path</code> <code>str</code> <p>Ruta base donde guardar los resultados. Por defecto \"results\".</p> <code>'results'</code> <code>search_alg</code> <code>str</code> <p>Algoritmo de b\u00fasqueda de Optuna (\"bayes\" o \"grid\"). Por defecto \"bayes\".</p> <code>'bayes'</code> <code>search_space</code> <code>dict</code> <p>Espacio de b\u00fasqueda para GridSampler. Por defecto None.</p> <code>None</code> <code>study_name</code> <code>str</code> <p>Nombre del estudio para la carpeta de resultados. Por defecto None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True si la optimizaci\u00f3n se ejecut\u00f3 correctamente.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Si el diccionario model_space no contiene exactamente las claves requeridas.</p> Source code in <code>ibioml/tuner.py</code> <pre><code>def run_study(X, y, T, model_space, num_trials=5, outer_folds=5, inner_folds=5, save_path=\"results\", search_alg=\"bayes\", search_space=None, study_name=None):\n    \"\"\"\n    Ejecuta una validaci\u00f3n cruzada anidada para la optimizaci\u00f3n de hiperpar\u00e1metros utilizando Optuna.\n\n    Esta funci\u00f3n realiza una b\u00fasqueda de hiperpar\u00e1metros mediante Optuna, empleando validaci\u00f3n cruzada anidada.\n    Permite la evaluaci\u00f3n de modelos con salida simple o m\u00faltiple, y almacena los resultados y configuraciones \u00f3ptimas.\n\n    Args:\n        X (np.ndarray): Matriz de caracter\u00edsticas de entrada.\n        y (np.ndarray): Matriz o vector de etiquetas objetivo.\n        T (np.ndarray): Vector de marcadores de ensayo para la divisi\u00f3n de folds.\n        model_space (dict): Diccionario con la configuraci\u00f3n base del modelo y par\u00e1metros fijos.\n        num_trials (int, optional): N\u00famero de pruebas de Optuna por fold interno. Por defecto 5.\n        outer_folds (int, optional): N\u00famero de folds para la validaci\u00f3n cruzada externa. Por defecto 5.\n        inner_folds (int, optional): N\u00famero de folds para la validaci\u00f3n cruzada interna. Por defecto 5.\n        save_path (str, optional): Ruta base donde guardar los resultados. Por defecto \"results\".\n        search_alg (str, optional): Algoritmo de b\u00fasqueda de Optuna (\"bayes\" o \"grid\"). Por defecto \"bayes\".\n        search_space (dict, optional): Espacio de b\u00fasqueda para GridSampler. Por defecto None.\n        study_name (str, optional): Nombre del estudio para la carpeta de resultados. Por defecto None.\n\n    Returns:\n        bool: True si la optimizaci\u00f3n se ejecut\u00f3 correctamente.\n\n    Raises:\n        ValueError: Si el diccionario model_space no contiene exactamente las claves requeridas.\n    \"\"\"\n    # Separa fijos y optimizables\n    fixed_space, auto_search_space = split_model_space(model_space)\n    # Usa el search_space expl\u00edcito si lo pasan, sino el generado\n    search_space = search_space or auto_search_space\n\n    # Solo chequea los m\u00ednimos requeridos\n    required_keys = {\n        \"model_class\",\n        \"output_size\",\n        \"device\",\n        \"num_epochs\",\n        \"es_patience\",\n        \"reg_type\",\n        \"lambda_reg\",\n        \"batch_size\",\n    }\n    missing = required_keys - set(fixed_space.keys())\n    if missing:\n        raise ValueError(\n            f\"model_space must contain at least these keys: {sorted(required_keys)}. \"\n            f\"Missing: {sorted(missing)}\"\n        )\n\n    evaluator = create_evaluator(y.shape[1], fixed_space[\"device\"])\n    model_class = create_model_class(fixed_space[\"model_class\"], y.shape[1])\n    fixed_space[\"model_class\"] = model_class\n\n    tuner = Tuner(fixed_space, evaluator)\n    tuner.set_study_params(\n        save_path=save_path,\n        study_name=study_name,\n        num_trials=num_trials,\n        outer_folds=outer_folds,\n        inner_folds=inner_folds,\n        search_alg=search_alg,\n        search_space=search_space\n    )\n    tuner.run(X, y, T)\n    return True\n</code></pre>"},{"location":"api/training/#ibioml.tuner.scale_data","title":"scale_data","text":"<pre><code>scale_data(X_train, y_train, X_test, y_test, return_scaler=False)\n</code></pre> <p>Escala los datos de forma autom\u00e1tica seg\u00fan sus caracter\u00edsticas.</p> <p>Parameters:</p> Name Type Description Default <code>X_train,</code> <code>(y_train, X_test, y_test)</code> <p>Datos a escalar</p> required <code>return_scaler</code> <p>Si True, devuelve tambi\u00e9n el escalador</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Datos escalados y opcionalmente el escalador</p> Source code in <code>ibioml/utils/data_scaler.py</code> <pre><code>def scale_data(X_train, y_train, X_test, y_test, return_scaler=False):\n    \"\"\"\n    Escala los datos de forma autom\u00e1tica seg\u00fan sus caracter\u00edsticas.\n\n    Args:\n        X_train, y_train, X_test, y_test: Datos a escalar\n        return_scaler: Si True, devuelve tambi\u00e9n el escalador\n\n    Returns:\n        tuple: Datos escalados y opcionalmente el escalador\n    \"\"\"\n    # Crear el escalador adecuado\n    scaler = create_scaler(X_train, y_train)\n\n    # Escalar los datos\n    X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled = scaler.standardize(\n        X_train, y_train, X_test, y_test\n    )\n\n    if return_scaler:\n        return X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, scaler\n\n    return X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled\n</code></pre>"},{"location":"api/training/#ibioml.tuner.split_model_space","title":"split_model_space","text":"<pre><code>split_model_space(model_space)\n</code></pre> <p>Separa los hiperpar\u00e1metros fijos de los optimizables (tuplas) en model_space. Devuelve (fixed_space, search_space)</p> Source code in <code>ibioml/utils/tuner_funcs.py</code> <pre><code>def split_model_space(model_space):\n    \"\"\"\n    Separa los hiperpar\u00e1metros fijos de los optimizables (tuplas) en model_space.\n    Devuelve (fixed_space, search_space)\n    \"\"\"\n    fixed_keys = {\n        \"model_class\", \"output_size\", \"device\", \"num_epochs\",\n        \"es_patience\", \"reg_type\", \"lambda_reg\", \"batch_size\"\n    }\n    fixed = {k: v for k, v in model_space.items() if k in fixed_keys or not isinstance(v, tuple)}\n    search_space = {}\n    for k, v in model_space.items():\n        if isinstance(v, tuple):\n            # INT: (int, low, high, [step])\n            if v[0] == int:\n                if len(v) == 4:\n                    search_space[k] = {\"type\": \"int\", \"low\": v[1], \"high\": v[2], \"step\": v[3]}\n                else:\n                    search_space[k] = {\"type\": \"int\", \"low\": v[1], \"high\": v[2]}\n            # FLOAT: (float, low, high, [log])\n            elif v[0] == float:\n                if len(v) == 4:\n                    search_space[k] = {\"type\": \"float\", \"low\": v[1], \"high\": v[2], \"log\": v[3]}\n                else:\n                    search_space[k] = {\"type\": \"float\", \"low\": v[1], \"high\": v[2]}\n    return fixed, search_space\n</code></pre>"},{"location":"examples/basic_tutorial/","title":"Tutorial B\u00e1sico","text":"<p>Este tutorial te guiar\u00e1 paso a paso para ejecutar tu primer experimento de neurodecodificaci\u00f3n con IBioML.</p>"},{"location":"examples/basic_tutorial/#preparacion-del-entorno","title":"Preparaci\u00f3n del Entorno","text":""},{"location":"examples/basic_tutorial/#1-instalacion","title":"1. Instalaci\u00f3n","text":"<pre><code>pip install ibioml\n</code></pre>"},{"location":"examples/basic_tutorial/#2-descargar-datos-de-ejemplo","title":"2. Descargar Datos de Ejemplo","text":"<p>Para este tutorial, asumiremos que tienes un archivo <code>.mat</code> con datos neuronales. Si no tienes datos propios, puedes usar datos sint\u00e9ticos:</p> <pre><code>import numpy as np\nimport scipy.io as sio\n\n# Generar datos sint\u00e9ticos para el tutorial\nnp.random.seed(42)\nn_samples = 10000\nn_neurons = 50\nn_trials = 100\n\n# Simular actividad neuronal\nneural_activity = np.random.poisson(0.1, (n_samples, n_neurons))\n\n# Simular posici\u00f3n y velocidad\ntime = np.linspace(0, 100, n_samples)\nposition = 5 * np.sin(0.1 * time) + np.random.normal(0, 0.5, n_samples)\nvelocity = np.gradient(position) + np.random.normal(0, 0.2, n_samples)\n\n# Simular contexto de recompensa\nreward_context = np.random.choice([0, 1], n_samples, p=[0.6, 0.4])\n\n# Crear marcadores de trials\ntrial_final_bins = np.sort(np.random.choice(\n    range(n_samples-100), n_trials, replace=False\n)) + 100\n\n# Simular d-prime (medida de rendimiento)\nd_prime = np.random.normal(2.8, 0.5, n_trials)\n\n# Simular duraci\u00f3n de trials\ntrial_durations = np.diff(np.concatenate([[0], trial_final_bins]))\n\n# Guardar como archivo .mat\nsynthetic_data = {\n    'neuronActivity': neural_activity,\n    'position': position.reshape(-1, 1),\n    'velocity': velocity.reshape(-1, 1),\n    'rewCtxt': reward_context.reshape(-1, 1),\n    'trialFinalBin': trial_final_bins,\n    'dPrime': d_prime,\n    'criterion': np.random.normal(0, 0.3, n_trials),\n    'trialDurationInBins': trial_durations\n}\n\n# Crear directorio si no existe\nimport os\nos.makedirs('tutorial_data', exist_ok=True)\n\n# Guardar datos\nsio.savemat('tutorial_data/synthetic_experiment.mat', synthetic_data)\nprint(\"\u2705 Datos sint\u00e9ticos generados en 'tutorial_data/synthetic_experiment.mat'\")\n</code></pre>"},{"location":"examples/basic_tutorial/#paso-1-preprocesamiento-de-datos","title":"Paso 1: Preprocesamiento de Datos","text":"<pre><code>from ibioml.preprocess_data import preprocess_data\nimport os\n\n# Crear directorio para datos procesados\nos.makedirs('tutorial_data/processed', exist_ok=True)\n\n# Preprocesar los datos\npreprocess_data(\n    file_path='tutorial_data/synthetic_experiment.mat',\n    file_name_to_save='tutorial_data/processed/experiment',\n    bins_before=5,      # 5 bins hacia atr\u00e1s\n    bins_after=5,       # 5 bins hacia adelante\n    bins_current=1,     # 1 bin actual\n    threshDPrime=2.5,   # Filtrar trials con d' &lt; 2.5\n    firingMinimo=50     # M\u00ednimo 50 spikes por neurona\n)\n\nprint(\"\u2705 Preprocesamiento completado\")\n</code></pre> <p>Esto generar\u00e1 12 archivos <code>.pickle</code> con diferentes configuraciones: - Con/sin contexto de recompensa - Solo posici\u00f3n, solo velocidad, o ambos - Datos aplanados (<code>_flat</code>) para MLP o datos temporales para RNNs</p>"},{"location":"examples/basic_tutorial/#paso-2-cargar-datos-procesados","title":"Paso 2: Cargar Datos Procesados","text":"<pre><code>import pickle\n\n# Cargar datos para decodificaci\u00f3n de posici\u00f3n con MLP\nwith open('tutorial_data/processed/experiment_withCtxt_onlyPosition_flat.pickle', 'rb') as f:\n    X, y, trial_markers = pickle.load(f)\n\nprint(f\"Forma de X (datos de entrada): {X.shape}\")\nprint(f\"Forma de y (objetivo): {y.shape}\")\nprint(f\"N\u00famero de trials \u00fanicos: {len(np.unique(trial_markers))}\")\n</code></pre>"},{"location":"examples/basic_tutorial/#paso-3-configurar-el-experimento","title":"Paso 3: Configurar el Experimento","text":"<pre><code>from ibioml.models import MLPModel\n\n# Configuraci\u00f3n del modelo con optimizaci\u00f3n de hiperpar\u00e1metros\nconfig = {\n    # Par\u00e1metros fijos del modelo\n    \"model_class\": MLPModel,\n    \"output_size\": 1,           # Decodificar solo posici\u00f3n\n    \"device\": \"cpu\",            # Usar CPU para el tutorial\n    \"num_epochs\": 100,          # Pocas \u00e9pocas para rapidez\n    \"es_patience\": 10,          # Early stopping\n    \"reg_type\": None,           # Sin regularizaci\u00f3n\n    \"lambda_reg\": None,\n    \"batch_size\": 32,\n\n    # Hiperpar\u00e1metros a optimizar (formato: tipo, min, max, step/log)\n    \"hidden_size\": (int, 64, 256, 32),     # Entre 64 y 256, paso 32\n    \"num_layers\": (int, 1, 3),             # Entre 1 y 3 capas\n    \"dropout\": (float, 0.0, 0.5),          # Dropout entre 0 y 0.5\n    \"lr\": (float, 1e-4, 1e-2, True),       # Learning rate (escala log)\n}\n\nprint(\"\u2705 Configuraci\u00f3n del modelo lista\")\n</code></pre>"},{"location":"examples/basic_tutorial/#paso-4-ejecutar-el-experimento","title":"Paso 4: Ejecutar el Experimento","text":"<pre><code>from ibioml.tuner import run_study\nimport os\n\n# Crear directorio para resultados\nos.makedirs('tutorial_results', exist_ok=True)\n\n# Ejecutar experimento (versi\u00f3n r\u00e1pida para tutorial)\nprint(\"\ud83d\ude80 Iniciando experimento...\")\nrun_study(\n    X, y, trial_markers,\n    model_space=config,\n    num_trials=5,           # Solo 5 configuraciones para rapidez\n    outer_folds=3,          # 3-fold cross-validation\n    inner_folds=1,          # Sin CV interna para rapidez\n    save_path=\"tutorial_results/position_decoding\",\n    search_alg=\"random\"     # B\u00fasqueda aleatoria\n)\n\nprint(\"\u2705 Experimento completado!\")\n</code></pre>"},{"location":"examples/basic_tutorial/#paso-5-analizar-resultados","title":"Paso 5: Analizar Resultados","text":"<pre><code>import json\nimport os\n\n# Encontrar la carpeta de resultados m\u00e1s reciente\nresults_base = \"tutorial_results/position_decoding\"\nstudy_folders = [f for f in os.listdir(results_base) if f.startswith('study_')]\nlatest_study = sorted(study_folders)[-1]\nresults_path = os.path.join(results_base, latest_study)\n\n# Cargar resultados finales\nwith open(os.path.join(results_path, 'final_results.json'), 'r') as f:\n    final_results = json.load(f)\n\nprint(\"\ud83d\udcca Resultados del Experimento:\")\nprint(f\"   Mejor R\u00b2 obtenido: {final_results['best_r2_score_test']:.4f}\")\nprint(f\"   R\u00b2 promedio: {final_results['mean_r2_test']:.4f} \u00b1 {final_results['std_r2_test']:.4f}\")\nprint(f\"   Mejores hiperpar\u00e1metros:\")\nfor param, value in final_results['best_params'].items():\n    print(f\"     {param}: {value}\")\n</code></pre>"},{"location":"examples/basic_tutorial/#paso-6-visualizacion-basica","title":"Paso 6: Visualizaci\u00f3n B\u00e1sica","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# Crear un gr\u00e1fico simple de los resultados\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Gr\u00e1fico 1: R\u00b2 scores por fold\nfold_r2_scores = []\nfor fold_idx in range(3):  # 3 folds en nuestro experimento\n    fold_path = os.path.join(results_path, 'training_results', f'fold_{fold_idx}', 'results.json')\n    if os.path.exists(fold_path):\n        with open(fold_path, 'r') as f:\n            fold_data = json.load(f)\n            fold_r2_scores.append(fold_data.get('r2_score', 0))\n\nax1.bar(range(len(fold_r2_scores)), fold_r2_scores, color='skyblue', alpha=0.7)\nax1.set_xlabel('Fold')\nax1.set_ylabel('R\u00b2 Score')\nax1.set_title('R\u00b2 Score por Fold')\nax1.set_ylim(0, 1)\n\n# Gr\u00e1fico 2: Distribuci\u00f3n de una muestra de datos\nsample_indices = np.random.choice(len(X), 1000, replace=False)\nX_sample = X[sample_indices]\ny_sample = y[sample_indices]\n\nax2.scatter(np.mean(X_sample, axis=1), y_sample, alpha=0.5, s=10)\nax2.set_xlabel('Actividad Neuronal Promedio')\nax2.set_ylabel('Posici\u00f3n')\nax2.set_title('Relaci\u00f3n Actividad-Posici\u00f3n (Muestra)')\n\nplt.tight_layout()\nplt.savefig('tutorial_results/experiment_summary.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"\ud83d\udcc8 Gr\u00e1ficos guardados en 'tutorial_results/experiment_summary.png'\")\n</code></pre>"},{"location":"examples/basic_tutorial/#paso-7-comparar-diferentes-configuraciones","title":"Paso 7: Comparar Diferentes Configuraciones","text":"<pre><code># Experimento adicional: comparar con y sin contexto\nprint(\"\ud83d\udd04 Ejecutando experimento sin contexto para comparaci\u00f3n...\")\n\n# Cargar datos sin contexto\nwith open('tutorial_data/processed/experiment_onlyPosition_flat.pickle', 'rb') as f:\n    X_no_ctx, y_no_ctx, T_no_ctx = pickle.load(f)\n\n# Mismo config pero para datos sin contexto\nconfig_no_ctx = config.copy()\n\nrun_study(\n    X_no_ctx, y_no_ctx, T_no_ctx,\n    model_space=config_no_ctx,\n    num_trials=5,\n    outer_folds=3,\n    inner_folds=1,\n    save_path=\"tutorial_results/position_decoding_no_context\",\n    search_alg=\"random\"\n)\n\nprint(\"\u2705 Experimento de comparaci\u00f3n completado!\")\n\n# Comparar resultados\nresults_no_ctx_base = \"tutorial_results/position_decoding_no_context\"\nstudy_folders_no_ctx = [f for f in os.listdir(results_no_ctx_base) if f.startswith('study_')]\nlatest_study_no_ctx = sorted(study_folders_no_ctx)[-1]\n\nwith open(os.path.join(results_no_ctx_base, latest_study_no_ctx, 'final_results.json'), 'r') as f:\n    final_results_no_ctx = json.load(f)\n\nprint(\"\\n\ud83d\udcca Comparaci\u00f3n de Resultados:\")\nprint(f\"Con contexto:    R\u00b2 = {final_results['best_r2_score_test']:.4f}\")\nprint(f\"Sin contexto:    R\u00b2 = {final_results_no_ctx['best_r2_score_test']:.4f}\")\nprint(f\"Mejora:          {final_results['best_r2_score_test'] - final_results_no_ctx['best_r2_score_test']:.4f}\")\n</code></pre>"},{"location":"examples/basic_tutorial/#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<p>\u00a1Felicitaciones! Has completado tu primer experimento con IBioML. Ahora puedes:</p>"},{"location":"examples/basic_tutorial/#explorar-diferentes-modelos","title":"\ud83e\udde0 Explorar Diferentes Modelos","text":"<pre><code>from ibioml.models import LSTMModel, GRUModel\n\n# Para datos temporales (sin '_flat'), usar modelos recurrentes\nlstm_config = config.copy()\nlstm_config[\"model_class\"] = LSTMModel\n# ... ejecutar experimento con LSTM\n</code></pre>"},{"location":"examples/basic_tutorial/#analisis-avanzado","title":"\ud83d\udcca An\u00e1lisis Avanzado","text":"<ul> <li>Usar la nueva clase <code>Visualizer</code> para an\u00e1lisis m\u00e1s detallados</li> <li>Comparar m\u00faltiples experimentos simult\u00e1neamente</li> <li>Analizar importancia de caracter\u00edsticas</li> </ul>"},{"location":"examples/basic_tutorial/#optimizacion","title":"\u2699\ufe0f Optimizaci\u00f3n","text":"<ul> <li>Aumentar <code>num_trials</code> para mejor optimizaci\u00f3n</li> <li>Usar <code>search_alg=\"bayes\"</code> para optimizaci\u00f3n bayesiana</li> <li>Implementar validaci\u00f3n cruzada anidada completa</li> </ul>"},{"location":"examples/basic_tutorial/#recursos-adicionales","title":"\ud83d\udcda Recursos Adicionales","text":"<ul> <li>Gu\u00eda de Experimentos Avanzados</li> <li>Documentaci\u00f3n de Modelos</li> <li>Visualizaci\u00f3n de Resultados</li> </ul> <p>\ud83d\udca1 Consejo: Para experimentos de producci\u00f3n, usa configuraciones m\u00e1s robustas con m\u00e1s trials, m\u00e1s folds y \u00e9pocas de entrenamiento m\u00e1s largas.</p>"},{"location":"examples/full_experiment/","title":"Experimento Completo","text":"<p>Este tutorial muestra c\u00f3mo realizar un experimento completo de neurodecodificaci\u00f3n, desde el preprocesamiento de datos hasta la visualizaci\u00f3n de resultados.</p>"},{"location":"examples/full_experiment/#prerrequisitos","title":"Prerrequisitos","text":"<p>Aseg\u00farate de tener instalado IBioML y sus dependencias:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"examples/full_experiment/#1-preparacion-de-datos","title":"1. Preparaci\u00f3n de Datos","text":""},{"location":"examples/full_experiment/#cargar-archivo-mat","title":"Cargar archivo .mat","text":"<pre><code>from ibioml.preprocessing import preprocess_data\n\n# Preprocesar los datos\npreprocess_data(\n    file_path='datasets/L5_bins200ms_completo.mat',\n    file_name_to_save='bins200ms_preprocessed',\n    bins_before=5,\n    bins_after=5,\n    bins_current=1,\n    threshDPrime=2.5,\n    firingMinimo=1000\n)\n</code></pre>"},{"location":"examples/full_experiment/#cargar-datos-preprocesados","title":"Cargar datos preprocesados","text":"<pre><code>import pickle\n\n# Cargar datos para modelos no recurrentes (MLP)\nwith open('data/bins200ms_preprocessed_withCtxt_flat.pickle', 'rb') as f:\n    X_flat, y_flat, T = pickle.load(f)\n\n# Cargar datos para modelos recurrentes (RNN/LSTM)\nwith open('data/bins200ms_preprocessed_withCtxt.pickle', 'rb') as f:\n    X_tensor, y_tensor, T_tensor = pickle.load(f)\n\nprint(f\"Forma de datos planos: {X_flat.shape}\")\nprint(f\"Forma de datos tensoriales: {X_tensor.shape}\")\n</code></pre>"},{"location":"examples/full_experiment/#2-configuracion-del-experimento","title":"2. Configuraci\u00f3n del Experimento","text":""},{"location":"examples/full_experiment/#definir-modelos-a-comparar","title":"Definir modelos a comparar","text":"<pre><code>from ibioml.models import MLPModel, LSTMModel\nfrom ibioml.utils.model_factory import create_model_class\n\n# Configuraci\u00f3n para MLP\nmlp_space = {\n    \"model_class\": create_model_class(MLPModel, y_flat.shape[1]),\n    \"output_size\": 2,  # posici\u00f3n y velocidad\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"num_epochs\": 200,\n    \"batch_size\": 32,\n    \"hidden_layer_sizes\": [128, 64],\n    \"dropout_rate\": 0.3,\n    \"learning_rate\": 0.001\n}\n\n# Configuraci\u00f3n para LSTM\nlstm_space = {\n    \"model_class\": create_model_class(LSTMModel, y_tensor.shape[1]),\n    \"output_size\": 2,\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"num_epochs\": 200,\n    \"batch_size\": 32,\n    \"hidden_size\": 64,\n    \"num_layers\": 2,\n    \"dropout_rate\": 0.3,\n    \"learning_rate\": 0.001\n}\n</code></pre>"},{"location":"examples/full_experiment/#3-ejecutar-experimentos","title":"3. Ejecutar Experimentos","text":""},{"location":"examples/full_experiment/#experimento-con-mlp","title":"Experimento con MLP","text":"<pre><code>from ibioml.tuner import run_study\n\n# Ejecutar estudio para MLP\nmlp_results = run_study(\n    X_flat, y_flat, T,\n    model_space=mlp_space,\n    num_trials=10,\n    outer_folds=5,\n    inner_folds=3,\n    save_path=\"results/mlp_comparison\"\n)\n</code></pre>"},{"location":"examples/full_experiment/#experimento-con-lstm","title":"Experimento con LSTM","text":"<pre><code># Ejecutar estudio para LSTM\nlstm_results = run_study(\n    X_tensor, y_tensor, T_tensor,\n    model_space=lstm_space,\n    num_trials=10,\n    outer_folds=5,\n    inner_folds=3,\n    save_path=\"results/lstm_comparison\"\n)\n</code></pre>"},{"location":"examples/full_experiment/#4-analisis-de-resultados","title":"4. An\u00e1lisis de Resultados","text":""},{"location":"examples/full_experiment/#cargar-resultados-guardados","title":"Cargar resultados guardados","text":"<pre><code>from ibioml.plots import load_results, extract_r2_scores\n\n# Cargar resultados de m\u00faltiples experimentos\nresults_dict = load_results({\n    'MLP': 'results/mlp_comparison/study_2024-01-01_12-00-00/final_results.json',\n    'LSTM': 'results/lstm_comparison/study_2024-01-01_13-00-00/final_results.json'\n})\n</code></pre>"},{"location":"examples/full_experiment/#extraer-metricas-de-rendimiento","title":"Extraer m\u00e9tricas de rendimiento","text":"<pre><code># Extraer scores R\u00b2\ntest_r2_scores, test_r2_pos, test_r2_vel, r2_df = extract_r2_scores(results_dict)\n\nprint(\"Resultados promedio por modelo:\")\nprint(r2_df.groupby('model').agg({\n    'r2_both': ['mean', 'std'],\n    'r2_position': ['mean', 'std'],\n    'r2_velocity': ['mean', 'std']\n}))\n</code></pre>"},{"location":"examples/full_experiment/#5-visualizacion","title":"5. Visualizaci\u00f3n","text":""},{"location":"examples/full_experiment/#graficos-de-comparacion","title":"Gr\u00e1ficos de comparaci\u00f3n","text":"<pre><code>from ibioml.plots import (\n    boxplot_test_r2_scores_both_targets,\n    plot_learning_curves,\n    plot_predictions_vs_real\n)\n\n# Boxplot comparativo\nboxplot_test_r2_scores_both_targets(r2_df)\n\n# Curvas de aprendizaje (si est\u00e1n disponibles)\nplot_learning_curves(results_dict)\n\n# Predicciones vs valores reales\nplot_predictions_vs_real(results_dict, 'MLP')\n</code></pre>"},{"location":"examples/full_experiment/#analisis-estadistico","title":"An\u00e1lisis estad\u00edstico","text":"<pre><code>from scipy import stats\n\n# Test t para comparar modelos\nmlp_scores = r2_df[r2_df['model'] == 'MLP']['r2_both']\nlstm_scores = r2_df[r2_df['model'] == 'LSTM']['r2_both']\n\nt_stat, p_value = stats.ttest_ind(mlp_scores, lstm_scores)\nprint(f\"Diferencia significativa entre MLP y LSTM: p = {p_value:.4f}\")\n</code></pre>"},{"location":"examples/full_experiment/#6-guardar-reporte","title":"6. Guardar Reporte","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Crear reporte completo\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Gr\u00e1fico 1: Boxplot de rendimiento\naxes[0, 0].boxplot([mlp_scores, lstm_scores], labels=['MLP', 'LSTM'])\naxes[0, 0].set_title('Comparaci\u00f3n de Rendimiento')\naxes[0, 0].set_ylabel('R\u00b2 Score')\n\n# Gr\u00e1fico 2: Distribuci\u00f3n de scores\naxes[0, 1].hist(mlp_scores, alpha=0.7, label='MLP', bins=10)\naxes[0, 1].hist(lstm_scores, alpha=0.7, label='LSTM', bins=10)\naxes[0, 1].set_title('Distribuci\u00f3n de Scores')\naxes[0, 1].legend()\n\n# Gr\u00e1fico 3: Rendimiento por target\ntargets = ['Posici\u00f3n', 'Velocidad']\nmlp_by_target = [r2_df[r2_df['model'] == 'MLP']['r2_position'].mean(),\n                 r2_df[r2_df['model'] == 'MLP']['r2_velocity'].mean()]\nlstm_by_target = [r2_df[r2_df['model'] == 'LSTM']['r2_position'].mean(),\n                  r2_df[r2_df['model'] == 'LSTM']['r2_velocity'].mean()]\n\nx = range(len(targets))\naxes[1, 0].bar([i - 0.2 for i in x], mlp_by_target, 0.4, label='MLP')\naxes[1, 0].bar([i + 0.2 for i in x], lstm_by_target, 0.4, label='LSTM')\naxes[1, 0].set_xticks(x)\naxes[1, 0].set_xticklabels(targets)\naxes[1, 0].set_title('Rendimiento por Target')\naxes[1, 0].legend()\n\n# Gr\u00e1fico 4: Tiempo de entrenamiento (si est\u00e1 disponible)\naxes[1, 1].text(0.5, 0.5, 'Estad\u00edsticas del Experimento\\n\\n' +\n                f'MLP R\u00b2 promedio: {mlp_scores.mean():.3f} \u00b1 {mlp_scores.std():.3f}\\n' +\n                f'LSTM R\u00b2 promedio: {lstm_scores.mean():.3f} \u00b1 {lstm_scores.std():.3f}\\n' +\n                f'Diferencia significativa: {\"S\u00ed\" if p_value &lt; 0.05 else \"No\"} (p={p_value:.4f})',\n                ha='center', va='center', fontsize=12)\naxes[1, 1].set_xlim(0, 1)\naxes[1, 1].set_ylim(0, 1)\naxes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.savefig('results/experiment_summary.png', dpi=300, bbox_inches='tight')\nplt.show()\n</code></pre>"},{"location":"examples/full_experiment/#conclusiones","title":"Conclusiones","text":"<p>Este flujo de trabajo te permite:</p> <ol> <li>Preparar datos de manera consistente y reproducible</li> <li>Configurar experimentos con diferentes arquitecturas de modelos</li> <li>Ejecutar estudios con validaci\u00f3n cruzada anidada</li> <li>Comparar resultados de manera estad\u00edsticamente rigurosa</li> <li>Visualizar hallazgos con gr\u00e1ficos informativos</li> <li>Documentar experimentos para reproducibilidad</li> </ol> <p>Para experimentos m\u00e1s avanzados, puedes:</p> <ul> <li>Explorar diferentes arquitecturas de red</li> <li>Ajustar hiperpar\u00e1metros con Optuna</li> <li>Implementar t\u00e9cnicas de regularizaci\u00f3n</li> <li>Analizar la importancia de caracter\u00edsticas</li> <li>Realizar an\u00e1lisis de sensibilidad</li> </ul>"},{"location":"source/conf/","title":"Conf","text":"<p>Configuration file for the Sphinx documentation builder.</p> <p>For the full list of built-in configuration values, see the documentation: https://www.sphinx-doc.org/en/master/usage/configuration.html</p> <p>-- Project information ----------------------------------------------------- https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\nsys.path.insert(0, os.path.abspath('../../ibioml'))\n</pre> import os import sys sys.path.insert(0, os.path.abspath('../../ibioml')) In\u00a0[\u00a0]: Copied! <pre>project = 'ibioml'\ncopyright = '2025, Juani'\nauthor = 'Juani'\nrelease = '0.1.0'\n</pre> project = 'ibioml' copyright = '2025, Juani' author = 'Juani' release = '0.1.0' <p>-- General configuration --------------------------------------------------- https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration</p> In\u00a0[\u00a0]: Copied! <pre>extensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.napoleon',            # Para Google o NumPy style docstrings\n    'sphinx.ext.viewcode',            # Link al c\u00f3digo fuente\n    'sphinx_autodoc_typehints',       # Mostrar type hints en docs\n]\n</pre> extensions = [     'sphinx.ext.autodoc',     'sphinx.ext.napoleon',            # Para Google o NumPy style docstrings     'sphinx.ext.viewcode',            # Link al c\u00f3digo fuente     'sphinx_autodoc_typehints',       # Mostrar type hints en docs ] In\u00a0[\u00a0]: Copied! <pre>templates_path = ['_templates']\nexclude_patterns = []\n</pre> templates_path = ['_templates'] exclude_patterns = [] In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>-- Options for HTML output ------------------------------------------------- https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output</p> In\u00a0[\u00a0]: Copied! <pre># html_theme = 'alabaster'\nhtml_theme = 'sphinx_rtd_theme'\nhtml_static_path = ['_static']\n</pre> # html_theme = 'alabaster' html_theme = 'sphinx_rtd_theme' html_static_path = ['_static']"}]}