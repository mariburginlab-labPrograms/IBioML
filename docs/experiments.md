# Configuraci√≥n y Ejecuci√≥n de Experimentos

IbioML proporciona un sistema flexible y potente para ejecutar experimentos de neurodecodificaci√≥n con optimizaci√≥n autom√°tica de hiperpar√°metros y validaci√≥n cruzada anidada.

## üéØ Visi√≥n General

Un experimento t√≠pico en IbioML incluye:

1. **Carga de datos** preprocesados
2. **Configuraci√≥n del modelo** y hiperpar√°metros
3. **Optimizaci√≥n** con Optuna (Bayesian, Random, Grid Search)
4. **Validaci√≥n cruzada anidada** para evaluaci√≥n robusta
5. **Guardado autom√°tico** de resultados y modelos

## üöÄ Experimento B√°sico

### Configuraci√≥n M√≠nima

```python
import pickle
from ibioml.models import MLPModel
from ibioml.tuner import run_study

# 1. Cargar datos preprocesados
with open('data/experimento_withCtxt_flat.pickle', 'rb') as f:
    X, y, trial_markers = pickle.load(f)

# 2. Configuraci√≥n b√°sica del modelo
config = {
    "model_class": MLPModel,
    "output_size": y.shape[1],  # 1 para single target, 2 para position+velocity
    "device": "cuda",           # "cuda" o "cpu"
    "num_epochs": 200,
    "es_patience": 10,          # Early stopping patience
    "reg_type": None,           # Regularizaci√≥n: None, 'l1', 'l2'
    "lambda_reg": None,
    "batch_size": 32,
    
    # Hiperpar√°metros fijos
    "hidden_size": 256,
    "num_layers": 2,
    "dropout": 0.2,
    "lr": 1e-3
}

# 3. Ejecutar experimento
run_study(
    X, y, trial_markers,
    model_space=config,
    num_trials=1,              # Solo un trial (sin optimizaci√≥n)
    outer_folds=5,
    inner_folds=1,
    save_path="results/experimento_basico"
)
```

## üîß Optimizaci√≥n de Hiperpar√°metros

### Configuraci√≥n con Optimizaci√≥n

Para activar la optimizaci√≥n, define hiperpar√°metros como tuplas:

```python
config_optimized = {
    # Par√°metros fijos
    "model_class": MLPModel,
    "output_size": 1,
    "device": "cuda",
    "num_epochs": 200,
    "es_patience": 10,
    "reg_type": None,
    "lambda_reg": None,
    "batch_size": 32,
    
    # Hiperpar√°metros a optimizar
    "hidden_size": (int, 128, 512, 64),     # (tipo, min, max, step)
    "num_layers": (int, 1, 4),              # (tipo, min, max)
    "dropout": (float, 0.0, 0.5),           # (tipo, min, max)
    "lr": (float, 1e-5, 1e-2, True),        # (tipo, min, max, log_scale)
}

run_study(
    X, y, trial_markers,
    model_space=config_optimized,
    num_trials=50,              # 50 configuraciones diferentes
    outer_folds=5,
    inner_folds=3,              # Validaci√≥n cruzada interna
    save_path="results/experimento_optimizado",
    search_alg="bayes"          # "bayes", "random", "grid"
)
```

### Formato de Hiperpar√°metros

#### Par√°metros Enteros
```python
"hidden_size": (int, min_val, max_val, step)
"num_layers": (int, 1, 5)  # step=1 por defecto
```

#### Par√°metros de Punto Flotante
```python
"dropout": (float, 0.0, 0.8)           # Escala lineal
"lr": (float, 1e-6, 1e-1, True)        # Escala logar√≠tmica
```

## üß† Modelos Disponibles

### Modelos para Datos Aplanados (`*_flat.pickle`)

```python
from ibioml.models import MLPModel

mlp_config = {
    "model_class": MLPModel,
    "hidden_size": (int, 64, 1024, 32),
    "num_layers": (int, 1, 5),
    "dropout": (float, 0.0, 0.7),
    "lr": (float, 1e-5, 1e-2, True),
    # ... otros par√°metros
}
```

### Modelos para Datos Temporales (`.pickle`)

```python
from ibioml.models import RNNModel, LSTMModel, GRUModel

# RNN b√°sica
rnn_config = {
    "model_class": RNNModel,
    "hidden_size": (int, 32, 256, 16),
    "num_layers": (int, 1, 3),
    "dropout": (float, 0.0, 0.5),
    "lr": (float, 1e-5, 1e-2, True),
    # ... otros par√°metros
}

# LSTM (recomendado para secuencias largas)
lstm_config = {
    "model_class": LSTMModel,
    "hidden_size": (int, 64, 512, 32),
    "num_layers": (int, 1, 4),
    "dropout": (float, 0.0, 0.6),
    "lr": (float, 1e-5, 1e-2, True),
    # ... otros par√°metros
}
```

## üìä Tipos de Experimentos

### Experimento Single-Target

```python
# Solo decodificar posici√≥n
with open('data/experimento_withCtxt_onlyPosition_flat.pickle', 'rb') as f:
    X_pos, y_pos, T = pickle.load(f)

position_config = {
    "model_class": MLPModel,
    "output_size": 1,  # Una sola salida
    "hidden_size": (int, 128, 512, 64),
    "lr": (float, 1e-5, 1e-2, True),
    # ... resto de configuraci√≥n
}

run_study(X_pos, y_pos, T, model_space=position_config, 
          save_path="results/position_decoding")
```

### Experimento Multi-Target

```python
# Decodificar posici√≥n y velocidad simult√°neamente
with open('data/experimento_withCtxt_bothTargets_flat.pickle', 'rb') as f:
    X_both, y_both, T = pickle.load(f)

dual_config = {
    "model_class": MLPModel,
    "output_size": 2,  # Posici√≥n + velocidad
    "hidden_size": (int, 256, 1024, 64),  # Redes m√°s grandes para dual-output
    "lr": (float, 1e-5, 1e-2, True),
    # ... resto de configuraci√≥n
}

run_study(X_both, y_both, T, model_space=dual_config,
          save_path="results/dual_target_decoding")
```

### Comparaci√≥n de Arquitecturas

```python
# Funci√≥n helper para experimentos comparativos
def run_architecture_comparison(X, y, T, base_path):
    architectures = {
        'mlp': MLPModel,
        'rnn': RNNModel,
        'lstm': LSTMModel,
        'gru': GRUModel
    }
    
    base_config = {
        "output_size": y.shape[1],
        "device": "cuda",
        "num_epochs": 150,
        "batch_size": 32,
        "hidden_size": (int, 128, 256, 32),
        "lr": (float, 1e-4, 1e-2, True),
    }
    
    for arch_name, model_class in architectures.items():
        config = base_config.copy()
        config["model_class"] = model_class
        
        run_study(
            X, y, T,
            model_space=config,
            num_trials=20,
            outer_folds=5,
            save_path=f"{base_path}/{arch_name}"
        )
        print(f"‚úÖ Completado: {arch_name}")

# Ejecutar comparaci√≥n
run_architecture_comparison(X, y, T, "results/architecture_comparison")
```

## ‚öôÔ∏è Configuraci√≥n Avanzada

### Algoritmos de Optimizaci√≥n

```python
# Optimizaci√≥n Bayesiana (recomendado)
run_study(X, y, T, model_space=config, 
          search_alg="bayes", num_trials=50)

# B√∫squeda aleatoria (para espacios grandes)
run_study(X, y, T, model_space=config,
          search_alg="random", num_trials=100)

# B√∫squeda en grilla (para espacios peque√±os)
run_study(X, y, T, model_space=config,
          search_alg="grid", num_trials=25)
```

### Configuraci√≥n de Validaci√≥n Cruzada

```python
# Validaci√≥n cruzada est√°ndar
run_study(X, y, T, model_space=config,
          outer_folds=5,    # 5-fold CV externo
          inner_folds=3)    # 3-fold CV interno

# Para datasets peque√±os
run_study(X, y, T, model_space=config,
          outer_folds=3,
          inner_folds=1)    # Sin CV interno

# Para evaluaci√≥n robusta
run_study(X, y, T, model_space=config,
          outer_folds=10,   # 10-fold CV
          inner_folds=5)
```

### Regularizaci√≥n

```python
# Sin regularizaci√≥n
config = {
    "reg_type": None,
    "lambda_reg": None,
    # ... otros par√°metros
}

# Con regularizaci√≥n L2
config = {
    "reg_type": "l2",
    "lambda_reg": (float, 1e-6, 1e-2, True),
    # ... otros par√°metros
}

# Con regularizaci√≥n L1
config = {
    "reg_type": "l1", 
    "lambda_reg": (float, 1e-5, 1e-1, True),
    # ... otros par√°metros
}
```

## üìÅ Estructura de Resultados

### Organizaci√≥n Autom√°tica

```
results/
‚îú‚îÄ‚îÄ experimento_basico/
‚îÇ   ‚îî‚îÄ‚îÄ study_2024-01-15_14-30-25/    # Timestamp autom√°tico
‚îÇ       ‚îú‚îÄ‚îÄ final_results.json         # Resultados finales
‚îÇ       ‚îî‚îÄ‚îÄ training_results/          # Resultados por fold
‚îÇ           ‚îú‚îÄ‚îÄ fold_0/
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ results.json       # M√©tricas del fold
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ best_model.pt      # Mejor modelo del fold
‚îÇ           ‚îú‚îÄ‚îÄ fold_1/
‚îÇ           ‚îî‚îÄ‚îÄ ...
```

### Contenido de Resultados

```python
# final_results.json
{
    "best_r2_score_test": 0.847,
    "best_params": {
        "hidden_size": 256,
        "lr": 0.0031,
        "dropout": 0.23
    },
    "mean_r2_test": 0.821,
    "std_r2_test": 0.045,
    "study_name": "study_2024-01-15_14-30-25",
    "total_trials": 50,
    "experiment_duration_minutes": 23.5
}
```

## üîç Monitoreo de Experimentos

### Seguimiento en Tiempo Real

```python
import optuna

# Visualizar progreso (requiere optuna-dashboard)
study = optuna.load_study(
    study_name="mi_experimento",
    storage="sqlite:///results/optuna_studies.db"
)

# Gr√°ficos de optimizaci√≥n
fig = optuna.visualization.plot_optimization_history(study)
fig.show()

fig = optuna.visualization.plot_param_importances(study)
fig.show()
```

### Logs y Debugging

```python
import logging

# Configurar logging detallado
logging.basicConfig(level=logging.INFO)

# Ejecutar con logs verbosos
run_study(X, y, T, model_space=config,
          save_path="results/experimento_debug",
          num_trials=5)  # Pocas iteraciones para debug
```

## üö® Soluci√≥n de Problemas

### Errores Comunes

!!! warning "CUDA out of memory"
    ```python
    config = {
        "batch_size": 16,    # Reducir tama√±o de lote
        "hidden_size": (int, 64, 256, 32),  # Redes m√°s peque√±as
        # ... otros par√°metros
    }
    ```

!!! warning "Experimento muy lento"
    ```python
    config = {
        "num_epochs": 50,     # Menos √©pocas por trial
        "es_patience": 5,     # Early stopping m√°s agresivo
        # ... otros par√°metros
    }
    
    run_study(X, y, T, model_space=config,
              num_trials=10,   # Menos trials
              outer_folds=3)   # Menos folds
    ```

!!! warning "Resultados inconsistentes"
    ```python
    # Fijar semillas para reproducibilidad
    import torch
    import numpy as np
    
    torch.manual_seed(42)
    np.random.seed(42)
    
    config = {
        "device": "cpu",  # Para m√°xima reproducibilidad
        # ... otros par√°metros
    }
    ```

## üìà Mejores Pr√°cticas

### Configuraci√≥n de Producci√≥n

```python
production_config = {
    "model_class": MLPModel,
    "output_size": 1,
    "device": "cuda",
    "num_epochs": 300,
    "es_patience": 15,
    "batch_size": 64,
    
    # Espacio de b√∫squeda bien definido
    "hidden_size": (int, 128, 512, 32),
    "num_layers": (int, 2, 4),
    "dropout": (float, 0.1, 0.5),
    "lr": (float, 1e-5, 1e-2, True),
}

run_study(
    X, y, T,
    model_space=production_config,
    num_trials=100,           # B√∫squeda exhaustiva
    outer_folds=10,           # Evaluaci√≥n robusta
    inner_folds=5,
    save_path="results/production_experiment",
    search_alg="bayes"
)
```

### Experimentos en Lotes

```python
def batch_experiments():
    datasets = [
        'data/S19_withCtxt_flat.pickle',
        'data/S20_withCtxt_flat.pickle', 
        'data/S21_withCtxt_flat.pickle'
    ]
    
    for dataset_path in datasets:
        subject_id = dataset_path.split('/')[-1].split('_')[0]
        
        with open(dataset_path, 'rb') as f:
            X, y, T = pickle.load(f)
        
        run_study(
            X, y, T,
            model_space=production_config,
            num_trials=50,
            outer_folds=5,
            save_path=f"results/batch_experiment/{subject_id}"
        )
        
        print(f"‚úÖ Completado: {subject_id}")

batch_experiments()
```

## üìä Pr√≥ximos Pasos

Despu√©s de ejecutar experimentos:

1. **[Visualizar resultados ‚Üí](visualization.md)** An√°lisis y gr√°ficos
2. **[API Reference ‚Üí](api/training.md)** Documentaci√≥n t√©cnica detallada
3. **[Ejemplos completos ‚Üí](examples/full_experiment.md)** Casos de uso avanzados
